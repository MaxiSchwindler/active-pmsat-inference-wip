{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS = \"bisimilar\", \"Precision\", \"Recall\", \"F-Score\", \"total_time\", \"learning_rounds\"\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f\"..\")\n",
    "sys.path.append(r\"../../pmsat-inference\")\n",
    "sys.path.append(r\"../../AALPy\")\n",
    "\n",
    "from evaluation.utils import print_results_info, print_results_info_per_alg, TracedMooreSUL\n",
    "from evaluation.charts import *\n",
    "from evaluation.charts_pandas import *\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Loading Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 # python evaluation/learn_automata.py -o \"None\" -n 10 -ns 3-9 -ni 3 -no 3 --learn_num_times 3 --glitch_percent 0.0 1.0 5.0 --glitch_mode \"enter_random_state\" -a \"APMSL\" \"APMSL(ddt=False)\" \"APMSL(ddt=False, orpg=True)\" \"APMSL(ONLY_RW)\" \"APMSL(ONLY_RW, ddt=True)\" \"APMSL(RW)\" \"APMSL(RW, ddt=False)\" \"APMSL(RW, ddt=False, orgp=True)\" \"APMSL(tc=500)\" \"APMSL(tc=500, ddt=False)\" \"APMSL(NO_REP, RW, ddt=False)\"  # interrupted when starting 7state \n",
    "# 26: python evaluation/learn_automata.py --learn_all_automata_from_dir example_automata/moore -a \"APMSL(RW)\" -o \"Random\" \"None\"  # terminated by MemoryError\n",
    "# 34:  python evaluation/learn_automata.py --files example_automata/moore -a \"APMSL(RW)\" \"APMSL(ONLY_RW)\" \"APMSL\" -o \"Random\" \"None\" --glitch_mode \"enter_random_state\" --glitch_percent 0.0 1.0\n",
    "# 49: # python evaluation/learn_automata.py -a \"APMSL\" \"APMSL(RW)\" \"APMSL(ONLY_RW)\" \"APMSL(RW,GTT1)\" \"APMSL(RW,GTT2)\" \"APMSL(ONLY_RW,GTT2)\"  -o \"Random\" \"None\" -f example_automata/moore/car_alarm.dot example_automata/moore/coffee_moore.dot --learn_num_times 10 --glitch_percent 0.0 0.5 1.0 --glitch_mode \"enter_random_state\"\n",
    "\n",
    "base_results_dir = r\"../../../OLD_active-pmsat-inference-wip/\"\n",
    "#results_dir = r\"learning_results_34\"\n",
    "results_dir = r\"learning_results_49\"\n",
    "#results_dir = r\"learning_results_25\"\n",
    "\n",
    "results_dir = os.path.join(base_results_dir, results_dir)\n",
    "results = load_results(results_dir)\n",
    "print(f\"Loaded {len(results)} results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # typo in orpg -> always failed. remove\n",
    "    df = df[df[\"algorithm_name\"] != \"APMSL('RW', ddt=False, orgp=True)\"].copy()\n",
    "    \n",
    "    # remove invalid results (exceptions)\n",
    "    df = df[df[\"exception\"].isna()].copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def postprocess_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ddt=True is default anyways\n",
    "    df.loc[df[\"algorithm_name\"] == \"APMSL('ONLY_RW', ddt=True)\", \"algorithm_name\"] = \"APMSL('ONLY_RW')\"\n",
    "\n",
    "    # Remove only a final empty parentheses pair \"()\"\n",
    "    df[\"algorithm_name\"] = df[\"algorithm_name\"].str.replace(r\"\\(\\)$\", \"\", regex=True)\n",
    "\n",
    "    # Remove single quotes in algorithm names\n",
    "    df[\"algorithm_name\"] = df[\"algorithm_name\"].str.replace(\"'\", \"\", regex=False)\n",
    "    \n",
    "    # Add 'model_name' column\n",
    "    df[\"model_name\"] = df[\"original_automaton\"].apply(lambda x: Path(x).stem)\n",
    "                                \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies filtering and post-processing to the results DataFrame.\"\"\"\n",
    "    return postprocess_results(remove_results(df))\n",
    "\n",
    "def print_exceptions(df: pd.DataFrame):\n",
    "    df_exceptions = df.copy().dropna(subset=['exception'])\n",
    "    \n",
    "    for exception, group in df_exceptions.groupby('exception'):\n",
    "        print(f\"Exception: {exception} occurred in following combinations:\")\n",
    "        for _, row in group.iterrows():\n",
    "            file, alg, orac, glitch = (row['original_automaton'], row['algorithm_name'], row['oracle'], row['glitch_percent'])\n",
    "            file = Path(file).stem\n",
    "            print(f\"   {file, alg, orac, glitch}\")\n",
    "        print()\n",
    "\n",
    "print_exceptions(results)\n",
    "results = clean_results(results)\n",
    "load_gsm_comparison_data(results_dir, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_mat(result: pd.DataFrame):\n",
    "    if not is_valid_result(result):\n",
    "        return False\n",
    "    return result.oracle == \"None\"\n",
    "\n",
    "kwargs = {}\n",
    "\n",
    "if results[\"glitch_percent\"].nunique() > 1:\n",
    "    kwargs[\"group_by\"] = [\"glitch_percent\"] \n",
    "\n",
    "car_alarm_results = results[results[\"model_name\"] == \"car_alarm\"]\n",
    "for stat in STATISTICS:\n",
    "    bar_chart_per_algorithm_and_oracle(car_alarm_results, stat, agg_method=\"mean\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Comparison with GSM (difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if results[\"glitch_percent\"].nunique() > 1:\n",
    "    kwargs[\"group_by\"] = [\"glitch_percent\"] \n",
    " \n",
    "for stat in STATISTICS:\n",
    "    if stat in (\"learning_rounds\", ):\n",
    "        continue\n",
    "    results[f\"{stat} vs GSM (pm)\"] = results[stat] - results[f\"GSM_with_purge_mismatches.{stat}\"]\n",
    "    results[f\"{stat} vs GSM\"] = results[stat] - results[f\"GSM_without_purge_mismatches.{stat}\"]\n",
    "    \n",
    "    multiple_bar_charts_per_algorithm_and_oracle(results[results[\"model_name\"] == \"car_alarm\"], [f\"{stat} vs GSM (pm)\", f\"{stat} vs GSM\"], agg_method=\"mean\", positioning_mode=\"below\", figsize=(12,4), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### GSM comparison (seperate charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if results[\"glitch_percent\"].nunique() > 1:\n",
    "    kwargs[\"group_by\"] = [\"glitch_percent\"] \n",
    "\n",
    "car_alarm_results = results[results[\"model_name\"] == \"car_alarm\"]\n",
    "for stat in STATISTICS:\n",
    "    if stat in (\"learning_rounds\", ):\n",
    "        continue\n",
    "    multiple_bar_charts_per_algorithm_and_oracle(car_alarm_results, [stat, f\"GSM_with_purge_mismatches.{stat}\", f\"GSM_without_purge_mismatches.{stat}\"], agg_method=\"mean\", positioning_mode=\"below\", figsize=(12,4), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Run counts per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = results.groupby(['original_automaton', 'algorithm_name', 'oracle', 'glitch_percent']).size().reset_index(name='count')\n",
    "counts_sorted = counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Group by count and print formatted output\n",
    "for count, group in counts_sorted.groupby('count', sort=False):\n",
    "    print(f\"{count} times:\")\n",
    "    for _, row in group.iterrows():\n",
    "        file, name, oracle, glitch, count = row\n",
    "        file = Path(file).stem\n",
    "        print(f\"   {file, name, oracle, glitch}\")  # Exclude the 'count' column when printing\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Grouped by model (1 chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for stat in STATISTICS:\n",
    "    bar_chart(results, stat, agg_method=\"mean\", group_by=(\"original_automaton\", \"glitch_percent\", \"oracle\", \"algorithm_name\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Grouped by model (seperate charts per model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_file in results[\"original_automaton\"].unique():\n",
    "    model_name = Path(model_file).stem\n",
    "    display(Markdown(f\"#### {model_name}\"))\n",
    "    \n",
    "    for stat in STATISTICS:\n",
    "        bar_chart(results[results[\"original_automaton\"] == model_file].copy(), \n",
    "                  stat,\n",
    "                  agg_method=\"mean\", \n",
    "                  group_by=(\"algorithm_name\", \"oracle\", \"glitch_percent\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Grouped by algorithm (seperate charts per algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in results[\"algorithm_name\"].unique():\n",
    "    display(Markdown(f\"### {algorithm}\"))\n",
    "    \n",
    "    for stat in STATISTICS:\n",
    "        bar_chart(results[results[\"algorithm_name\"] == algorithm].copy(), \n",
    "                  stat,\n",
    "                  agg_method=\"mean\", \n",
    "                  group_by=(\"original_automaton\", \"oracle\", \"glitch_percent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Comparison with GSM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsm_comparison(learning_results: pd.DataFrame, comparison_results: pd.DataFrame):\n",
    "    comparison_results\n",
    "\n",
    "gsm_comparison_folder = Path(results_dir) / \"GSM_comparison\"\n",
    "if not gsm_comparison_folder.exists():\n",
    "    print(f\"No GSM comparison folder {gsm_comparison_folder} found.\\nCreate it by running evaluation/compare_with_gsm.py {results_dir}\")\n",
    "else:\n",
    "    data = []\n",
    "    for file in gsm_comparison_folder.iterdir():\n",
    "        if file.name.startswith('info') or not file.name.endswith(\".json\"):\n",
    "            continue\n",
    "        with open(file.resolve(), \"r\") as f:\n",
    "            result = json.load(f)\n",
    "            result[\"comparison_results_file\"] = file.name\n",
    "            data.append(result)\n",
    "    comparison_results = pd.json_normalize(data)\n",
    "    comparison_results[\"model_name\"] = comparison_results[\"original_automaton\"].apply(lambda x: Path(x).stem)\n",
    "    comparison_results[\"apmsl_variant\"] = comparison_results.apply(\n",
    "        lambda row: f\"{row['algorithm_name']} ({row['oracle']})\", axis=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    #gsm_comparison(results, comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"bisimilar\", \"Precision\", \"Recall\", \"F-Score\"]\n",
    "algorithms = [\"apmsl_algorithm\", \"GSM_with_purge_mismatches\", \"GSM_without_purge_mismatches\"]\n",
    "colors = [\"blue\", \"orange\", \"green\"]  # Define colors for algorithms\n",
    "\n",
    "means = {metric: [comparison_results[comparison_results[\"model_name\"] == \"car_alarm\"][f\"{algo}.{metric}\"].mean() for algo in algorithms] \n",
    "         for metric in metrics}\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.bar(x + i * width, [means[metric][i] for metric in metrics], width, label=algo, color=colors[i])\n",
    "\n",
    "ax.set_xticks(x + width)  # Adjust x-tick positions\n",
    "ax.set_xticklabels(metrics)  # Set metric names on x-axis\n",
    "ax.set_ylabel(\"Mean Value\")\n",
    "ax.set_title(\"Comparison of Algorithm Statistics\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"bisimilar\", \"Precision\", \"Recall\", \"F-Score\"]\n",
    "algorithms = [\"apmsl_algorithm\", \"GSM_with_purge_mismatches\", \"GSM_without_purge_mismatches\"]\n",
    "colors = [\"blue\", \"orange\", \"green\"]\n",
    "\n",
    "group_col = \"glitch_percent\"  # Change to \"model_name\" if needed\n",
    "grouped = comparison_results.groupby(group_col)\n",
    "\n",
    "# Create subplots for each group\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(16, 5), sharey=True)\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    means = grouped[[f\"{algo}.{metric}\" for algo in algorithms]].mean()\n",
    "\n",
    "    x = np.arange(len(means))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, algo in enumerate(algorithms):\n",
    "        ax.bar(x + i * width, means[f\"{algo}.{metric}\"], width, label=algo, color=colors[i])\n",
    "\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(means.index, rotation=15)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(\"Mean Value\")\n",
    "\n",
    "axes[0].legend(title=\"Algorithm\")  # Show legend only once\n",
    "plt.suptitle(f\"Comparison of Algorithm Statistics Grouped by {group_col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "df = comparison_results\n",
    "# df = df[df[\"model_name\"] == \"car_alarm\"]\n",
    "df = df[df[\"oracle\"] == \"None\"]\n",
    "df = df[df[\"original_num_states\"] > 4]\n",
    "\n",
    "metrics = [\"bisimilar\", \"Precision\", \"Recall\", \"F-Score\"]\n",
    "gsm_algorithms = [\"GSM_with_purge_mismatches\", \"GSM_without_purge_mismatches\"]\n",
    "\n",
    "apmsl_variants = df[\"algorithm_name\"].unique().tolist()\n",
    "algorithms = apmsl_variants + gsm_algorithms\n",
    "\n",
    "group_col = \"glitch_percent\"  # Change to \"model_name\" if needed\n",
    "\n",
    "# 1 subplot for each metric\n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(16, 5), sharey=True)\n",
    "\n",
    "width = 0.08\n",
    "colors = [\"blue\", \"orange\", \"green\", \"red\", \"purple\", \"cyan\", \"pink\", \"brown\"]\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # compute means for APMSL variants\n",
    "    grouped_means_apmsl = df.groupby([group_col, \"algorithm_name\"])[f\"apmsl_algorithm.{metric}\"].mean().unstack()\n",
    "    \n",
    "    # compute means for GSM algorithms\n",
    "    grouped_means_gsm = df.groupby(group_col)[[f\"{algo}.{metric}\" for algo in gsm_algorithms]].mean()\n",
    "\n",
    "    # Merge them together\n",
    "    grouped_means = pd.concat([grouped_means_apmsl, grouped_means_gsm], axis=1)\n",
    "    grouped_means = grouped_means.reset_index()\n",
    "\n",
    "    #print(f\"\\nMetric: {metric}\\n\", grouped_means)\n",
    "\n",
    "    unique_groups = grouped_means[group_col].unique()\n",
    "    x = np.arange(len(unique_groups))\n",
    "\n",
    "    for i, algo in enumerate(algorithms):\n",
    "        if algo in apmsl_variants:\n",
    "            algo_col = algo  # APMSL variants are in columns directly\n",
    "        else:\n",
    "            algo_col = f\"{algo}.{metric}\"  # GSM columns are explicitly named\n",
    "\n",
    "        # Extract means (align missing values)\n",
    "        y_values = grouped_means.set_index(group_col).get(algo_col, np.nan).reindex(unique_groups).values\n",
    "\n",
    "        ax.bar(x + i * width, y_values, width, label=algo, color=colors[i % len(colors)])\n",
    "\n",
    "    ax.set_xticks(x + (width * len(algorithms) / 2))\n",
    "    ax.set_xticklabels(unique_groups, rotation=15)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(\"Mean Value\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.suptitle(f\"Comparison of Algorithm Variants Grouped by {group_col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
