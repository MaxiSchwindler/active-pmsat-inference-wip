{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FIGURES_TO = \"thesis_charts\"  # or None to just show figures\n",
    "STYLE = \"thesis\"  # \"thesis\": styled like my thesis (times new roman, etc); \"latex\": default latex look\n",
    "\n",
    "ALGORITHM_NAMES = {\n",
    "    \"APMSL('ONLY_RW')\": 'APMSL (ic, rw, term-impr)',\n",
    "    \"APMSL('ONLY_RW', 'GTT2')\": 'APMSL (ic, rw, term-thresh2)',\n",
    "    \"APMSL('RW', orpg=True)\": 'APMSL (ic, rw, replay, repro, term-impr)',\n",
    "    \"APMSL('RW', 'GTT2', orpg=True)\": 'APMSL (ic, rw, replay, repro, term-thresh2)',\n",
    "\n",
    "    \"GSM('PURGE')\": 'GSM (purge, rw, term-bisim)',\n",
    "    \"GSM('NO_PURGE')\": 'GSM (no-purge, rw, term-bisim)',\n",
    "}\n",
    "\n",
    "BASE_ALGORITHM_HATCHES = {\n",
    "    \"APMSL\": \"xxx\",\n",
    "    \"GSM\": \"...\",\n",
    "}\n",
    "\n",
    "BASE_ALGORITHM_COLORMAP = {\n",
    "    \"APMSL\": \"plasma\",\n",
    "    \"GSM\": \"viridis\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports & Misc Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"..\")\n",
    "sys.path.append(r\"../../pmsat-inference\")\n",
    "sys.path = [r\"../../AALpy\"] + sys.path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evaluation.utils import print_results_info, print_results_info_per_alg, TracedMooreSUL\n",
    "import evaluation.charts as charts\n",
    "import evaluation.charts_pandas as charts_pd\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    os.makedirs(SAVE_FIGURES_TO, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = r\"../server_results/learning_results_7\"\n",
    "\n",
    "results, results_df = charts.load_results(results_dir, remove_traces_used_to_learn=True, is_server_results=True, as_pandas=True)\n",
    "print(f\"Loaded {len(results)} results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_result_if(result):\n",
    "    return False\n",
    "\n",
    "def postprocess_result(result):\n",
    "    if result[\"algorithm_name\"] in ALGORITHM_NAMES:\n",
    "        result[\"algorithm_name\"] = ALGORITHM_NAMES[result[\"algorithm_name\"]]\n",
    "    else:\n",
    "        print(f'Warning: \"{result[\"algorithm_name\"]}\" not found in algorithm_names!')\n",
    "\n",
    "    return result\n",
    "\n",
    "def filter_results(results):\n",
    "    if isinstance(results, pd.DataFrame):\n",
    "        mask = results.apply(lambda r: not remove_result_if(r), axis=1)\n",
    "        filtered_df = results[mask].copy()\n",
    "        return filtered_df.apply(postprocess_result, axis=1)\n",
    "    else:\n",
    "        return [postprocess_result(r) for r in results if not remove_result_if(r)]\n",
    "\n",
    "results = filter_results(results)\n",
    "results_df = filter_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Plot Config (automatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"default\")  # initialize to default - seaborn overrides!\n",
    "\n",
    "# color map creation\n",
    "CM_OFFSET = 2\n",
    "ALL_ALGORITHMS_IN_RESUlTS = sorted(results_df[\"algorithm_name\"].unique().tolist())\n",
    "ALGORITHM_COLORS = {}\n",
    "ALGORITHM_HATCHES = {}\n",
    "for base_alg, colormap in BASE_ALGORITHM_COLORMAP.items():\n",
    "    alg_versions = [a for a in ALL_ALGORITHMS_IN_RESUlTS if a.startswith(base_alg)]\n",
    "    colormap = plt.get_cmap(BASE_ALGORITHM_COLORMAP[base_alg], lut=len(alg_versions) + CM_OFFSET)\n",
    "    for i, alg_version in enumerate(alg_versions):\n",
    "        color = colormap(i + (CM_OFFSET // 2))\n",
    "        ALGORITHM_COLORS[alg_version] = color\n",
    "        ALGORITHM_HATCHES[alg_version] = BASE_ALGORITHM_HATCHES.get(base_alg, '')\n",
    "\n",
    "if STYLE == \"latex\":\n",
    "    mpl.rcParams.update({\n",
    "        \"text.usetex\": False,  # True if LaTeX is installed\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"CMU Serif\"],  # download from https://ctan.org/pkg/cm-unicode\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"font.size\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"axes.titlesize\": 14\n",
    "    })\n",
    "\n",
    "elif STYLE == \"thesis\":\n",
    "    mpl.rcParams.update({\n",
    "        # Fonts\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
    "        \"font.size\": 11,                # Match thesis body text\n",
    "        \"axes.titlesize\": 13,           # Section-style figure titles\n",
    "        \"axes.labelsize\": 11,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "\n",
    "        # Axes & Lines\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"lines.linewidth\": 1.5,\n",
    "        \"lines.markersize\": 6,\n",
    "\n",
    "        # Legend\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 10,\n",
    "        \"legend.handlelength\": 1.8,\n",
    "        \"legend.handleheight\": 0.8,\n",
    "        \"legend.borderaxespad\": 0.8,\n",
    "        \"legend.borderpad\": 0.5,\n",
    "        \"legend.labelspacing\": 0.4,\n",
    "        \"legend.handletextpad\": 0.5,\n",
    "        \"legend.columnspacing\": 1.2,\n",
    "        \"legend.fancybox\": False,\n",
    "\n",
    "        # Figure\n",
    "        # \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        # \"figure.figsize\": (9, 6),  # Good for 2-column layout\n",
    "        \"figure.constrained_layout.use\": True,\n",
    "\n",
    "        # Ticks\n",
    "        \"xtick.major.size\": 4,\n",
    "        \"xtick.major.width\": 0.8,\n",
    "        \"ytick.major.size\": 4,\n",
    "        \"ytick.major.width\": 0.8,\n",
    "\n",
    "        # PDF output\n",
    "        \"pdf.fonttype\": 42,  # Ensures text remains text in PDFs\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def bar_chart_pd(\n",
    "    ax,\n",
    "    df: pd.DataFrame,\n",
    "    key: str,\n",
    "    agg_method: str,\n",
    "    group_by: list[str],\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    y_as_percentage: bool = False,\n",
    "    x_as_percentage: bool = False,\n",
    "    legend: bool = True,\n",
    "    bar_label_fontsize: int = 10,\n",
    "    bar_label_decimal_digits: int = 2,\n",
    "    simple: bool = False,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Draws a bar chart on the given `ax`, and returns legend handles and labels.\"\"\"\n",
    "\n",
    "    pivot_df = df.groupby(by=group_by)[key].agg(agg_method).unstack().astype(float)\n",
    "    if y_as_percentage:\n",
    "        pivot_df *= 100\n",
    "\n",
    "    pivot_df.plot(kind=\"bar\", ax=ax, width=0.92, xlabel=xlabel, ylabel=ylabel, rot=0, legend=False, title=title)\n",
    "    if simple:\n",
    "        # don't do anything except plotting - no visual adjustments\n",
    "        return [], []\n",
    "\n",
    "    num_groups = len(pivot_df.index)\n",
    "    margin = 0.92 / 2  # half bar width\n",
    "    padding = 0.1  # extra space on sides\n",
    "    ax.set_xlim(-margin - padding, num_groups - 1 + margin + padding)\n",
    "\n",
    "    def percent_label(value, *args, **kwargs):\n",
    "        suffix = kwargs.get('suffix', '%')\n",
    "        return (f\"{value:.0f}\" if value % 1 == 0 else f\"{value:.1f}\") + suffix\n",
    "\n",
    "    if y_as_percentage:\n",
    "        for bars in ax.containers:\n",
    "            ax.bar_label(bars, labels=[percent_label(v) for v in bars.datavalues], fontsize=bar_label_fontsize)\n",
    "        ax.yaxis.set_major_formatter(mtick.FuncFormatter(percent_label))\n",
    "    else:\n",
    "        for bars in ax.containers:\n",
    "            ax.bar_label(bars, fmt=f\"%.{bar_label_decimal_digits}f\", fontsize=10)\n",
    "\n",
    "    if x_as_percentage:\n",
    "        ax.set_xticklabels([percent_label(x) for x in pivot_df.index])\n",
    "    else:\n",
    "        ax.set_xticklabels([x for x in pivot_df.index])\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for bars in ax.containers:\n",
    "        bar_label = bars.get_label()\n",
    "        if bar_label in ALL_ALGORITHMS_IN_RESUlTS:\n",
    "            algorithm = bar_label\n",
    "            hatch = ALGORITHM_HATCHES[algorithm]\n",
    "            color = ALGORITHM_COLORS[algorithm]\n",
    "        \n",
    "            for bar in bars:\n",
    "                if hatch:\n",
    "                    bar.set_facecolor(\"none\")\n",
    "                    bar.set_hatch(hatch)\n",
    "                    bar.set_edgecolor(color)\n",
    "                    bar.set_linewidth(1.2)\n",
    "                    patch_kwargs = dict(facecolor=\"none\", edgecolor=color, hatch=hatch + (hatch[0] * 2))\n",
    "                else:\n",
    "                    bar.set_facecolor(color)\n",
    "                    patch_kwargs = dict(facecolor=color)\n",
    "                \n",
    "            if bar_label not in labels:\n",
    "                patch = mpatches.Patch(label=bar_label, **patch_kwargs)\n",
    "                handles.append(patch)\n",
    "                labels.append(algorithm)\n",
    "\n",
    "    if legend:\n",
    "        ax.legend(handles=handles, labels=labels, title=\"Algorithm\", loc=\"best\")\n",
    "\n",
    "    return handles, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"Bisimilarity\", legend=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"F-Score\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean F-score over all results\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=\"F-score\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"\", legend=False, bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Percentage of timed-out results\", y_as_percentage=True,\n",
    "    title=\"\", legend=False, x_as_percentage=False,bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Time-Outs\", y=1.25)\n",
    "\n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    title=\"Algorithm\",\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.2),\n",
    "    ncol=3,\n",
    ")\n",
    "\n",
    "ax1.set_title(\"Per glitch percentage\", fontsize=12)\n",
    "ax2.set_title(\"Per number of states\", fontsize=12)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'timeouts.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "\n",
    "timed_out_results = results_df[results_df['timed_out']]\n",
    "\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, timed_out_results, \"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"Bisimilarity of timed-out results\", legend=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, timed_out_results, \"F-Score\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean F-score over timed-out results\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=\"F-score of timed-out results\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'timed_out_correctness.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learned_correctly\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=False,\n",
    "    title=\"Bisimilarity\", legend=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"F-Score\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Mean F-score over all results\", x_as_percentage=False,\n",
    "    y_as_percentage=False, title=\"F-score\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness_over_states.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=results_df,\n",
    "    x=\"glitch_percent\",\n",
    "    y=\"F-Score\",\n",
    "    hue=\"algorithm_name\",\n",
    "    # fill=False,  # this colors the whiskers, but not the boxes itself - maybe i can use this and apply hatch myself?\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    hue_order=ALL_ALGORITHMS_IN_RESUlTS,\n",
    "    flierprops=dict(marker='o', markersize=3, alpha=0.3),\n",
    ")\n",
    "plt.title(\"F-score per Run\")\n",
    "\n",
    "ax.set_ylabel(\"F-score\")\n",
    "ax.set_xlabel(\"Glitch percentage\")\n",
    "\n",
    "def percent_label(label):\n",
    "    return f\"{int(float(label))}%\"\n",
    "\n",
    "ax.set_xticklabels([percent_label(t.get_text()) for t in ax.get_xticklabels()])\n",
    "\n",
    "l = ax.legend(title=\"Algorithm\")\n",
    "\n",
    "# set hatches (& colors) # CAUTION: this might not work with other matplotlib / seaborn versions!\n",
    "patches = [p for p in ax.patches if type(p) == mpl.patches.PathPatch]\n",
    "num_groups = len(results_df[\"glitch_percent\"].unique())\n",
    "for alg_idx, alg in enumerate(ALL_ALGORITHMS_IN_RESUlTS):\n",
    "    hatch = ALGORITHM_HATCHES[alg]\n",
    "    color = ALGORITHM_COLORS[alg]\n",
    "    for patch in patches[(alg_idx * num_groups):((alg_idx + 1) * num_groups)]:\n",
    "        if hatch:\n",
    "            # patch.set_fill(True)\n",
    "            patch.set_facecolor(\"none\")\n",
    "            patch.set_hatch(hatch)\n",
    "            patch.set_edgecolor(color)\n",
    "            patch.set_linewidth(1.2)\n",
    "\n",
    "# fix legend for hatches\n",
    "for lp, hatch in zip(l.get_patches(), ALGORITHM_HATCHES.values()):\n",
    "    lp.set_hatch((hatch + hatch[0] * 2))\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor('none')\n",
    "\n",
    "if False:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'fscore_boxplot.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps per run\", x_as_percentage=True,\n",
    "    title=\"Mean number of steps\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"queries_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of queries per run\",\n",
    "    title=\"Mean number of queries\", legend=False, bar_label_decimal_digits=1, x_as_percentage=True,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_and_queries.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"InformationPerStep\"] = results_df[\"F-Score\"] / results_df[\"steps_learning\"]\n",
    "results_df[\"InformationPerQuery\"] = results_df[\"F-Score\"] / results_df[\"queries_learning\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"InformationPerStep\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Information per step\", x_as_percentage=True,\n",
    "    title=\"Information per step\", legend=True, bar_label_decimal_digits=5,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"InformationPerQuery\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Information per query\",\n",
    "    title=\"Information per query\", legend=False, bar_label_decimal_digits=5, x_as_percentage=True,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'info_per_steps_and_queries.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"algorithm_name\", \"glitch_percent\", \"original_automaton\"]].drop_duplicates().values  # unique combos\n",
    "\n",
    "group_cols = [\"algorithm_name\", \"glitch_percent\", \"original_automaton\"]\n",
    "target_metric = \"F-Score\"\n",
    "\n",
    "divergence = results_df.groupby(group_cols)[target_metric].std().reset_index()\n",
    "divergence = divergence.rename(columns={target_metric: \"within_group_std\"})\n",
    "\n",
    "stats_per_unique_run_combo = results_df.groupby(group_cols)[target_metric].agg(\n",
    "    mean=\"mean\", std=\"std\", min=\"min\", max=\"max\",\n",
    "    range=lambda x: x.max() - x.min()\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_models_df = (\n",
    "    results_df[[\"original_automaton\", \"original_automaton_size\", \"original_automaton_num_outputs\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"original_automaton_size\", \"original_automaton_num_outputs\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "unique_models_df[\"model_index\"] = np.arange(len(unique_models_df))\n",
    "model_to_int = dict(zip(unique_models_df[\"original_automaton\"], unique_models_df[\"model_index\"]))\n",
    "results_df[\"model_index\"] = results_df[\"original_automaton\"].map(model_to_int)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sizes = unique_models_df[\"original_automaton_size\"].values\n",
    "change_indices = np.where(np.diff(sizes) != 0)[0] + 0.5\n",
    "\n",
    "for i, algorithm in enumerate(ALL_ALGORITHMS_IN_RESUlTS):\n",
    "    ax = axes[i]\n",
    "    subset = results_df[results_df[\"algorithm_name\"] == algorithm]\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=subset,\n",
    "        x=\"model_index\",\n",
    "        y=\"F-Score\",\n",
    "        hue=\"glitch_percent\",\n",
    "        ax=ax,\n",
    "        palette=\"plasma\",\n",
    "        alpha=0.6,\n",
    "        legend=(i == 0)  # only show legend once\n",
    "    )\n",
    "\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel(\"\")  # common x label set below\n",
    "    ax.set_ylabel(\"F-Score\")\n",
    "    ax.set_title(algorithm)\n",
    "\n",
    "    # Draw vertical lines for size changes\n",
    "    for pos in change_indices:\n",
    "        ax.axvline(x=pos, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # improve ticks for only bottom plots\n",
    "    # if i < len(ALL_ALGORITHMS_IN_RESUlTS) - 2:\n",
    "    #     ax.set_xticklabels([])\n",
    "\n",
    "# Common x-label and adjust layout  # TODO very manual...\n",
    "fig.text(0.5, -0.04, \"Model Index (sorted by number of states)\", ha='center', fontsize=12)\n",
    "fig.text(0.04, 0.5, \"F-Score\", va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1, 2, 30]\n",
    "fig, axes = plt.subplots(len(durations), 1, figsize=(9, 4 * len(durations)), sharex=True)\n",
    "\n",
    "for i, sec_per_step in enumerate(durations):\n",
    "    col_name = f\"FictionalRuntime_{sec_per_step}Sec\"\n",
    "    results_df[col_name] = results_df[\"total_time\"] + results_df[\"steps_learning\"] * sec_per_step\n",
    "\n",
    "    bar_chart_pd(\n",
    "        axes[i], results_df, col_name, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=\"Mean fictional runtime (sec)\",\n",
    "        x_as_percentage=True, legend=(i == 0), bar_label_decimal_digits=0,\n",
    "        title=f\"Mean Fictional Runtime ({sec_per_step} sec/step)\"\n",
    "    )\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'fictional_runtime.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "step_durations = np.arange(0, 31)  # from 0 to 30 seconds\n",
    "\n",
    "plot_data = []\n",
    "for sec_per_step in step_durations:\n",
    "    temp_df = results_df.copy()\n",
    "    temp_df[\"fictional_runtime\"] = temp_df[\"total_time\"] + temp_df[\"steps_learning\"] * sec_per_step\n",
    "\n",
    "    grouped = (\n",
    "        temp_df\n",
    "        .groupby([\"glitch_percent\", \"algorithm_name\"])[\"fictional_runtime\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"sec_per_step\"] = sec_per_step\n",
    "    plot_data.append(grouped)\n",
    "\n",
    "combined_df = pd.concat(plot_data)\n",
    "\n",
    "unique_glitches = sorted(combined_df[\"glitch_percent\"].unique())\n",
    "n = len(unique_glitches)\n",
    "\n",
    "fig, axes = plt.subplots(n, 1, figsize=(10, 4 * n), sharex=True)\n",
    "    \n",
    "for ax, glitch in zip(axes, unique_glitches):\n",
    "    subset = combined_df[combined_df[\"glitch_percent\"] == glitch]\n",
    "    \n",
    "    for algorithm_name, group in subset.groupby(\"algorithm_name\"):\n",
    "        ax.plot(group[\"sec_per_step\"], group[\"fictional_runtime\"], label=algorithm_name, color=ALGORITHM_COLORS[algorithm_name])\n",
    "\n",
    "    ax.set_title(f\"Fictional runtime vs. per-step duration ({glitch:.0f}% glitches)\")\n",
    "    ax.set_ylabel(\"Fictional runtime (sec)\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "axes[-1].set_xlabel(\"Assumed time per step (sec)\")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'fictional_runtime_linecharts_by_glitch.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
