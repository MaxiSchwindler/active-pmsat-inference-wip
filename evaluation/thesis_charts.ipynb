{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from evaluation.charts import format_seconds\n",
    "\n",
    "# RESULTS_DIR = (r\"../server_results/benchmarks_mat\", r\"../server_results/old_bad_mat_results/benchmarks_mat_1_5_10p\")\n",
    "# RESULTS_DIR = r\"../server_results/benchmarks_mat\"\n",
    "# RESULTS_DIR = r\"../local_results/avl_apc_gtt2\"\n",
    "# RESULTS_DIR = r\"../server_results/benchmarks_nomat_with_steps\"\n",
    "# RESULTS_DIR = r\"../server_results/benchmarks_nomat\"\n",
    "# RESULTS_DIR = r\"../local_results/nomat_very_small_automata\"\n",
    "# RESULTS_DIR = r\"../local_results/rw_vs_norw_eval_to10hrs\"\n",
    "# RESULTS_DIR = (r\"../server_results/nomat_gnu_tls_24h_timeout\", r\"../server_results/nomat_gnu_tls_48h_timeout\")\n",
    "# RESULTS_DIR = (r\"../local_results/avl_apc_git\", r\"../local_results/avl_apc_gtt2\")\n",
    "# RESULTS_DIR = r\"../local_results/avl_apc_350steps_3el_newrw_to5h\"\n",
    "# RESULTS_DIR = r\"../local_results/avl_apc_0.5p\"\n",
    "# RESULTS_DIR = (r\"../thinkpad_results/learning_results_4\", r\"../thinkpad_results/learning_results_5\")\n",
    "# RESULTS_DIR = (r\"../server_results/benchmarks_mat\", r\"../server_results/benchmarks_mat_2p\")\n",
    "# RESULTS_DIR = (r\"../local_results/nomat_smaller_automata_holiday_eval\", r\"../local_results/nomat_smaller_automata_holiday_eval_0p\", r\"../local_results/nomat_smaller_automata_holiday_eval_0.1p\")\n",
    "# RESULTS_DIR = r\"../server_results/WIP_mat_benchmarks_new_oracle\"\n",
    "# RESULTS_DIR = r\"../learning_results_65\"  #kv with majtrace abort\n",
    "# RESULTS_DIR = r\"../learning_results_68\"  #kv without majtrace abort\n",
    "# RESULTS_DIR = r\"../learning_results_84\" # kv without majtrace abort\n",
    "# RESULTS_DIR = r\"../learning_results_85\" # kv with majtrace abort\n",
    "# RESULTS_DIR = \"../learning_results_86\", \"../learning_results_87\", \"../learning_results_88\"  # car alarm\n",
    "# RESULTS_DIR = \"../thinkpad_results/WIP_canonical/learning_results_6\"\n",
    "# RESULTS_DIR = \"../learning_results_93\"\n",
    "# RESULTS_DIR = \"../local_results/coffeemachine_complex_nomat\"\n",
    "# RESULTS_DIR = \"../local_results/coffeemachine_complex_mat\"\n",
    "# RESULTS_DIR = \"../local_results/avl_apc_0.5p_new_el4\"\n",
    "RESULTS_DIR = \"../server_results/benchmarks_nomat\"\n",
    "\n",
    "# RESULTS_DIR = \"../local_results/AVL_APC/nomat/1p\"\n",
    "# RESULTS_DIR = \"../local_results/BLE_CYW/cywble_nomat\"\n",
    "# RESULTS_DIR = \"../local_results/Car Alarm/0.5p_el2\", \"../local_results/Car Alarm/0.5p_el3\", \"../local_results/Car Alarm/gsm\"\n",
    "# RESULTS_DIR = \"../local_results/Coffeemachine/coffeemachine_complex_nomat/0.5p_rw_el2\", \"../local_results/Coffeemachine/coffeemachine_complex_nomat/0.5p_rw2_el2\"\n",
    "# RESULTS_DIR = \"../local_results/Coffeemachine/coffeemachine_complex_mat\"\n",
    "# RESULTS_DIR = \"../local_results/BLE_CYW/reduced_states/without_mtu_req/0.5p_rw\", \"../local_results/BLE_CYW/reduced_states/without_mtu_req/0.5p_rw2\"\n",
    "# RESULTS_DIR = \"../local_results/BLE_CYW/reduced_states/without_mtu_req_and_feature_rsp/mat_0.5p\"\n",
    "# RESULTS_DIR = \"../local_results/BLE_CYW/reduced_states/without_mtu_req/mat_0.5p\"\n",
    "# RESULTS_DIR = \"../local_results/random_automata/rw_vs_rw2\"\n",
    "\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/selected_findings/rw\"  # or None to just show figures\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/APC/new\"  # or None to just show figures\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/mat/new_with_2\"  # or None to just show figures\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/mat/new_with_2\"  # or None to just show figures\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/APC/0.5p\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/mat_new_oracle\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/car_alarm\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/canonical/thinkpad\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/canonical/coffeemachine_complex/nomat\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/canonical/car_alarm\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/nomat/potential\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/APC/0.5p_new_4el\"\n",
    "# SAVE_FIGURES_TO = \"thesis_charts/canonical/coffeemachine_complex/nomat/0.5p_rw_el2/\"\n",
    "SAVE_FIGURES_TO = None\n",
    "STYLE = \"thesis\"  # \"thesis\": styled like my thesis (times new roman, etc); \"latex\": default latex look\n",
    "\n",
    "IS_SERVER_RESULTS = False\n",
    "X_AXIS = 'glitch_percent' #'timeout_str' # \"glitch_percent\"\n",
    "X_AXIS_TITILE = 'Glitch percentage' #'Timeout'  # 'Glitch percentage'\n",
    "\n",
    "APMSL_NAME = \"APMSL\"\n",
    "GSM_NAME = \"SM-GT\"\n",
    "\n",
    "DATA_GENERATION_TRANSLATION = {\n",
    "    'input-completeness': 'ic',\n",
    "    'random-walks': 'rw',\n",
    "    'random-walks-with-reset-prob': 'rw2',\n",
    "    'replay-glitches': 'replay',\n",
    "    'reproduce-glitches': 'repro',\n",
    "    'cex-processing': 'cex'\n",
    "}\n",
    "\n",
    "TERMINATION_MODE_TRANSLATION = {\n",
    "    'glitch-improvement': 'term-impr',\n",
    "    'glitch-threshold': 'term-thresh',\n",
    "    'glitch-threshold-twice': 'term-thresh2',\n",
    "    'bisimilarity': 'term-bisim',\n",
    "    'oracle': 'term-oracle',\n",
    "}\n",
    "\n",
    "GSM_PARAM_TRANSLATION = {\n",
    "    'purge': 'purge',\n",
    "    'no-purge': 'no-purge',\n",
    "}\n",
    "\n",
    "BASE_ALGORITHM_HATCHES = {\n",
    "    APMSL_NAME: \"xxx\",\n",
    "    GSM_NAME: \"...\",\n",
    "}\n",
    "\n",
    "BASE_ALGORITHM_COLORMAP = {\n",
    "    APMSL_NAME: \"plasma\",\n",
    "    GSM_NAME: \"viridis\",\n",
    "}\n",
    "\n",
    "STYLE = \"thesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports & Misc Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(f\"..\")\n",
    "sys.path.append(r\"../../pmsat-inference\")\n",
    "sys.path = [r\"../../AALpy\"] + sys.path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evaluation.utils import print_results_info, print_results_info_per_alg, TracedMooreSUL\n",
    "import evaluation.charts as charts\n",
    "import evaluation.charts_pandas as charts_pd\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    os.makedirs(SAVE_FIGURES_TO, exist_ok=True)\n",
    "\n",
    "PRECISION_KEY = \"Precision_v2\"\n",
    "RECALL_KEY = \"Recall_v2\"\n",
    "FSCORE_KEY = \"F-Score_v2\"\n",
    "\n",
    "ACCURACY_KEY = \"Accuracy\"\n",
    "ACCURACY_NAME = \"accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_o_results, _o_results_df = charts.load_results(RESULTS_DIR, remove_traces_used_to_learn=True, is_server_results=IS_SERVER_RESULTS, as_pandas=True)\n",
    "print(f\"Loaded {len(_o_results)} results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_result_if(result):\n",
    "    if isinstance(result, dict):\n",
    "        has_exception = \"exception\" in result\n",
    "    else:  # pandas.Series\n",
    "        has_exception = \"exception\" in result and pd.notna(result[\"exception\"])\n",
    "\n",
    "    if has_exception:\n",
    "        print(f\"WARNING: Exception in {result['results_file']}\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def build_algorithm_name(result):\n",
    "    if 'params' not in result.get('detailed_learning_info', {}):\n",
    "        print(\"Use old-style algorithm name builder; inferred default kwargs may be incorrect\")\n",
    "        return build_algorithm_name_old(result)\n",
    "    \n",
    "    name_in_result = result[\"algorithm_name\"]\n",
    "    alg_kwargs = result['detailed_learning_info']['params']\n",
    "    \n",
    "    # hack: define this so that eval('random_suffix') works:\n",
    "    random_suffix = \"random_suffix\"\n",
    "    \n",
    "    if name_in_result.startswith(\"APMSL\"):\n",
    "        name = APMSL_NAME\n",
    "        params = []\n",
    "\n",
    "        params.append(f\"el={alg_kwargs['extension_length']}\")\n",
    "        \n",
    "        if eval(alg_kwargs[\"input_completeness_preprocessing\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"input-completeness\"])\n",
    "            \n",
    "        if eval(alg_kwargs[\"random_steps_per_round\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"random-walks\"])\n",
    "            \n",
    "        if eval(alg_kwargs[\"random_steps_per_round_with_reset_prob\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"random-walks-with-reset-prob\"])\n",
    "\n",
    "        if 'EqOracleTermination' in alg_kwargs['termination_mode']:\n",
    "            # only relevant with oracle\n",
    "            if eval(alg_kwargs[\"cex_processing\"]):\n",
    "                params.append(DATA_GENERATION_TRANSLATION[\"cex-processing\"])\n",
    "\n",
    "                if not eval(alg_kwargs[\"discard_glitched_traces\"]):\n",
    "                    raise ValueError(f\"Ran with cex_processing, but without discard_glitched_traces in {result['results_file']}?!\")\n",
    "\n",
    "                if not eval(alg_kwargs[\"add_cex_as_hard_clauses\"]):\n",
    "                    raise ValueError(f\"Ran with cex_processing, but without add_cex_as_hard_clauses in {result['results_file']}?!\")\n",
    "            else:\n",
    "                if eval(alg_kwargs[\"discard_glitched_traces\"]):\n",
    "                    raise ValueError(f\"Ran without cex_processing, but with discard_glitched_traces in {result['results_file']}?!\")\n",
    "\n",
    "                if eval(alg_kwargs[\"add_cex_as_hard_clauses\"]):\n",
    "                    raise ValueError(f\"Ran without cex_processing, but with add_cex_as_hard_clauses in {result['results_file']}?!\")\n",
    "\n",
    "            \n",
    "        if eval(alg_kwargs[\"replay_glitches\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"replay-glitches\"])\n",
    "            \n",
    "            if not eval(alg_kwargs[\"only_replay_peak_glitches\"]):\n",
    "                raise ValueError(f\"Ran without only_replay_peak_glitches in {result['results_file']}?!\")\n",
    "            \n",
    "        if eval(alg_kwargs[\"glitch_processing\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"reproduce-glitches\"])\n",
    "\n",
    "        term_mode = alg_kwargs['termination_mode']  # string\n",
    "        term_mode = term_mode.removeprefix(\"<active_pmsatlearn.defs.\")\n",
    "        if term_mode.startswith(\"GlitchImprovementTermination\"):\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['glitch-improvement'])\n",
    "        elif term_mode.startswith(\"GlitchThresholdTermination\"):\n",
    "            first_time = \"first_time=True\" in term_mode\n",
    "            if first_time:\n",
    "                params.append(TERMINATION_MODE_TRANSLATION['glitch-threshold'])\n",
    "            else:\n",
    "                assert \"first_time=False\" in term_mode\n",
    "                params.append(TERMINATION_MODE_TRANSLATION['glitch-threshold-twice'])\n",
    "        elif term_mode.startswith(\"EqOracleTermination\"):\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['oracle'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown termination mode '{term_mode}' in {result['results_file']}\")\n",
    "        \n",
    "    elif name_in_result.startswith(\"GSM\"):\n",
    "        name = GSM_NAME\n",
    "        params = []\n",
    "        if eval(alg_kwargs[\"purge_mismatches\"]):\n",
    "            params.append(GSM_PARAM_TRANSLATION['purge'])\n",
    "        else:\n",
    "            params.append(GSM_PARAM_TRANSLATION['no-purge'])\n",
    "        \n",
    "        if eval(alg_kwargs[\"random_steps_per_round\"]):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"random-walks\"])\n",
    "        \n",
    "        if alg_kwargs[\"eq_oracle\"] != \"None\":\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['oracle'])\n",
    "        else:\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['bisimilarity'])\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    return f\"{name}({', '.join(params)})\"\n",
    "    \n",
    "    \n",
    "def build_algorithm_name_old(result):\n",
    "    name_in_result = result[\"algorithm_name\"]\n",
    "    alg_kwargs = result[\"algorithm_kwargs\"]\n",
    "    if name_in_result.startswith(\"APMSL\"):\n",
    "        name = APMSL_NAME\n",
    "        params = []\n",
    "        \n",
    "        if alg_kwargs.get(\"input_completeness_preprocessing\", True):  # default is True\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"input-completeness\"])\n",
    "            \n",
    "        if alg_kwargs.get(\"random_steps_per_round\", 0):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"random-walks\"])\n",
    "            \n",
    "        if alg_kwargs.get(\"replay_glitches\", True):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"replay-glitches\"])\n",
    "            \n",
    "            if not alg_kwargs.get(\"only_replay_peak_glitches\", False):\n",
    "                raise ValueError(f\"Ran without only_replay_peak_glitches in {result['results_file']}?!\")\n",
    "            \n",
    "        if alg_kwargs.get(\"glitch_processing\", True):\n",
    "            params.append(DATA_GENERATION_TRANSLATION[\"reproduce-glitches\"])\n",
    "            \n",
    "        term_mode = alg_kwargs.get('termination_mode', 'GlitchImprovementTermination')\n",
    "        term_mode.removeprefix(\"<active_pmsatlearn.defs.\")\n",
    "        if term_mode.startswith(\"GlitchImprovementTermination\"):\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['glitch-improvement'])\n",
    "        elif term_mode.startswith(\"GlitchThresholdTermination\"):\n",
    "            first_time = \"first_time=True\" in term_mode\n",
    "            if first_time:\n",
    "                params.append(TERMINATION_MODE_TRANSLATION['glitch-threshold'])\n",
    "            else:\n",
    "                assert \"first_time=False\" in term_mode\n",
    "                params.append(TERMINATION_MODE_TRANSLATION['glitch-threshold-twice'])\n",
    "        elif term_mode.startswith(\"EqOracleTermination\"):\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['oracle'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown termination mode '{term_mode}' in {result['results_file']}\")\n",
    "        \n",
    "    elif name_in_result.startswith(\"GSM\"):\n",
    "        name = GSM_NAME\n",
    "        params = []\n",
    "        if alg_kwargs[\"purge_mismatches\"] is True:\n",
    "            params.append(GSM_PARAM_TRANSLATION['purge'])\n",
    "        else:\n",
    "            params.append(GSM_PARAM_TRANSLATION['no-purge'])\n",
    "        \n",
    "        params.append(DATA_GENERATION_TRANSLATION['random-walks'])  # default is True, always on\n",
    "        \n",
    "        if alg_kwargs.get(\"eq_oracle\", None) is not None:\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['oracle'])\n",
    "        else:\n",
    "            params.append(TERMINATION_MODE_TRANSLATION['bisimilarity'])\n",
    "    else:\n",
    "        name = name_in_result\n",
    "        print(f\"Unhandled algorithm {name}\")\n",
    "        return name\n",
    "        \n",
    "    return f\"{name}({', '.join(params)})\"\n",
    "\n",
    "def postprocess_result(result):\n",
    "    result[\"algorithm_name\"] = build_algorithm_name(result)\n",
    "\n",
    "    # if \"params\" not in result[\"detailed_learning_info\"] and result[\"glitch_percent\"] == 1:\n",
    "    #     # For combined MAT results\n",
    "    #     result[\"glitch_percent\"] = 1.1\n",
    "\n",
    "    return result\n",
    "\n",
    "def filter_results(results):\n",
    "    if isinstance(results, pd.DataFrame):\n",
    "        mask = results.apply(lambda r: not remove_result_if(r), axis=1)\n",
    "        filtered_df = results[mask].copy()\n",
    "        return filtered_df.apply(postprocess_result, axis=1)\n",
    "    else:\n",
    "        return [postprocess_result(r) for r in results if not remove_result_if(r)]\n",
    "\n",
    "results = copy.deepcopy(_o_results)\n",
    "results_df = _o_results_df.copy()\n",
    "\n",
    "len_before = len(results)\n",
    "assert len_before == len(results_df)\n",
    "\n",
    "results = filter_results(results)\n",
    "results_df = filter_results(results_df)\n",
    "\n",
    "len_after = len(results)\n",
    "assert len_after == len(results_df)\n",
    "\n",
    "if len_before != len_after:\n",
    "    print(f\"Removed {len_before - len_after} results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_automata = results_df[\"original_automaton\"].unique()\n",
    "automata_sizes = results_df[\"original_automaton_size\"].unique()\n",
    "automata_inputs = results_df[\"original_automaton_num_inputs\"].unique()\n",
    "automata_outputs = results_df[\"original_automaton_num_outputs\"].unique()\n",
    "glitch_percents = results_df[\"glitch_percent\"].unique()\n",
    "unique_algorithms = results_df[\"algorithm_name\"].unique()\n",
    "\n",
    "# constellation = [\"algorithm_name\", \"glitch\"]\n",
    "# \n",
    "# for ground_truth in unique_automata:\n",
    "#     results_df[\"original_automaton\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Plot Config (automatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"default\")  # initialize to default - seaborn overrides!\n",
    "\n",
    "# color map creation\n",
    "CM_OFFSET = 2\n",
    "ALL_ALGORITHMS_IN_RESULTS = sorted(results_df[\"algorithm_name\"].unique().tolist())\n",
    "ALGORITHM_COLORS = {}\n",
    "ALGORITHM_HATCHES = {}\n",
    "for base_alg, colormap in BASE_ALGORITHM_COLORMAP.items():\n",
    "    alg_versions = [a for a in ALL_ALGORITHMS_IN_RESULTS if a.startswith(base_alg)]\n",
    "    colormap = plt.get_cmap(BASE_ALGORITHM_COLORMAP[base_alg], lut=len(alg_versions) + CM_OFFSET)\n",
    "    for i, alg_version in enumerate(alg_versions):\n",
    "        color = colormap(i + (CM_OFFSET // 2))\n",
    "        ALGORITHM_COLORS[alg_version] = color\n",
    "        ALGORITHM_HATCHES[alg_version] = BASE_ALGORITHM_HATCHES.get(base_alg, '')\n",
    "\n",
    "for i, alg in enumerate(ALL_ALGORITHMS_IN_RESULTS):\n",
    "    if alg not in ALGORITHM_COLORS:\n",
    "        ALGORITHM_COLORS[alg] = plt.get_cmap(\"Blues\", lut=len(ALL_ALGORITHMS_IN_RESULTS))(i)\n",
    "    if alg not in ALGORITHM_HATCHES:\n",
    "        ALGORITHM_HATCHES[alg] = \"///\"\n",
    "\n",
    "if STYLE == \"latex\":\n",
    "    mpl.rcParams.update({\n",
    "        \"text.usetex\": False,  # True if LaTeX is installed\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"CMU Serif\"],  # download from https://ctan.org/pkg/cm-unicode\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"font.size\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"axes.titlesize\": 14\n",
    "    })\n",
    "    \n",
    "elif STYLE == \"thesis\":\n",
    "    COLUMN_WIDTH_IN_PT = 455.24413  # latex: \\the\\columnwidth\n",
    "    COLUMN_WIDTH_IN_IN = COLUMN_WIDTH_IN_PT / 72.27\n",
    "    FIG_WIDTH = round(COLUMN_WIDTH_IN_IN, 1) # theoretically best\n",
    "\n",
    "    def get_fig_size(width_fraction: float = 1, aspect_ratio: float = 0.618):\n",
    "        fig_width = FIG_WIDTH * width_fraction\n",
    "        fig_height = FIG_WIDTH * aspect_ratio\n",
    "        return (fig_width, fig_height)\n",
    "    \n",
    "    mpl.rcParams.update({\n",
    "        # Fonts\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n",
    "        \"font.size\": 11,                # Match thesis body text\n",
    "        \"axes.titlesize\": 13,           # Section-style figure titles\n",
    "        \"axes.labelsize\": 11,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \n",
    "        # \"font.size\": 9,                \n",
    "        # \"axes.titlesize\": 11,           \n",
    "        # \"axes.labelsize\": 9,\n",
    "        # \"xtick.labelsize\": 8,\n",
    "        # \"ytick.labelsize\": 8,\n",
    "\n",
    "        # Axes & Lines\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"lines.linewidth\": 1.5,\n",
    "        \"lines.markersize\": 6,\n",
    "\n",
    "        # Legend\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 10,\n",
    "        # \"legend.fontsize\": 8,\n",
    "        # \"legend.title_fontsize\": 8,\n",
    "        \"legend.handlelength\": 1.8,\n",
    "        \"legend.handleheight\": 0.8,\n",
    "        \"legend.borderaxespad\": 0.8,\n",
    "        \"legend.borderpad\": 0.5,\n",
    "        \"legend.labelspacing\": 0.4,\n",
    "        \"legend.handletextpad\": 0.5,\n",
    "        \"legend.columnspacing\": 1.2,\n",
    "        \"legend.fancybox\": False,\n",
    "\n",
    "        # Figure\n",
    "        # \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        # \"figure.figsize\": (9, 6),  # Good for 2-column layout\n",
    "        \"figure.constrained_layout.use\": True,\n",
    "\n",
    "        # Ticks\n",
    "        \"xtick.major.size\": 4,\n",
    "        \"xtick.major.width\": 0.8,\n",
    "        \"ytick.major.size\": 4,\n",
    "        \"ytick.major.width\": 0.8,\n",
    "\n",
    "        # PDF output\n",
    "        \"pdf.fonttype\": 42,  # Ensures text remains text in PDFs\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def bar_chart_pd(\n",
    "    ax,\n",
    "    df: pd.DataFrame,\n",
    "    key: str,\n",
    "    agg_method: str,\n",
    "    group_by: list[str],\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    y_as_percentage: bool = False,\n",
    "    x_as_percentage: bool = False,\n",
    "    show_num_results_on_bar: bool = False,\n",
    "    legend: bool | str = True,\n",
    "    bar_label_fontsize: int = 10,\n",
    "    bar_label_decimal_digits: int = 2,\n",
    "    simple: bool = False,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Draws a bar chart on the given `ax`, and returns legend handles and labels.\"\"\"\n",
    "\n",
    "    pivot_df = df.groupby(by=group_by)[key].agg(agg_method).unstack().astype(float)\n",
    "    if y_as_percentage:\n",
    "        pivot_df *= 100\n",
    "\n",
    "    pivot_df.plot(kind=\"bar\", ax=ax, width=0.92, xlabel=xlabel, ylabel=ylabel, rot=0, legend=False, title=title)\n",
    "    if simple:\n",
    "        # don't do anything except plotting - no visual adjustments\n",
    "        return [], []\n",
    "\n",
    "    num_groups = len(pivot_df.index)\n",
    "    margin = 0.92 / 2  # half bar width\n",
    "    padding = 0.1  # extra space on sides\n",
    "    ax.set_xlim(-margin - padding, num_groups - 1 + margin + padding)\n",
    "\n",
    "    def percent_label(value, *args, **kwargs):\n",
    "        suffix = kwargs.get('suffix', '%')\n",
    "        return (f\"{value:.0f}\" if value % 1 == 0 else f\"{value:.1f}\") + suffix\n",
    "\n",
    "    # percentage formatting\n",
    "    if y_as_percentage:\n",
    "        for bars in ax.containers:\n",
    "            ax.bar_label(bars, labels=[percent_label(v) for v in bars.datavalues], fontsize=bar_label_fontsize)\n",
    "        ax.yaxis.set_major_formatter(mtick.FuncFormatter(percent_label))\n",
    "    else:\n",
    "        for bars in ax.containers:\n",
    "            ax.bar_label(bars, fmt=f\"%.{bar_label_decimal_digits}f\", fontsize=bar_label_fontsize)\n",
    "\n",
    "    if x_as_percentage:\n",
    "        ax.set_xticklabels([percent_label(x) for x in pivot_df.index])\n",
    "    else:\n",
    "        ax.set_xticklabels([x for x in pivot_df.index])\n",
    "        \n",
    "    # number of results on bar\n",
    "    if show_num_results_on_bar:\n",
    "        assert len(group_by) == 2, f\"Only works with exactly two group-by entries!\"\n",
    "        grouped_df = df.groupby(by=group_by)\n",
    "        result_counts = {\n",
    "            key: len(group)\n",
    "            for key, group in grouped_df\n",
    "        }\n",
    "        for bars in ax.containers:\n",
    "            col_key = bars.get_label()\n",
    "            count_labels = []\n",
    "            for i, row_key in enumerate(pivot_df.index):\n",
    "                row_key_tuple = (row_key, ) if not isinstance(row_key, tuple) else row_key\n",
    "                group_key = row_key_tuple + (col_key,)\n",
    "                count = result_counts.get(group_key, 0)\n",
    "                \n",
    "                if y_as_percentage:\n",
    "                    bar_perc = bars.datavalues[i]\n",
    "                    bar_num = int(count * bar_perc / 100)\n",
    "                    count_label = f\"n={bar_num}\\nof {count}\"\n",
    "                else:\n",
    "                    count_label = f\"n={count}\"\n",
    "                count_labels.append(count_label)\n",
    "                \n",
    "            ax.bar_label(bars, labels=count_labels, fontsize=bar_label_fontsize, label_type='center', \n",
    "                         backgroundcolor='white', bbox=dict(alpha=0.75, \n",
    "                                                            color=\"white\", \n",
    "                                                            boxstyle=\"square, pad=0.05\",\n",
    "                                                            capstyle='round'), \n",
    "                         clip_on=False,\n",
    "                         # rotation=90\n",
    "                         )\n",
    "\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # hatches and colors\n",
    "    for bars in ax.containers:\n",
    "        bar_label = bars.get_label()\n",
    "        if bar_label in ALL_ALGORITHMS_IN_RESULTS:\n",
    "            algorithm = bar_label\n",
    "            hatch = ALGORITHM_HATCHES[algorithm]\n",
    "            color = ALGORITHM_COLORS[algorithm]\n",
    "        \n",
    "            for bar in bars:\n",
    "                if hatch:\n",
    "                    bar.set_facecolor(\"none\")\n",
    "                    bar.set_hatch(hatch)\n",
    "                    bar.set_edgecolor(color)\n",
    "                    bar.set_linewidth(1.2)\n",
    "                    patch_kwargs = dict(facecolor=\"none\", edgecolor=color, hatch=hatch + (hatch[0] * 2))\n",
    "                else:\n",
    "                    bar.set_facecolor(color)\n",
    "                    patch_kwargs = dict(facecolor=color)\n",
    "                \n",
    "            if bar_label not in labels:\n",
    "                patch = mpatches.Patch(label=bar_label, **patch_kwargs)\n",
    "                handles.append(patch)\n",
    "                labels.append(algorithm)\n",
    "\n",
    "    if legend:\n",
    "        if isinstance(legend, bool):\n",
    "            legend = 'best'\n",
    "        ax.legend(handles=handles, labels=labels, title=\"Algorithm\", loc=legend)\n",
    "\n",
    "    return handles, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size settings for standalone: (9, 6), include legend in first (True), ylim 107/1.07\n",
    "# size settings for triple fig: (9, 4.5), include legend in one of the 3 figs, ylim 110/1.1\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 4.5), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"bisimilar\", agg_method=\"mean\", group_by=[X_AXIS, \"algorithm_name\"],\n",
    "    xlabel=X_AXIS_TITILE, ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=(X_AXIS == 'glitch_percent'),\n",
    "    title=\"Bisimilarity\", legend=False,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, ACCURACY_KEY, agg_method=\"mean\", group_by=[X_AXIS, \"algorithm_name\"],\n",
    "    xlabel=X_AXIS_TITILE, ylabel=f\"Mean {ACCURACY_NAME} over all results\", x_as_percentage=(X_AXIS == 'glitch_percent'),\n",
    "    y_as_percentage=False, title=f\"{ACCURACY_NAME}\".title(), #bar_label_decimal_digits=3,\n",
    "    legend='lower left',\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 110)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"Bisimilarity\", legend=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, ACCURACY_KEY, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=f\"Mean {ACCURACY_NAME} over all results\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=f\"{ACCURACY_NAME}\".title(),\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 105)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness_horizontal.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df[\"timed_out\"] == False].groupby([\"glitch_percent\", \"algorithm_name\"])[\"total_time\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "bar_chart_pd(\n",
    "    ax, results_df, ACCURACY_KEY, agg_method=\"mean\", group_by=[X_AXIS, \"algorithm_name\"],\n",
    "    xlabel=X_AXIS_TITILE, ylabel=f\"Mean {ACCURACY_NAME} over all results\", x_as_percentage=(X_AXIS == 'glitch_percent'),\n",
    "    y_as_percentage=False, title=f\"{ACCURACY_NAME}\".title(), show_num_results_on_bar=True,\n",
    "    legend=True,\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'accuracy.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=False)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"\", legend=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Percentage of timed-out results\", y_as_percentage=True,\n",
    "    title=\"\", legend=False, x_as_percentage=False, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 107)\n",
    "ax2.set_ylim(0, 107)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'timeouts_vert.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), sharey=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"\", legend=False, bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"timed_out\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Percentage of timed-out results\", y_as_percentage=True,\n",
    "    title=\"\", legend=False, x_as_percentage=False,bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "# fig.suptitle(\"Time-Outs\", y=1.25)  # nomat\n",
    "fig.suptitle(\"Time-Outs\", y=1.3)  # mat\n",
    "\n",
    "legend_columns = len(ALL_ALGORITHMS_IN_RESULTS) / 2\n",
    "if legend_columns <= 2:\n",
    "    if len(ALL_ALGORITHMS_IN_RESULTS) <= 5:\n",
    "        legend_columns = len(ALL_ALGORITHMS_IN_RESULTS)\n",
    "        \n",
    "fig.legend(\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    "    title=\"Algorithm\",\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.27), #nomat: 1.2\n",
    "    ncol=legend_columns,\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 105)\n",
    "ax2.set_ylim(0, 105)\n",
    "\n",
    "ax1.set_title(\"Per glitch percentage\", fontsize=12)\n",
    "ax2.set_title(\"Per number of states\", fontsize=12)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'timeouts.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 4.5), sharex=True)\n",
    "\n",
    "timed_out_results = results_df[results_df['timed_out']]\n",
    "\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, timed_out_results, \"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"Bisimilarity of timed-out results\", legend=False, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, timed_out_results, ACCURACY_KEY, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean Accuracy over timed-out results\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=\"Accuracy of timed-out results\",\n",
    "    legend=False, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 110)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'timed_out_correctness.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 4.5), sharex=True)\n",
    "\n",
    "not_timed_out_results = results_df[results_df['timed_out'] == False]\n",
    "\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, not_timed_out_results, \"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar non-timed-out results\", y_as_percentage=True, x_as_percentage=True,\n",
    "    title=\"Bisimilarity of non-timed-out results\", legend=False, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, not_timed_out_results, ACCURACY_KEY, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean Accuracy over non-timed-out results\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=\"Accuracy of non-timed-out results\", show_num_results_on_bar=True,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 110)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'not_timed_out_correctness.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for alg_name in not_timed_out_results[\"algorithm_name\"].unique():\n",
    "#     print(alg_name)\n",
    "#     for gp in [1,5,10]:\n",
    "#         print(f\"{gp}%\")\n",
    "#         print(len(not_timed_out_results[not_timed_out_results[\"algorithm_name\"] == alg_name][not_timed_out_results[\"glitch_percent\"] == gp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(1, 1, figsize=(9, 5), sharex=True)\n",
    "# \n",
    "# not_bisim_out_results = results_df[results_df['learned_correctly'] == False]\n",
    "# \n",
    "# bar_chart_pd(\n",
    "#     ax1, not_bisim_out_results, ACCURACY_KEY, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "#     xlabel='Glitch percentage', ylabel=\"Mean Accuracy over non-bisimilar results\", x_as_percentage=True,\n",
    "#     y_as_percentage=False, title=\"Accuracy of non-bisimilar results\",\n",
    "#     legend=True,\n",
    "# )\n",
    "# \n",
    "# ax1.set_ylim(0, 1.05)\n",
    "# \n",
    "# if SAVE_FIGURES_TO:\n",
    "#     plt.savefig(Path(SAVE_FIGURES_TO)/'not_bisim_correctness.png')\n",
    "# else:\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learned_correctly\", agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=False,\n",
    "    title=\"Bisimilarity\", legend=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, ACCURACY_KEY, agg_method=\"mean\", group_by=[\"original_automaton_size\", \"algorithm_name\"],\n",
    "    xlabel='Number of states in the ground truth model', ylabel=\"Mean Accuracy over all results\", x_as_percentage=False,\n",
    "    y_as_percentage=False, title=\"Accuracy\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "ax1.set_ylim(0, 105)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness_over_states.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=results_df,\n",
    "    x=X_AXIS,\n",
    "    y=ACCURACY_KEY,\n",
    "    hue=\"algorithm_name\",\n",
    "    # fill=False,  # this colors the whiskers, but not the boxes itself - maybe i can use this and apply hatch myself?\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    hue_order=ALL_ALGORITHMS_IN_RESULTS,\n",
    "    flierprops=dict(marker='o', markersize=3, alpha=0.3),\n",
    ")\n",
    "plt.title(\"Accuracy per Run\")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(X_AXIS_TITILE)\n",
    "\n",
    "def percent_label(label):\n",
    "    return f\"{int(float(label))}%\"\n",
    "\n",
    "if X_AXIS == \"glitch_percent\":\n",
    "    ax.set_xticklabels([percent_label(t.get_text()) for t in ax.get_xticklabels()])\n",
    "\n",
    "l = ax.legend(title=\"Algorithm\")\n",
    "\n",
    "# set hatches (& colors) # CAUTION: this might not work with other matplotlib / seaborn versions!\n",
    "patches = [p for p in ax.patches if type(p) == mpl.patches.PathPatch]\n",
    "num_groups = len(results_df[X_AXIS].unique())\n",
    "for alg_idx, alg in enumerate(ALL_ALGORITHMS_IN_RESULTS):\n",
    "    hatch = ALGORITHM_HATCHES[alg]\n",
    "    color = ALGORITHM_COLORS[alg]\n",
    "    for patch in patches[(alg_idx * num_groups):((alg_idx + 1) * num_groups)]:\n",
    "        if hatch:\n",
    "            # patch.set_fill(True)\n",
    "            patch.set_facecolor(\"none\")\n",
    "            patch.set_hatch(hatch)\n",
    "            patch.set_edgecolor(color)\n",
    "            patch.set_linewidth(1.2)\n",
    "\n",
    "# fix legend for hatches\n",
    "for lp, hatch in zip(l.get_patches(), ALGORITHM_HATCHES.values()):\n",
    "    lp.set_hatch((hatch + hatch[0] * 2))\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor('none')\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'accuracy_boxplot.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4), sharey=True)\n",
    "\n",
    "sns.boxplot(\n",
    "    data=results_df,\n",
    "    x=X_AXIS,\n",
    "    y=ACCURACY_KEY,\n",
    "    hue=\"algorithm_name\",\n",
    "    # fill=False,  # this colors the whiskers, but not the boxes itself - maybe i can use this and apply hatch myself?\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    hue_order=ALL_ALGORITHMS_IN_RESULTS,\n",
    "    flierprops=dict(marker='o', markersize=3, alpha=0.3),\n",
    "    ax=ax1\n",
    ")\n",
    "fig.suptitle(\"Accuracy per Run\")\n",
    "\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "def percent_label(label):\n",
    "    return f\"{int(float(label))}%\"\n",
    "\n",
    "# if X_AXIS == \"glitch_percent\":\n",
    "#     ax1.set_xticklabels([percent_label(t.get_text()) for t in ax1.get_xticklabels()])\n",
    "\n",
    "l = ax1.legend(title=\"Algorithm\")\n",
    "\n",
    "# set hatches (& colors) # CAUTION: this might not work with other matplotlib / seaborn versions!\n",
    "patches = [p for p in ax1.patches if type(p) == mpl.patches.PathPatch]\n",
    "num_groups = len(results_df[X_AXIS].unique())\n",
    "for alg_idx, alg in enumerate(ALL_ALGORITHMS_IN_RESULTS):\n",
    "    hatch = ALGORITHM_HATCHES[alg]\n",
    "    color = ALGORITHM_COLORS[alg]\n",
    "    for patch in patches[(alg_idx * num_groups):((alg_idx + 1) * num_groups)]:\n",
    "        if hatch:\n",
    "            # patch.set_fill(True)\n",
    "            patch.set_facecolor(\"none\")\n",
    "            patch.set_hatch(hatch)\n",
    "            patch.set_edgecolor(color)\n",
    "            patch.set_linewidth(1.2)\n",
    "\n",
    "# fix legend for hatches\n",
    "for lp, hatch in zip(l.get_patches(), ALGORITHM_HATCHES.values()):\n",
    "    lp.set_hatch((hatch + hatch[0] * 2))\n",
    "    fc = lp.get_facecolor()\n",
    "    lp.set_edgecolor(fc)\n",
    "    lp.set_facecolor('none')\n",
    "    \n",
    "sns.stripplot(\n",
    "    data=results_df,\n",
    "    x=\"algorithm_name\",\n",
    "    y=ACCURACY_KEY,\n",
    "    hue=\"algorithm_name\",\n",
    "    order=ALL_ALGORITHMS_IN_RESULTS,\n",
    "    # fill=False,  # this colors the whiskers, but not the boxes itself - maybe i can use this and apply hatch myself?\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    hue_order=ALL_ALGORITHMS_IN_RESULTS,\n",
    "    # flierprops=dict(marker='o', markersize=3, alpha=0.3),\n",
    "    ax=ax2,\n",
    "    # s=5,\n",
    "    # marker='x',\n",
    "    # dodge=True,\n",
    ")\n",
    "ax2.set_xlabel('')\n",
    "ax1.set_xticks([])\n",
    "ax2.set_xticks([])\n",
    "\n",
    "ax1.set_title(\"Boxplot\")\n",
    "ax2.set_title(\"Individual Runs\")\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "\n",
    "# Put legend into ax2 instead\n",
    "ax2.legend(handles, labels, title=\"Algorithm\", loc=\"lower left\")\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'accuracy_boxplot_and_scatter.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "sns.stripplot(\n",
    "    data=results_df,\n",
    "    x=X_AXIS,\n",
    "    y=ACCURACY_KEY,\n",
    "    hue=\"algorithm_name\",\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    size=5,\n",
    "    ax=ax,\n",
    "    # marker=\"x\"\n",
    "    # alpha=0.6,\n",
    ")\n",
    "ax.set_xlabel(X_AXIS_TITILE)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(title=\"Algorithm\")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / 'accuracy_scatterplot.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles = (\n",
    "    results_df.groupby(['algorithm_name', 'glitch_percent'])[ACCURACY_KEY]\n",
    "    .quantile([0.25, 0.5, 0.75])\n",
    "    .unstack(level=2)\n",
    "    .rename(columns={0.25: 'Q1', 0.5: 'Median', 0.75: 'Q3'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_boxplot_stats(group):\n",
    "    scores = group[ACCURACY_KEY]\n",
    "    q1 = scores.quantile(0.25)\n",
    "    q3 = scores.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = scores[scores >= q1 - 1.5 * iqr].min()\n",
    "    upper_whisker = scores[scores <= q3 + 1.5 * iqr].max()\n",
    "    outliers = scores[(scores < lower_whisker) | (scores > upper_whisker)].tolist()\n",
    "\n",
    "    return pd.Series({\n",
    "        'Q1': q1,\n",
    "        'Q3': q3,\n",
    "        'Median': scores.median(),\n",
    "        'IQR': iqr,\n",
    "        'Lower Whisker': lower_whisker,\n",
    "        'Upper Whisker': upper_whisker,\n",
    "        \"Number of Outliers\": len(outliers),\n",
    "        'Outliers': outliers\n",
    "    })\n",
    "\n",
    "# Group and apply the function\n",
    "boxplot_stats = (\n",
    "    results_df.groupby(['algorithm_name', 'glitch_percent'])\n",
    "      .apply(compute_boxplot_stats)\n",
    "      .reset_index()\n",
    ")\n",
    "boxplot_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "# handles, labels = bar_chart_pd(\n",
    "#     ax1, results_df, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "#     xlabel='Glitch percentage', ylabel=\"Mean number of steps per run\", x_as_percentage=True,\n",
    "#     title=\"Mean number of steps\", legend=True, bar_label_decimal_digits=1,\n",
    "# )\n",
    "# \n",
    "# bar_chart_pd(\n",
    "#     ax2, results_df, \"queries_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "#     xlabel='Glitch percentage', ylabel=\"Mean number of queries per run\",\n",
    "#     title=\"Mean number of queries\", legend=False, bar_label_decimal_digits=1, x_as_percentage=True,\n",
    "# )\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(9, 4), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"steps_learning\", agg_method=\"mean\", group_by=[X_AXIS, \"algorithm_name\"],\n",
    "    xlabel=X_AXIS_TITILE, ylabel=\"Mean number of steps per run\", x_as_percentage=(X_AXIS == 'glitch_percent'),\n",
    "    title=\"Mean number of steps\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "# ax1.set_ylim(0, 3600)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(9, 5), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, not_timed_out_results, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps per run in non-timed-out results\", x_as_percentage=True,\n",
    "    title=\"Mean number of steps (without timeouts)\", legend=True, bar_label_decimal_digits=1, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_without_timeouts.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"InformationPerStep\"] = results_df[ACCURACY_KEY] / results_df[\"steps_learning\"]\n",
    "results_df[\"InformationPerQuery\"] = results_df[ACCURACY_KEY] / results_df[\"queries_learning\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"InformationPerStep\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Information per step\", x_as_percentage=True,\n",
    "    title=\"Information per step\", legend=True, bar_label_decimal_digits=5,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"InformationPerQuery\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Information per query\",\n",
    "    title=\"Information per query\", legend=False, bar_label_decimal_digits=5, x_as_percentage=True,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'info_per_steps_and_queries.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relplot(df, columns: list[str]):\n",
    "    ROWS = 2\n",
    "    MANUAL_LEGEND = True\n",
    "    GLITCH_PERCENTAGE_NAME = \"Glitch percentage\"\n",
    "    SHOW_GRID = False\n",
    "    \n",
    "    df[GLITCH_PERCENTAGE_NAME] = df[\"glitch_percent\"].apply(lambda x: f\"{int(x)}%\")  # so we can simply use the column title for relplot, and its already formatted\n",
    "    col_order = columns\n",
    "    \n",
    "    kwargs = dict()\n",
    "    if MANUAL_LEGEND:\n",
    "        kwargs[\"facet_kws\"] = dict(legend_out=False)\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        df, \n",
    "        x=\"steps_learning\", y=ACCURACY_KEY, \n",
    "        col=\"algorithm_name\", col_wrap=ROWS, col_order=col_order,\n",
    "        hue=GLITCH_PERCENTAGE_NAME, style=GLITCH_PERCENTAGE_NAME, alpha=0.5, palette=[\"green\", \"blue\", \"red\"],\n",
    "        height=4, aspect=1.2, **kwargs,\n",
    "    )\n",
    "    g.figure.suptitle(\"Steps vs. Accuracy\", y=1.04)\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    g.set_axis_labels(\"Steps\", \"Accuracy\")\n",
    "    \n",
    "    if MANUAL_LEGEND:\n",
    "        # sns.move_legend(\n",
    "        #     g, loc=\"upper center\", \n",
    "        #     title=None,\n",
    "        #     bbox_to_anchor=(0.5, 1.03),  # Centered above the entire figure\n",
    "        #     bbox_transform=g.figure.transFigure,\n",
    "        #     frameon=True,\n",
    "        #     ncol=results_df[GLITCH_PERCENTAGE_NAME].nunique(),\n",
    "        # )#, edgecolor='0.8', fancybox=True)\n",
    "        \n",
    "        # remove the existing legend\n",
    "        g._legend.remove()  \n",
    "    \n",
    "        handles, labels = g.axes[0].get_legend_handles_labels()\n",
    "        \n",
    "        # Insert title into first entry as a dummy handle\n",
    "        from matplotlib.lines import Line2D\n",
    "        title_handle = Line2D([], [], linestyle='None')  # No marker, empty handle\n",
    "        handles.insert(0, title_handle)\n",
    "        labels.insert(0, f\"{GLITCH_PERCENTAGE_NAME}:\")\n",
    "        \n",
    "        # Add custom legend\n",
    "        g.figure.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.03),\n",
    "            bbox_transform=g.figure.transFigure,\n",
    "            ncol=len(labels),  # All on one row, including the title\n",
    "            frameon=True,\n",
    "            handletextpad=0.5,\n",
    "            # columnspacing=1.2,\n",
    "        )\n",
    "        \n",
    "    if SHOW_GRID:\n",
    "        for ax in g.axes.flatten():\n",
    "            ax.grid(True, which='major', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "            \n",
    "    return g\n",
    "\n",
    "\n",
    "g = relplot(results_df, columns=ALL_ALGORITHMS_IN_RESULTS)\n",
    "if SAVE_FIGURES_TO:\n",
    "    g.savefig(Path(SAVE_FIGURES_TO)/'steps_vs_accuracy.png')\n",
    "    \n",
    "# g2 = relplot(results_df, columns=[a for a in ALL_ALGORITHMS_IN_RESULTS if a.startswith(\"APMSL\")])\n",
    "# if SAVE_FIGURES_TO:\n",
    "#     g2.savefig(Path(SAVE_FIGURES_TO)/'steps_vs_accuracy_apmsl.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.scatterplot(\n",
    "    data=results_df,\n",
    "    x=\"steps_learning\",\n",
    "    y=ACCURACY_KEY,\n",
    "    hue=\"algorithm_name\",\n",
    "    style=\"glitch_percent\",\n",
    "    #size=\"queries_learning\",\n",
    "    palette=ALGORITHM_COLORS,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Accuracy vs. Steps\")\n",
    "# ax.set_xlabel(\"Steps\")\n",
    "# ax.set_ylabel(\"Accuracy\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"algorithm_name\", \"glitch_percent\", \"original_automaton\"]].drop_duplicates().values  # unique combos\n",
    "\n",
    "group_cols = [\"algorithm_name\", \"glitch_percent\", \"original_automaton\"]\n",
    "target_metric = ACCURACY_KEY\n",
    "\n",
    "divergence = results_df.groupby(group_cols)[target_metric].std().reset_index()\n",
    "divergence = divergence.rename(columns={target_metric: \"within_group_std\"})\n",
    "\n",
    "stats_per_unique_run_combo = results_df.groupby(group_cols)[target_metric].agg(\n",
    "    mean=\"mean\", std=\"std\", min=\"min\", max=\"max\",\n",
    "    range=lambda x: x.max() - x.min()\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_models_df = (\n",
    "    results_df[[\"original_automaton\", \"original_automaton_size\", \"original_automaton_num_outputs\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=[\"original_automaton_size\", \"original_automaton_num_outputs\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "unique_models_df[\"model_index\"] = np.arange(len(unique_models_df))\n",
    "model_to_int = dict(zip(unique_models_df[\"original_automaton\"], unique_models_df[\"model_index\"]))\n",
    "results_df[\"model_index\"] = results_df[\"original_automaton\"].map(model_to_int)\n",
    "GLITCH_PERCENTAGE_NAME = \"Glitch percentage\"\n",
    "\n",
    "def format_glitch(g):\n",
    "    if int(g) == g:\n",
    "        return f\"{int(g)}%\"\n",
    "    else:\n",
    "        return f\"{g:.1f}%\"\n",
    "results_df[GLITCH_PERCENTAGE_NAME] = results_df[\"glitch_percent\"].apply(format_glitch)  # so we can simply use the column title for relplot, and its already formatted\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "sizes = unique_models_df[\"original_automaton_size\"].values\n",
    "change_indices = np.where(np.diff(sizes) != 0)[0] + 0.5\n",
    "\n",
    "for i, algorithm in enumerate(ALL_ALGORITHMS_IN_RESULTS):\n",
    "    ax = axes[i]\n",
    "    subset = results_df[results_df[\"algorithm_name\"] == algorithm]\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=subset,\n",
    "        x=\"model_index\",\n",
    "        y=ACCURACY_KEY,\n",
    "        hue=GLITCH_PERCENTAGE_NAME,\n",
    "        ax=ax,\n",
    "        palette=[\"green\", \"blue\", \"red\", \"magenta\"],\n",
    "        alpha=0.6,\n",
    "        legend=(i == 0)  # only show legend once\n",
    "    )\n",
    "    \n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel(\"\")  # common x label set below\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(algorithm)\n",
    "\n",
    "    # Draw vertical lines for size changes\n",
    "    for pos in change_indices:\n",
    "        ax.axvline(x=pos, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # improve ticks for only bottom plots\n",
    "    # if i < len(ALL_ALGORITHMS_IN_RESUlTS) - 2:\n",
    "    #     ax.set_xticklabels([])\n",
    "\n",
    "# Common x-label and adjust layout  # TODO very manual...\n",
    "fig.text(0.5, -0.04, \"Model Index (sorted by number of states)\", ha='center', fontsize=12)\n",
    "# fig.text(0.04, 0.5, \"Accuracy\", va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'accuracy_per_model_scatterplot.png')\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [0, 1, 2, 30]\n",
    "fig, axes = plt.subplots(len(durations), 1, figsize=(9, 3 * len(durations)), sharex=True)\n",
    "\n",
    "for i, sec_per_step in enumerate(durations):\n",
    "    if sec_per_step == 0:\n",
    "        title = f\"Mean Actual Runtime ({sec_per_step} sec/step)\"\n",
    "    else:\n",
    "        title = f\"Mean Fictional Runtime ({sec_per_step} sec/step)\"\n",
    "        \n",
    "    col_name = f\"FictionalRuntime_{sec_per_step}Sec\"\n",
    "    results_df[col_name] = results_df[\"total_time\"] + (results_df[\"steps_learning\"] * sec_per_step)\n",
    "\n",
    "    bar_chart_pd(\n",
    "        axes[i], results_df[results_df[\"timed_out\"] == False], col_name, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=\"Mean fictional runtime (sec)\",\n",
    "        x_as_percentage=True, legend=(i == 0), bar_label_decimal_digits=0, bar_label_fontsize=9,\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'fictional_runtime.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "step_durations = np.arange(0, 31)  # from 0 to 30 seconds\n",
    "\n",
    "plot_data = []\n",
    "for sec_per_step in step_durations:\n",
    "    temp_df = results_df.copy()\n",
    "    temp_df[\"fictional_runtime\"] = temp_df[\"total_time\"] + temp_df[\"steps_learning\"] * sec_per_step\n",
    "\n",
    "    grouped = (\n",
    "        temp_df\n",
    "        .groupby([\"glitch_percent\", \"algorithm_name\"])[\"fictional_runtime\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"sec_per_step\"] = sec_per_step\n",
    "    plot_data.append(grouped)\n",
    "\n",
    "combined_df = pd.concat(plot_data)\n",
    "\n",
    "# unique_glitches = sorted(combined_df[\"glitch_percent\"].unique())\n",
    "unique_glitches = [1, 5, 10]\n",
    "n = len(unique_glitches)\n",
    "\n",
    "fig, axes = plt.subplots(n, 1, figsize=(9, 3 * n), sharex=True)\n",
    "    \n",
    "for ax, glitch in zip(axes, unique_glitches):\n",
    "    subset = combined_df[combined_df[\"glitch_percent\"] == glitch]\n",
    "    \n",
    "    for algorithm_name, group in subset.groupby(\"algorithm_name\"):\n",
    "        ax.plot(group[\"sec_per_step\"], group[\"fictional_runtime\"], label=algorithm_name, color=ALGORITHM_COLORS[algorithm_name])\n",
    "\n",
    "    ax.set_title(f\"Fictional runtime vs. per-step duration ({glitch:.0f}% glitches)\")\n",
    "    ax.set_ylabel(\"Fictional runtime (sec)\")\n",
    "    ax.grid(True)\n",
    "    ax.set_ylim(0, 100_000)\n",
    "    if ax is axes[0]:\n",
    "        ax.legend()\n",
    "\n",
    "axes[-1].set_xlabel(\"Assumed time per step (sec)\")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'fictional_runtime_linecharts_by_glitch.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(9, 5), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df[results_df[\"timed_out\"] == False], \"learning_rounds\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of learning rounds per not-timed-out run\", x_as_percentage=True,\n",
    "    title=\"Mean number of learning rounds (non-timed-out results)\", legend=True, bar_label_decimal_digits=1, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'learning_rounds_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(9, 5), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learning_rounds\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of learning rounds per run\", x_as_percentage=True,\n",
    "    title=\"Mean number of learning rounds\", legend=True, bar_label_decimal_digits=1, show_num_results_on_bar=False,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'learning_rounds.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df[\"actual_glitch_percentage\"] = results_df[\"sul_num_glitched_steps\"] / (results_df[\"steps_eq_oracle\"] + results_df[\"steps_learning\"])\n",
    "results_df[\"actual_glitch_percentage\"] = results_df[\"sul_num_glitched_steps\"] / results_df[\"steps_eq_oracle\"]\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(9, 5), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"actual_glitch_percentage\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of glitched steps per run\", x_as_percentage=True, y_as_percentage=True,\n",
    "    title=\"Mean number of glitched steps\", legend=True, bar_label_decimal_digits=1, show_num_results_on_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(9, 9), sharex=False)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"learning_rounds\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of learning rounds\", y_as_percentage=False, x_as_percentage=True,\n",
    "    title=\"\", legend=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"num_cex_queries\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of cex queries\", y_as_percentage=False,\n",
    "    title=\"\", legend=False, x_as_percentage=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax3, results_df, \"queries_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of oracle queries\", y_as_percentage=False,\n",
    "    title=\"\", legend=False, x_as_percentage=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "# ax1.set_ylim(0, 107)\n",
    "# ax2.set_ylim(0, 107)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(9, 9), sharex=False)\n",
    "non_timed_out_results = results_df[results_df[\"timed_out\"] == False]\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, non_timed_out_results, \"learning_rounds\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of learning rounds (non-timed-out results)\", y_as_percentage=False, x_as_percentage=True,\n",
    "    title=\"\", legend=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, non_timed_out_results, \"num_cex_queries\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of cex queries (non-timed-out results)\", y_as_percentage=False,\n",
    "    title=\"\", legend=False, x_as_percentage=True, #bar_label_fontsize=6\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax3, non_timed_out_results, \"queries_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Number of oracle queries (non-timed-out results)\", y_as_percentage=False,\n",
    "    title=\"\", legend=False, x_as_percentage=True, #bar_label_fontsize=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "    \"counterexamples_investigated\": \"counterexamples investigated\",\n",
    "    \"counterexamples_invalidated\": \"counterexamples invalidated\",\n",
    "    \"steps_eq_oracle\": \"steps in oracle\",\n",
    "    # \"no_majority_trace_found\": \"No majority trace found\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(keys), figsize=(4* len(keys), 4), sharex=True)\n",
    "\n",
    "for i, (key, title) in enumerate(keys.items()):\n",
    "    bar_chart_pd(\n",
    "        axes[i], results_df, key, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=f\"mean {title} per run\", title=title.title(), y_as_percentage=False, x_as_percentage=True,\n",
    "        legend=False, show_num_results_on_bar=False, bar_label_decimal_digits=1, #bar_label_fontsize=6,\n",
    "    )\n",
    "    axes[i].ticklabel_format(style='plain', axis='y')\n",
    "    \n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'cex_and_steps.png')\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "    \"bisimilar\": \"bisimilar models\",\n",
    "    # \"no_majority_trace_found\": \"no majority trace found\",\n",
    "    \"aborted_after_no_majority_trace\": \"aborts\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(keys), figsize=(4* len(keys), 4), sharey=True)\n",
    "\n",
    "for i, (key, title) in enumerate(keys.items()):\n",
    "    bar_chart_pd(\n",
    "        axes[i], results_df, key, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=f\"percentage of runs\", title=title.title(), y_as_percentage=True, x_as_percentage=True,\n",
    "        legend=False, show_num_results_on_bar=False, bar_label_decimal_digits=2, #bar_label_fontsize=6,\n",
    "    )\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'bisim_nomaj_abort.png')\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df[\"bisimilar\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"bisimilar\",\n",
    "    \"learning_rounds\",\n",
    "    \"steps_learning\",\n",
    "    \"steps_eq_oracle\",\n",
    "    \"find_cex_called\",\n",
    "    \"counterexamples_investigated\",\n",
    "    \"counterexamples_validated\",\n",
    "    \"counterexamples_invalidated\",\n",
    "    \"no_majority_trace_found\",\n",
    "    \"continued_after_no_majority_trace\",\n",
    "    \"aborted_after_no_majority_trace\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(keys), 1, figsize=(8, 3* len(keys)), sharex=True)\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    bar_chart_pd(\n",
    "        axes[i], results_df, key, agg_method=\"sum\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=key, title=key, y_as_percentage=False, x_as_percentage=True,\n",
    "        legend=(i==0), show_num_results_on_bar=True, #bar_label_fontsize=6,\n",
    "    )\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'relevant_keys.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"steps_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps in the EQ oracle per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of EQ oracle steps\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"queries_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of queries in the EQ oracle per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of EQ oracle queries\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_eq_vs_learning_seperate.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_timed_out_results = results_df[results_df['timed_out'] == False]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, not_timed_out_results, \"steps_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps in the EQ oracle per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of EQ oracle steps (non-timed-out results)\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, not_timed_out_results, \"queries_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of queries in the EQ oracle per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of EQ oracle queries (non-timed-out results)\", legend=True, bar_label_decimal_digits=1,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_eq_vs_learning_seperate_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True)\n",
    "\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of learning steps\", legend=True, bar_label_decimal_digits=0,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"steps_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps in the EQ oracle per round\", x_as_percentage=True,\n",
    "    title=\"Mean number of EQ oracle steps\", legend=False, bar_label_decimal_digits=0,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import pandas as pd\n",
    "\n",
    "def stacked_bar_chart(\n",
    "    ax,\n",
    "    df: pd.DataFrame,\n",
    "    group_by: list[str] = [\"glitch_percent\", \"algorithm_name\"],\n",
    "    values: list[str] = [\"steps_learning\", \"steps_eq_oracle\"],\n",
    "    agg_method: str = \"mean\",\n",
    "    xlabel: str = \"Glitch percentage\",\n",
    "    ylabel: str = \"Mean number of steps per round\",\n",
    "    title: str = \"Mean number of total steps\",\n",
    "    bar_width: float = 0.18,\n",
    "    legend: bool = True,\n",
    "    show_num_results_on_bar: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws a stacked bar chart: for each glitch percentage, each algorithm\n",
    "    gets one bar consisting of stacked segments (learning + eq oracle).\n",
    "    \"\"\"\n",
    "    # Aggregate\n",
    "    pivot_df = df.groupby(group_by)[values].agg(agg_method).reset_index()\n",
    "    pivot_df = pivot_df.pivot(\n",
    "        index=group_by[0], columns=group_by[1], values=values\n",
    "    )\n",
    "\n",
    "    x = range(len(pivot_df.index))\n",
    "    algorithms = pivot_df[values[0]].columns\n",
    "    offsets = {alg: i * bar_width for i, alg in enumerate(algorithms)}\n",
    "    \n",
    "    NO_HATCH_TAG = \"__nohatch__\"\n",
    "\n",
    "    # Plot bars\n",
    "    for alg in algorithms:\n",
    "        learning = pivot_df[values[0]][alg].values\n",
    "        eq_oracle = pivot_df[values[1]][alg].values\n",
    "        pos = [i + offsets[alg] for i in x]\n",
    "\n",
    "        ax.bar(\n",
    "            pos, learning, bar_width,\n",
    "            #edgecolor=ALGORITHM_COLORS[alg], hatch=ALGORITHM_HATCHES[alg], facecolor=\"none\",\n",
    "            label=alg\n",
    "        )\n",
    "        ax.bar(\n",
    "            pos, eq_oracle, bar_width,\n",
    "            bottom=learning,\n",
    "            #edgecolor=ALGORITHM_COLORS[alg], hatch=ALGORITHM_HATCHES[alg], facecolor=\"none\",\n",
    "            label=f\"{NO_HATCH_TAG}{alg}\"\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xticks([i + bar_width * (len(offsets) - 1) / 2 for i in x])\n",
    "    ax.set_xticklabels([f\"{p}%\" for p in pivot_df.index])\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # bar labels\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(\n",
    "            bars, \n",
    "            fmt=f\"%.0f\",\n",
    "            fontsize=9,\n",
    "            # backgroundcolor='white', \n",
    "            # bbox=dict(\n",
    "            #     alpha=0.75, color=\"white\", \n",
    "            #     boxstyle=\"square, pad=0.05\",\n",
    "            #     capstyle='round'\n",
    "            # ),\n",
    "            # padding=1.5,\n",
    "        )\n",
    "\n",
    "    # number of results on bar\n",
    "    if show_num_results_on_bar:\n",
    "        assert len(group_by) == 2, f\"Only works with exactly two group-by entries!\"\n",
    "        grouped_df = df.groupby(by=group_by)\n",
    "        result_counts = {\n",
    "            key: len(group)\n",
    "            for key, group in grouped_df\n",
    "        }\n",
    "        for bars in ax.containers:\n",
    "            col_key = bars.get_label()\n",
    "            if NO_HATCH_TAG in col_key:\n",
    "                col_key = col_key[len(NO_HATCH_TAG):]\n",
    "            else:\n",
    "                continue  #  only put labels on the non-hatched (upper) bars\n",
    "\n",
    "            count_labels = []\n",
    "            for i, row_key in enumerate(pivot_df.index):\n",
    "                row_key_tuple = (row_key, ) if not isinstance(row_key, tuple) else row_key\n",
    "                group_key = row_key_tuple + (col_key,)\n",
    "                count = result_counts.get(group_key, 0)\n",
    "\n",
    "                count_label = f\"n={count}\"\n",
    "                count_labels.append(count_label)\n",
    "\n",
    "            ax.bar_label(bars, labels=count_labels, fontsize=9, label_type='center',\n",
    "                         backgroundcolor='white', bbox=dict(alpha=0.75,\n",
    "                                                            color=\"white\",\n",
    "                                                            boxstyle=\"square, pad=0.05\",\n",
    "                                                            capstyle='round'),\n",
    "                         clip_on=False,\n",
    "                         # rotation=90\n",
    "                         )\n",
    "        \n",
    "    # hatches and colors\n",
    "    handles = []\n",
    "    labels = []\n",
    "    alpha = 0.2  # alpha for the non-hatched bars\n",
    "    for bars in ax.containers:\n",
    "        algorithm = bars.get_label()\n",
    "        if NO_HATCH_TAG in algorithm:\n",
    "            algorithm = algorithm[len(NO_HATCH_TAG):]\n",
    "            hatch = None\n",
    "        else:\n",
    "            hatch = ALGORITHM_HATCHES[algorithm]\n",
    "            \n",
    "        color = ALGORITHM_COLORS[algorithm]\n",
    "    \n",
    "        for bar in bars:\n",
    "            if hatch:\n",
    "                bar.set_facecolor(\"none\")\n",
    "                bar.set_hatch(hatch)\n",
    "            else:\n",
    "                # bar.set_facecolor(\"none\")\n",
    "                # bar.set_hatch(hatch)\n",
    "                bar.set_facecolor((color[0], color[1], color[2], alpha))\n",
    "                # bar.set_alpha(0.2)\n",
    "            bar.set_edgecolor(color)\n",
    "            bar.set_linewidth(1.2)\n",
    "                \n",
    "        if algorithm not in labels and hatch:\n",
    "            patch_hatch = mpatches.Patch(facecolor=\"none\", edgecolor=color, hatch=hatch + (hatch[0] * 2))\n",
    "            patch_alpha = mpatches.Patch(facecolor=(color[0], color[1], color[2], alpha), edgecolor=color)\n",
    "            handles.append((patch_hatch, patch_alpha))\n",
    "            labels.append(algorithm)\n",
    "                \n",
    "    # algorithms legend\n",
    "    if legend:\n",
    "        alg_legend = ax.legend(handles=handles, labels=labels, title=\"Algorithm\", loc=\"upper left\",\n",
    "                  handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "                  handlelength=2 * mpl.rcParams[\"legend.handlelength\"])\n",
    "\n",
    "        hatch_patch = mpatches.Patch(facecolor=\"none\", edgecolor=\"gray\", hatch=\"////\", label=\"Learning steps\")\n",
    "        alpha_patch = mpatches.Patch(facecolor=(0.5, 0.5, 0.5, 0.2), edgecolor=\"gray\", label=\"EQ oracle steps\")\n",
    "        ax.add_artist(alg_legend)\n",
    "\n",
    "        fig = ax.figure\n",
    "        fig.canvas.draw()\n",
    "        fig.set_constrained_layout_pads()   # force layout recompute\n",
    "        alg_legend_bbox = alg_legend.get_window_extent().transformed(fig.transFigure.inverted())\n",
    "        # alg_legend_bbox = alg_legend.get_window_extent().transformed(fig.transFigure.inverted())\n",
    "\n",
    "        # steps legend\n",
    "        steps_legend = ax.legend(\n",
    "            handles=[hatch_patch, alpha_patch],\n",
    "            title=\"Step type\",\n",
    "            loc=\"upper left\",  # anchor point = upper-left corner of legend\n",
    "            bbox_to_anchor=(alg_legend_bbox.x0, alg_legend_bbox.y0 - 0.02),  # subtract small y to move below\n",
    "            fontsize=10,\n",
    "            bbox_transform=fig.transFigure\n",
    "        )\n",
    "\n",
    "        ax.add_artist(steps_legend)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "stacked_bar_chart(ax, results_df)\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_eq_vs_learning_combined.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "not_timed_out_results = results_df[results_df['timed_out'] == False]\n",
    "\n",
    "stacked_bar_chart(ax, not_timed_out_results, show_num_results_on_bar=True)\n",
    "ax.set_title(ax.get_title() + \" (non-timed-out results)\")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_eq_vs_learning_combined_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import pandas as pd\n",
    "\n",
    "def stacked_bar_chart_single_legend(\n",
    "    ax,\n",
    "    df: pd.DataFrame,\n",
    "    group_by: list[str] = [\"glitch_percent\", \"algorithm_name\"],\n",
    "    values: list[str] = [\"steps_learning\", \"steps_eq_oracle\"],\n",
    "    agg_method: str = \"mean\",\n",
    "    xlabel: str = \"Glitch percentage\",\n",
    "    ylabel: str = \"Mean number of steps per round\",\n",
    "    title: str = \"Mean number of total steps\",\n",
    "    bar_width: float = 0.18,\n",
    "    legend: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Draws a stacked bar chart: for each glitch percentage, each algorithm\n",
    "    gets one bar consisting of stacked segments (learning + eq oracle).\n",
    "    \"\"\"\n",
    "    # Aggregate\n",
    "    pivot_df = df.groupby(group_by)[values].agg(agg_method).reset_index()\n",
    "    pivot_df = pivot_df.pivot(\n",
    "        index=group_by[0], columns=group_by[1], values=values\n",
    "    )\n",
    "\n",
    "    x = range(len(pivot_df.index))\n",
    "    algorithms = pivot_df[values[0]].columns\n",
    "    offsets = {alg: i * bar_width for i, alg in enumerate(algorithms)}\n",
    "\n",
    "    NO_HATCH_TAG = \"__nohatch__\"\n",
    "\n",
    "    # Plot bars\n",
    "    for alg in algorithms:\n",
    "        learning = pivot_df[values[0]][alg].values\n",
    "        eq_oracle = pivot_df[values[1]][alg].values\n",
    "        pos = [i + offsets[alg] for i in x]\n",
    "\n",
    "        ax.bar(\n",
    "            pos, learning, bar_width,\n",
    "            #edgecolor=ALGORITHM_COLORS[alg], hatch=ALGORITHM_HATCHES[alg], facecolor=\"none\",\n",
    "            label=alg\n",
    "        )\n",
    "        ax.bar(\n",
    "            pos, eq_oracle, bar_width,\n",
    "            bottom=learning,\n",
    "            #edgecolor=ALGORITHM_COLORS[alg], hatch=ALGORITHM_HATCHES[alg], facecolor=\"none\",\n",
    "            label=f\"{NO_HATCH_TAG}{alg}\"\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xticks([i + bar_width * (len(offsets) - 1) / 2 for i in x])\n",
    "    ax.set_xticklabels([f\"{p}%\" for p in pivot_df.index])\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # bar labels\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(\n",
    "            bars,\n",
    "            fmt=f\"%.0f\",\n",
    "            fontsize=9,\n",
    "            # backgroundcolor='white',\n",
    "            # bbox=dict(\n",
    "            #     alpha=0.75, color=\"white\",\n",
    "            #     boxstyle=\"square, pad=0.05\",\n",
    "            #     capstyle='round'\n",
    "            # ),\n",
    "            # padding=1.5,\n",
    "        )\n",
    "\n",
    "    # hatches and colors\n",
    "    alg_handles = []\n",
    "    alg_labels = []\n",
    "    alpha = 0.2  # alpha for the non-hatched bars\n",
    "    for bars in ax.containers:\n",
    "        algorithm = bars.get_label()\n",
    "        if NO_HATCH_TAG in algorithm:\n",
    "            algorithm = algorithm[len(NO_HATCH_TAG):]\n",
    "            hatch = None\n",
    "        else:\n",
    "            hatch = ALGORITHM_HATCHES[algorithm]\n",
    "\n",
    "        color = ALGORITHM_COLORS[algorithm]\n",
    "\n",
    "        for bar in bars:\n",
    "            if hatch:\n",
    "                bar.set_facecolor(\"none\")\n",
    "                bar.set_hatch(hatch)\n",
    "            else:\n",
    "                # bar.set_facecolor(\"none\")\n",
    "                # bar.set_hatch(hatch)\n",
    "                bar.set_facecolor((color[0], color[1], color[2], alpha))\n",
    "                # bar.set_alpha(0.2)\n",
    "            bar.set_edgecolor(color)\n",
    "            bar.set_linewidth(1.2)\n",
    "\n",
    "        if algorithm not in alg_labels and hatch:\n",
    "            patch_hatch = mpatches.Patch(facecolor=\"none\", edgecolor=color, hatch=hatch + (hatch[0] * 2))\n",
    "            patch_alpha = mpatches.Patch(facecolor=(color[0], color[1], color[2], alpha), edgecolor=color)\n",
    "            alg_handles.append((patch_hatch, patch_alpha))\n",
    "            alg_labels.append(algorithm)\n",
    "\n",
    "    # algorithms legend\n",
    "    if legend:\n",
    "        # alg_legend = ax.legend(handles=alg_handles, labels=alg_labels, title=\"Algorithm\", loc=\"upper left\",\n",
    "        #                        handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "        #                        handlelength=2 * mpl.rcParams[\"legend.handlelength\"])\n",
    "\n",
    "        hatch_patch = mpatches.Patch(facecolor=\"none\", edgecolor=\"gray\", hatch=\"////\", label=\"Learning steps\")\n",
    "        alpha_patch = mpatches.Patch(facecolor=(0.5, 0.5, 0.5, 0.2), edgecolor=\"gray\", label=\"EQ oracle steps\")\n",
    "\n",
    "        step_handles = [hatch_patch, alpha_patch]\n",
    "        step_labels = [h.get_label() for h in step_handles]\n",
    "\n",
    "        # --- Section headers (dummy, invisible) ---\n",
    "        alg_header = mpatches.Patch(alpha=0, linewidth=0, label=\"Algorithm\")\n",
    "        step_header = mpatches.Patch(alpha=0, linewidth=0, label=\"Step type\")\n",
    "\n",
    "        # --- Combine everything ---\n",
    "        combined_handles = [alg_header] + alg_handles + [step_header] + step_handles\n",
    "        combined_labels = [\"Algorithm\"] + alg_labels + [\"Step type\"] + step_labels\n",
    "\n",
    "        ax.legend(\n",
    "            handles=combined_handles,\n",
    "            labels=combined_labels,\n",
    "            loc=\"best\",\n",
    "            handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "            handlelength=2 * mpl.rcParams[\"legend.handlelength\"],\n",
    "        )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "stacked_bar_chart_single_legend(ax, results_df)\n",
    "if False:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'----steps_eq_vs_learning_combined.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_all = True\n",
    "include_timed_out = False\n",
    "include_non_timed_out = True\n",
    "\n",
    "include_learning_steps = True\n",
    "include_eq_steps = False\n",
    "\n",
    "dfs_to_plot = []\n",
    "if include_all:\n",
    "    dfs_to_plot.append(results_df)\n",
    "if include_timed_out:\n",
    "    timed_out_results = results_df[results_df['timed_out'] == True]\n",
    "    dfs_to_plot.append(timed_out_results)\n",
    "if include_non_timed_out:\n",
    "    not_timed_out_results = results_df[results_df['timed_out'] == False]\n",
    "    dfs_to_plot.append(not_timed_out_results)\n",
    "\n",
    "fig, axes = plt.subplots((2 if include_eq_steps else 1), len(dfs_to_plot), figsize=((12), (8 if include_eq_steps else 6)) , sharex=True, sharey='row')\n",
    "\n",
    "for i, res in enumerate(dfs_to_plot):\n",
    "    if include_eq_steps:\n",
    "        upper_ax = axes[0][i]\n",
    "        lower_ax = axes[1][i]\n",
    "    else:\n",
    "        upper_ax = axes[i]\n",
    "        lower_ax = axes[i]\n",
    "\n",
    "    if res is results_df:\n",
    "        name = \"all\"\n",
    "    elif res is not_timed_out_results:\n",
    "        name = \"non-timed-out\"\n",
    "    elif res is timed_out_results:\n",
    "        name = \"timed-out\"\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    bar_chart_pd(\n",
    "        upper_ax, res, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        xlabel='Glitch percentage', ylabel=\"Mean number of steps per round\", x_as_percentage=True,\n",
    "        title=f\"Mean learning steps ({name} results)\", legend=(i==0), bar_label_decimal_digits=0, show_num_results_on_bar=(i!=0)\n",
    "    )\n",
    "\n",
    "    if include_eq_steps:\n",
    "        bar_chart_pd(\n",
    "            lower_ax, res, \"queries_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "            xlabel='Glitch percentage', ylabel=\"Mean number of steps in the EQ oracle per round\", x_as_percentage=True,\n",
    "            title=f\"Mean EQ oracle steps ({name} results)\", legend=False, bar_label_decimal_digits=0, show_num_results_on_bar=(i!=0)\n",
    "        )\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_all_vs_timeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "def bar_charts_grid(\n",
    "        col_dataframes: dict[str, pd.DataFrame],  # columns: one col per dataframe\n",
    "        rows: list[dict[str, Any]],  # rows: one row per kwargs dict\n",
    "        charting_func: Callable = bar_chart_pd,\n",
    "        single_plot_height: int = 4,\n",
    "        total_width: int = 12,\n",
    "        legend_in: tuple[int, int] = (0,0)\n",
    "):\n",
    "\n",
    "    nrows = len(rows)\n",
    "    ncols = len(col_dataframes)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(total_width, single_plot_height * nrows) , sharex=True, sharey='row')\n",
    "\n",
    "    for row_i, row_kwargs in enumerate(rows):\n",
    "        if nrows == 1:\n",
    "            row_ax = axes\n",
    "        else:\n",
    "            row_ax = axes[row_i]\n",
    "\n",
    "        r_title = row_kwargs.pop('title')\n",
    "\n",
    "        for col_i, (df_name, df) in enumerate(col_dataframes.items()):\n",
    "            if ncols == 1:\n",
    "                ax = row_ax\n",
    "            else:\n",
    "                ax = row_ax[col_i]\n",
    "\n",
    "            title = r_title.format(df_name=df_name)\n",
    "            charting_func(\n",
    "                ax, df, legend=(legend_in == (row_i, col_i)), title=title, **row_kwargs\n",
    "            )\n",
    "            if (row_i, col_i) != (0,0):\n",
    "                ax.set_ylabel(None)\n",
    "\n",
    "\n",
    "not_timed_out_results = results_df[results_df['timed_out'] == False]\n",
    "timed_out_results = results_df[results_df['timed_out'] == True]\n",
    "\n",
    "bar_charts_grid(\n",
    "    col_dataframes={\n",
    "        \"all\": results_df, \"not-timed-out\": not_timed_out_results,\n",
    "    },\n",
    "    rows=[\n",
    "        dict(\n",
    "            key=\"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "            xlabel='Glitch percentage', ylabel=\"Mean number of steps per round\", x_as_percentage=True,\n",
    "            title=\"Mean learning steps ({df_name} results)\", bar_label_decimal_digits=0, show_num_results_on_bar=True\n",
    "        ),\n",
    "        # dict(\n",
    "        #     key=\"steps_eq_oracle\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "        #     xlabel='Glitch percentage', ylabel=\"Mean number of steps per round\", x_as_percentage=True,\n",
    "        #     title=\"Mean learning steps ({df_name} results)\", bar_label_decimal_digits=0, show_num_results_on_bar=True\n",
    "        # ),\n",
    "    ],\n",
    "    total_width=14,\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_all_vs_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_charts_grid(\n",
    "    col_dataframes={\n",
    "        \"all\": results_df, \"not-timed-out\": not_timed_out_results,\n",
    "    },\n",
    "    rows=[\n",
    "        dict(\n",
    "            key=\"learned_correctly\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "            xlabel='Glitch percentage', ylabel=\"Percentage of bisimilar results\", y_as_percentage=True, x_as_percentage=True,\n",
    "            title=\"Bisimilarity ({df_name} results)\", show_num_results_on_bar=True,\n",
    "        ),\n",
    "        dict(\n",
    "            key=ACCURACY_KEY, agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "            xlabel='Glitch percentage', ylabel=f\"Mean {ACCURACY_NAME} over all results\", x_as_percentage=True,\n",
    "            y_as_percentage=False, title=ACCURACY_NAME + \" ({df_name} results)\", show_num_results_on_bar=True, bar_label_decimal_digits=3,\n",
    "        )\n",
    "    ],\n",
    "    total_width=10,\n",
    "    single_plot_height=4\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'correctness_all_vs_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_charts_grid(\n",
    "    col_dataframes={\n",
    "        \"all\": results_df, \"not-timed-out\": not_timed_out_results,\n",
    "    },\n",
    "    charting_func=stacked_bar_chart,\n",
    "    rows=[\n",
    "        dict(\n",
    "            title=\"Total mean steps ({df_name} results)\",\n",
    "        ),\n",
    "    ],\n",
    "    total_width=14,\n",
    "    single_plot_height=5\n",
    ")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / 'steps_stacked_all_vs_notimeout.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def scatter_plot1(result: dict):\n",
    "    rounds = result[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "\n",
    "    # flatten into a df\n",
    "    data = []\n",
    "    for round_id, round_info in rounds.items():\n",
    "        num_traces = round_info[\"num_traces\"]\n",
    "        for model_size, pmsat_info in round_info[\"pmsat_info\"].items():\n",
    "            data.append({\n",
    "                \"round\": int(round_id),\n",
    "                \"model_size\": int(model_size),\n",
    "                \"solve_time\": pmsat_info[\"solve_time\"],\n",
    "                \"num_traces\": num_traces,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"round\",\n",
    "        y=\"model_size\",\n",
    "        size=\"solve_time\",\n",
    "        sizes=(50, 500),\n",
    "        legend=\"brief\"\n",
    "    )\n",
    "\n",
    "    # Add text labels (num_traces) on top of dots\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(\n",
    "            row[\"round\"], row[\"model_size\"],\n",
    "            str(row[\"num_traces\"]),\n",
    "            ha=\"center\", va=\"center\", color=\"black\", fontsize=9, weight=\"bold\"\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Learning Progress: {result['algorithm_name']}\")\n",
    "    ax.set_xlabel(\"Learning Round\")\n",
    "    ax.set_ylabel(\"Model Size\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "scatter_plot1(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def scatterplot_2(result, ax=None):\n",
    "    if 'learning_rounds' in result['detailed_learning_info']:\n",
    "        rounds = result[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "    else:\n",
    "        rounds = result[\"detailed_learning_info\"]\n",
    "\n",
    "    # flatten into df\n",
    "    data = []\n",
    "    for round_id, round_info in rounds.items():\n",
    "        num_traces = round_info[\"num_traces\"]\n",
    "        for model_size, pmsat_info in round_info[\"pmsat_info\"].items():\n",
    "            data.append({\n",
    "                \"round\": int(round_id),\n",
    "                \"model_size\": int(model_size),\n",
    "                \"solve_time\": pmsat_info[\"solve_time\"],\n",
    "                \"num_traces\": num_traces,\n",
    "                \"timed_out\": pmsat_info[\"timed_out\"],\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    ax = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"round\",\n",
    "        y=\"solve_time\",\n",
    "        size=\"num_traces\",\n",
    "        hue=\"num_traces\",\n",
    "        # hue=\"timed_out\",\n",
    "        # palette={False: \"C0\", True: \"red\"},\n",
    "        palette=BASE_ALGORITHM_COLORMAP[APMSL_NAME] + \"_r\",\n",
    "        sizes=(100, 1000),\n",
    "        legend=\"brief\",\n",
    "        alpha=0.6,\n",
    "        # markers=\"O\"\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Add labels (model size) on top of dots\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(\n",
    "            row[\"round\"], row[\"solve_time\"],\n",
    "            f\"n={int(row['model_size'])}\",\n",
    "            ha=\"center\", va=\"center\", color=\"white\", fontsize=9, weight=\"bold\", #alpha=0.5\n",
    "        )\n",
    "        \n",
    "    # Customize legend title for sizes\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles, labels=labels, title=\"Number of traces\", loc=\"best\")\n",
    "\n",
    "    ax.set_title(f\"Learning Progress: {result['algorithm_name']}\")\n",
    "    ax.set_xlabel(\"Learning Round\")\n",
    "    ax.set_ylabel(\"Solve Time (s)\")\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "scatterplot_2(results[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def scatterplot_grid(results, ncols=2, figsize_per_plot=(6, 5), **subplots_kwargs):\n",
    "    nplots = len(results)\n",
    "    nrows = math.ceil(nplots / ncols)\n",
    "    \n",
    "    # Compute global min/max of num_traces\n",
    "    all_traces = []\n",
    "    for result in results:\n",
    "        if 'learning_rounds' in result['detailed_learning_info']:\n",
    "            rounds = result[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "        else:\n",
    "            rounds = result[\"detailed_learning_info\"]\n",
    "        for round_info in rounds.values():\n",
    "            all_traces.append(round_info[\"num_traces\"])\n",
    "    min_traces, max_traces = min(all_traces), max(all_traces)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(figsize_per_plot[0]*ncols, figsize_per_plot[1]*nrows), **subplots_kwargs)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        scatterplot_2(result, ax=axes[i])#, size_range=(min_traces, max_traces))\n",
    "\n",
    "    # remove unused axes\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "scatterplot_grid(results, ncols=2, sharey='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def solve_time_plot(results: list[dict], **scatterplot_kwargs):\n",
    "    # Flatten the data\n",
    "    records = []\n",
    "    for res in results:\n",
    "        if not res[\"algorithm_name\"].startswith(\"APMSL\"):\n",
    "            continue\n",
    "        algo_name = res[\"algorithm_name\"]\n",
    "        if \"learning_rounds\" in res[\"detailed_learning_info\"]:\n",
    "            learning_rounds = res[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "        else:\n",
    "            learning_rounds = res[\"detailed_learning_info\"]\n",
    "        \n",
    "        for round_num, round_info in learning_rounds.items():\n",
    "            num_traces = round_info[\"num_traces\"]\n",
    "            if \"traces_used_to_learn\" in round_info:\n",
    "                traces = round_info[\"traces_used_to_learn\"]\n",
    "                num_steps = 0\n",
    "                for trace in traces:\n",
    "                    num_steps += len(trace) - 1\n",
    "            else:\n",
    "                num_steps = -1\n",
    "                \n",
    "            for model_size, pmsat in round_info[\"pmsat_info\"].items():\n",
    "                solve_time = pmsat[\"solve_time\"]\n",
    "                records.append({\n",
    "                    \"algorithm\": algo_name,\n",
    "                    \"learning_round\": int(round_num),\n",
    "                    \"num_traces\": num_traces,\n",
    "                    \"num_steps\": num_steps,\n",
    "                    \"num_vars\": pmsat[\"num_vars\"],\n",
    "                    \"num_hard\": pmsat[\"num_hard\"],\n",
    "                    \"num_soft\": pmsat[\"num_soft\"],\n",
    "                    \"model_size\": int(model_size),\n",
    "                    \"solve_time\": solve_time\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"num_steps\",\n",
    "        y=\"solve_time\",\n",
    "        size=\"model_size\",\n",
    "        hue=\"model_size\",\n",
    "        sizes=(10,10),\n",
    "        alpha=0.6,\n",
    "        **scatterplot_kwargs,\n",
    "    )\n",
    "    # plt.yscale('log')\n",
    "    # plt.xscale('log')\n",
    "    \n",
    "    # sns.boxplot(\n",
    "    #     data=df,\n",
    "    #     y=\"solve_time\",\n",
    "    #     x=\"model_size\",\n",
    "    # )\n",
    "#     \n",
    "solve_time_plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_solve_times_df(results: list[dict]):\n",
    "    records = []\n",
    "    for res in results:\n",
    "        if not res[\"algorithm_name\"].startswith(\"APMSL\"):\n",
    "            continue\n",
    "        algo_name = res[\"algorithm_name\"]\n",
    "        if \"learning_rounds\" in res[\"detailed_learning_info\"]:\n",
    "            learning_rounds = res[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "        else:\n",
    "            learning_rounds = res[\"detailed_learning_info\"]\n",
    "\n",
    "        for round_num, round_info in learning_rounds.items():\n",
    "            num_traces = round_info[\"num_traces\"]\n",
    "            if \"traces_used_to_learn\" in round_info:\n",
    "                traces = round_info[\"traces_used_to_learn\"]\n",
    "                num_steps = 0\n",
    "                for trace in traces:\n",
    "                    num_steps += len(trace) - 1\n",
    "            else:\n",
    "                num_steps = -1\n",
    "\n",
    "            for model_size, pmsat in round_info[\"pmsat_info\"].items():\n",
    "                records.append({\n",
    "                    \"algorithm_name\": algo_name,\n",
    "                    \"learning_round\": int(round_num),\n",
    "                    \"actual_glitch_percent\": res[\"glitch_percent\"],\n",
    "                    \"found_glitch_percent\": pmsat[\"percent_glitches\"],\n",
    "                    \"num_traces\": num_traces,\n",
    "                    \"num_steps\": num_steps,\n",
    "                    \"num_vars\": pmsat[\"num_vars\"],\n",
    "                    \"num_hard\": pmsat[\"num_hard\"],\n",
    "                    \"num_soft\": pmsat[\"num_soft\"],\n",
    "                    \"model_size\": int(model_size),\n",
    "                    \"solve_time\": pmsat[\"solve_time\"],\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "solve_times_df = build_solve_times_df(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "\n",
    "def format_seconds(x, pos):\n",
    "    if x < 0.1:\n",
    "        f = f\"{x:.2f}s\"\n",
    "    elif int(x) <= 0:\n",
    "        f = f\"{x:.1f}s\"\n",
    "    elif x < 60:\n",
    "        f =  f\"{int(x)}s\"\n",
    "    elif x < 3600:\n",
    "        f = f\"{int(x // 60)}m {int(x % 60)}s\"\n",
    "    else:\n",
    "        f = f\"{int(x // 3600)}h {int((x % 3600) // 60)}m\"\n",
    "\n",
    "    return f\n",
    "\n",
    "def solve_time_histogram(df, ax=None):\n",
    "    ax = sns.histplot(df[\"solve_time\"], bins=100, log_scale=True, ax=ax)\n",
    "    # plt.gca().xaxis.set_major_formatter(FuncFormatter(format_seconds))\n",
    "    ax.set_xlabel(\"Solve Time (s)\", labelpad=10)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "    # Secondary x-axis for human-readable ticks\n",
    "    secax = ax.secondary_xaxis('bottom')\n",
    "\n",
    "    def format_seconds_underneath(*args, **kwargs):\n",
    "        return f\"\\n({format_seconds(*args, **kwargs)})\"\n",
    "\n",
    "    secax.xaxis.set_major_formatter(FuncFormatter(format_seconds_underneath))\n",
    "    return ax\n",
    "\n",
    "ax = solve_time_histogram(solve_times_df)\n",
    "ax.set_title(\"Solve Times\")\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / 'solve_times_histogram.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 8\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,3))\n",
    "\n",
    "solve_time_histogram(solve_times_df[solve_times_df[\"model_size\"] <= SPLIT], ax1)\n",
    "solve_time_histogram(solve_times_df[solve_times_df[\"model_size\"] > SPLIT], ax2)\n",
    "\n",
    "ax1.set_title(f\"Solve Times for Models <= {SPLIT} states\")\n",
    "ax2.set_title(f\"Solve Times for Models > {SPLIT} states\")\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / f'solve_times_histograms_{SPLIT}states.png')\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    data=solve_times_df,\n",
    "    x=\"model_size\",\n",
    "    y=\"solve_time\",\n",
    ")\n",
    "ax.set_title(\"Mean Solve Time per Model Size\")\n",
    "ax.set_xlabel(\"Model Size\")\n",
    "ax.set_ylabel(\"Solve Time in Minutes\", labelpad=20)\n",
    "\n",
    "ax.set_yticks([0, 8*60, 16*60, 24*60, 32*60, 40*60])\n",
    "\n",
    "def format_to_min(x, pos):\n",
    "    return x // 60\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(format_to_min))\n",
    "\n",
    "# secax = ax.secondary_yaxis('left')\n",
    "#\n",
    "# def format_seconds_underneath(*args, **kwargs):\n",
    "#     return f\"\\n\\n({format_seconds(*args, **kwargs)})\"\n",
    "\n",
    "# secax.yaxis.set_major_formatter(FuncFormatter(format_seconds_underneath))\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / f'mean_solve_time_linechart.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    data=solve_times_df,\n",
    "    x=\"found_glitch_percent\",\n",
    "    y=\"solve_time\",\n",
    ")\n",
    "ax.set_title(\"Mean Solve Time per Glitch Percentage\")\n",
    "ax.set_xlabel(\"Glitch Percentage\")\n",
    "ax.set_ylabel(\"Solve Time in Minutes\", labelpad=20)\n",
    "\n",
    "# ax.set_yticks([0, 8*60, 16*60, 24*60, 32*60, 40*60])\n",
    "\n",
    "# def format_to_min(x, pos):\n",
    "#     return x // 60\n",
    "# ax.yaxis.set_major_formatter(FuncFormatter(format_to_min))\n",
    "\n",
    "# secax = ax.secondary_yaxis('left')\n",
    "#\n",
    "# def format_seconds_underneath(*args, **kwargs):\n",
    "#     return f\"\\n\\n({format_seconds(*args, **kwargs)})\"\n",
    "\n",
    "# secax.yaxis.set_major_formatter(FuncFormatter(format_seconds_underneath))\n",
    "# if SAVE_FIGURES_TO:\n",
    "#     plt.savefig(Path(SAVE_FIGURES_TO) / f'mean_solve_time_linechart_glitch_percent.png')\n",
    "# else:\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_times_df.groupby(\"actual_glitch_percent\")[\"solve_time\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_seconds = [0, 1, 60, 30*60, 60*60, 90*60, 110*60]\n",
    "\n",
    "for sec in interesting_seconds:\n",
    "    num_solves_over_sec = len(solve_times_df[solve_times_df[\"solve_time\"] > sec])\n",
    "    print(f\"{sec}: {num_solves_over_sec} ({num_solves_over_sec / len(solve_times_df) * 100:.2f}%)\")\n",
    "\n",
    "len(solve_times_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def scatterplot_of_single_result(result, ax=None):\n",
    "    if 'learning_rounds' in result['detailed_learning_info']:\n",
    "        rounds = result[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "    else:\n",
    "        rounds = result[\"detailed_learning_info\"]\n",
    "\n",
    "    # flatten into df\n",
    "    data = []\n",
    "    for round_num, round_info in rounds.items():\n",
    "        num_traces = round_info[\"num_traces\"]\n",
    "\n",
    "        if \"traces_used_to_learn\" in round_info:\n",
    "            traces = round_info[\"traces_used_to_learn\"]\n",
    "            num_steps = 0\n",
    "            for trace in traces:\n",
    "                num_steps += len(trace) - 1\n",
    "        else:\n",
    "            num_steps = -1\n",
    "\n",
    "        for model_size, pmsat in round_info[\"pmsat_info\"].items():\n",
    "            data.append({\n",
    "                \"round\": int(round_num),\n",
    "                \"num_traces\": num_traces,\n",
    "                \"num_steps\": num_steps,\n",
    "                \"num_vars\": pmsat[\"num_vars\"],\n",
    "                \"num_hard\": pmsat[\"num_hard\"],\n",
    "                \"num_soft\": pmsat[\"num_soft\"],\n",
    "                \"model_size\": int(model_size),\n",
    "                \"solve_time\": pmsat[\"solve_time\"],\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"round\",\n",
    "        y=\"solve_time\",\n",
    "        size=\"num_steps\",\n",
    "        hue=\"num_steps\",\n",
    "        palette=BASE_ALGORITHM_COLORMAP[APMSL_NAME] + \"_r\",\n",
    "        sizes=(100, 1000),\n",
    "        legend=\"brief\",\n",
    "        alpha=0.6,\n",
    "        # markers=\"O\"\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Add labels (model size) on top of dots\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(\n",
    "            row[\"round\"], row[\"solve_time\"],\n",
    "            f\"n={int(row['model_size'])}\",\n",
    "            ha=\"center\", va=\"center\", color=\"white\", fontsize=9, weight=\"bold\", #alpha=0.5\n",
    "        )\n",
    "\n",
    "    # Customize legend title for sizes\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles, labels=labels, title=\"Number of steps\", loc=\"upper left\")\n",
    "\n",
    "    ax.set_title(f\"Learning Progress: {result['algorithm_name']}\")\n",
    "    ax.set_xlabel(\"Learning Round\")\n",
    "    ax.set_ylabel(\"Solve Time (s)\")\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "scatterplot_of_single_result(results[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def scatterplot_of_single_result(result, ax):\n",
    "    if 'learning_rounds' in result['detailed_learning_info']:\n",
    "        rounds = result[\"detailed_learning_info\"][\"learning_rounds\"]\n",
    "    else:\n",
    "        rounds = result[\"detailed_learning_info\"]\n",
    "\n",
    "    # flatten into df\n",
    "    data = []\n",
    "    for round_num, round_info in rounds.items():\n",
    "        num_traces = round_info[\"num_traces\"]\n",
    "\n",
    "        if \"traces_used_to_learn\" in round_info:\n",
    "            traces = round_info[\"traces_used_to_learn\"]\n",
    "            num_steps = 0\n",
    "            for trace in traces:\n",
    "                num_steps += len(trace) - 1\n",
    "        else:\n",
    "            num_steps = -1\n",
    "\n",
    "        for model_size, pmsat in round_info[\"pmsat_info\"].items():\n",
    "            data.append({\n",
    "                \"round\": int(round_num),\n",
    "                \"num_traces\": num_traces,\n",
    "                \"num_steps\": num_steps,\n",
    "                \"num_vars\": pmsat[\"num_vars\"],\n",
    "                \"num_hard\": pmsat[\"num_hard\"],\n",
    "                \"num_soft\": pmsat[\"num_soft\"],\n",
    "                \"model_size\": int(model_size),\n",
    "                \"solve_time\": pmsat[\"solve_time\"],\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"round\",\n",
    "        y=\"solve_time\",\n",
    "        size=\"num_steps\",\n",
    "        hue=\"num_steps\",\n",
    "        palette=\"flare\",\n",
    "        sizes=(10, 350),\n",
    "        legend=\"brief\",\n",
    "        # alpha=0.6,\n",
    "        # markers=\"O\"\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Add labels (model size) on top of dots\n",
    "    for _, row in df.iterrows():\n",
    "        ax.text(\n",
    "            row[\"round\"], row[\"solve_time\"],\n",
    "            f\"n={int(row['model_size'])}\",\n",
    "            ha=\"center\", va=\"center\", color=\"white\", fontsize=8, weight=\"bold\", #alpha=0.5\n",
    "        )\n",
    "\n",
    "    # Customize legend title for sizes\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles, labels=labels, title=\"Number of steps\", loc=\"upper left\", fontsize=9)\n",
    "\n",
    "    ax.set_title(f\"{result['algorithm_name']}\\nlearning a {result['original_automaton_size']}-state ground truth model\",\n",
    "                 fontsize=10)\n",
    "    ax.set_xlabel(\"Learning Round\", fontsize=9)\n",
    "    ax.set_ylabel(\"Solve Time (s)\", fontsize=9)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "def grid_of_result_scatterplots(results, ncols=2, figsize_per_plot=(6, 5), **subplots_kwargs):\n",
    "    nplots = len(results)\n",
    "    nrows = math.ceil(nplots / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows),\n",
    "                             **subplots_kwargs)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        scatterplot_of_single_result(result, ax=axes[i])  #, size_range=(min_traces, max_traces))\n",
    "\n",
    "    # remove unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "results_to_plot = []\n",
    "\n",
    "row_algs = [\"APMSL(ic, rw, term-impr)\", \"APMSL(ic, rw, replay, repro, term-impr)\"]\n",
    "col_num_states = [6,7,8]\n",
    "\n",
    "for alg_name in row_algs:\n",
    "    for num_states in col_num_states:\n",
    "        # Find the result matching this algorithm and automaton size\n",
    "        matching_results = [\n",
    "            r for r in results\n",
    "            if r[\"algorithm_name\"] == alg_name \n",
    "               and r[\"original_automaton_size\"] == num_states\n",
    "               and r[\"timed_out\"] is True\n",
    "               and r[\"learning_rounds\"] > 7\n",
    "        ]\n",
    "        if matching_results:\n",
    "            results_to_plot.append(matching_results[1])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "grid_of_result_scatterplots(results_to_plot, ncols=len(col_num_states), sharey=False, figsize_per_plot=(3,3))\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / f'selected_example_runs.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_chart_over_learning_rounds(results: list[dict], key: Key, only_if=lambda res: True, stay_at_last_val=lambda val: False,\n",
    "                                    get_from_full_result: bool = False, colorcode_by_alg: bool = True):\n",
    "    algs = get_algorithm_names(results)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = {a: plt.colormaps['tab10'](i) for i, a in enumerate(algs)}\n",
    "    legend_labels = set()\n",
    "\n",
    "    def add_line(result, color, label):\n",
    "        learning_info = result['detailed_learning_info']\n",
    "        steps = sorted(int(step) for step in learning_info.keys())\n",
    "        if not get_from_full_result:\n",
    "            values = [get_val(learning_info[str(step)], key, default=-1) for step in steps]\n",
    "        else:\n",
    "            values = [get_val(result, key, default=-1, callable_kwargs=dict(step=step)) for step in steps]\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            if stay_at_last_val(values[i]) and i > 0:\n",
    "                values[i] = values[i-1]\n",
    "\n",
    "        kwargs = {}\n",
    "        if label not in legend_labels:\n",
    "            kwargs[\"label\"] = label\n",
    "            legend_labels.add(label)\n",
    "        if colorcode_by_alg:\n",
    "            kwargs[\"color\"] = color\n",
    "\n",
    "        lines = plt.plot(steps, values, alpha=0.7, **kwargs)\n",
    "        plt.scatter(steps[-1], values[-1], alpha=0.7, marker='*', color=lines[0].get_color())\n",
    "\n",
    "    for result in results:\n",
    "        if not only_if(result):\n",
    "            continue\n",
    "        alg = result[\"algorithm_name\"]\n",
    "        if alg in algs:\n",
    "            add_line(result, color=colors[alg], label=alg)\n",
    "\n",
    "    plt.xlabel(\"learning round\")\n",
    "    pretty_key = get_pretty_key(key)\n",
    "    plt.ylabel(pretty_key)\n",
    "    plt.title(f\"{pretty_key} across learning rounds\")\n",
    "    plt.legend(title=\"Algorithm\", loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aalpy.utils import load_automaton_from_file\n",
    "from aalpy import MooreMachine\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "def transition_statistics(result: dict,\n",
    "                          transition_coverage_frequency_threshold = 1,\n",
    "                          transition_coverage_frequency_threshold_mode = 'absolute'):\n",
    "    transitions = TracedMooreSUL.flatten_transitions_dict(result[\"sul_transitions\"])\n",
    "    \n",
    "    original_automaton_file: str = result[\"original_automaton\"]\n",
    "    original_automaton: MooreMachine = load_automaton_from_file(original_automaton_file, \"moore\")\n",
    "\n",
    "    glitched_freqs = []\n",
    "    dominant_freqs = []\n",
    "    for (s1, l, s2), trans_count in transitions.items():\n",
    "        s1 = original_automaton.get_state_by_id(s1)\n",
    "        s2 = original_automaton.get_state_by_id(s2)\n",
    "        \n",
    "        if l not in s1.transitions:\n",
    "            # JSON serialization made our int-transitions into strings - revert\n",
    "            l = int(l)\n",
    "            \n",
    "        if s1.transitions[l] != s2:\n",
    "            glitched_freqs.append(trans_count)\n",
    "        else:\n",
    "            dominant_freqs.append(trans_count)\n",
    "\n",
    "    total_num_glitches = sum(g for g in glitched_freqs)\n",
    "    total_num_dominant = sum(d for d in dominant_freqs)\n",
    "    total_num_steps = total_num_glitches + total_num_dominant\n",
    "    # assert total_num_steps == result[\"steps_learning\"], f\"number of steps does not match: {total_num_steps=} | {result['steps_learning']=} | {result['results_file']=}\" # TODO un-comment\n",
    "\n",
    "    glitch_percentage = total_num_glitches / total_num_steps * 100\n",
    "\n",
    "    possible_transitions = 0\n",
    "    taken_transitions = 0\n",
    "\n",
    "    if transition_coverage_frequency_threshold_mode == 'absolute':\n",
    "        threshold = transition_coverage_frequency_threshold\n",
    "    elif transition_coverage_frequency_threshold_mode == 'relative':\n",
    "        threshold = total_num_steps * transition_coverage_frequency_threshold\n",
    "    else:\n",
    "        raise NotImplementedError(transition_coverage_frequency_threshold_mode)\n",
    "\n",
    "    for s1 in original_automaton.states:\n",
    "        for l, s2 in s1.transitions.items():\n",
    "            possible_transitions += 1\n",
    "\n",
    "            if (s1.state_id, l, s2.state_id) not in transitions:\n",
    "                # see above; JSON serialization\n",
    "                l = str(l)\n",
    "            \n",
    "            if transitions[(s1.state_id, l, s2.state_id)] >= threshold:\n",
    "                taken_transitions += 1\n",
    "\n",
    "    return {\n",
    "        \"glitched_freqs\": glitched_freqs,\n",
    "        \"dominant_freqs\": dominant_freqs,\n",
    "\n",
    "        \"glitch_percentage\": glitch_percentage,\n",
    "\n",
    "        \"total_num_steps\": total_num_steps,\n",
    "        \"total_num_glitches\": total_num_glitches,\n",
    "        \"total_num_dominant\": total_num_dominant,\n",
    "\n",
    "        \"max_glitched_freq\": max(glitched_freqs),\n",
    "        \"mean_glitched_freq\": np.mean(glitched_freqs),\n",
    "        \"median_glitched_freq\": np.median(glitched_freqs),\n",
    "\n",
    "        \"min_dominant_freq\": min(dominant_freqs),\n",
    "        \"max_dominant_freq\": max(dominant_freqs),\n",
    "        \"mean_dominant_freq\": np.mean(dominant_freqs),\n",
    "        \"median_dominant_freq\": np.median(dominant_freqs),\n",
    "\n",
    "        \"transition_coverage\": taken_transitions / possible_transitions,\n",
    "    }\n",
    "\n",
    "def transition_stat(key: str):\n",
    "    def fun(res):\n",
    "        return transition_statistics(res)[key]\n",
    "    fun.__name__ = key\n",
    "    return fun\n",
    "\n",
    "def n_frequent_transition_coverage(threshold: int):\n",
    "    def fun(res):\n",
    "        return transition_statistics(res, threshold,\n",
    "                                     transition_coverage_frequency_threshold_mode='absolute')[\"transition_coverage\"]\n",
    "    fun.__name__ = f\"{threshold}-frequent transition coverage\"\n",
    "    return fun\n",
    "\n",
    "def relative_transition_coverage(threshold: float):\n",
    "    def fun(res):\n",
    "        return transition_statistics(res, threshold/100,\n",
    "                                     transition_coverage_frequency_threshold_mode='relative')[\"transition_coverage\"]\n",
    "    fun.__name__ = f\"relative transition coverage (at least {threshold}% of steps)\"\n",
    "    return fun\n",
    "\n",
    "from charts import bar_chart_per_algorithm\n",
    "bar_chart_per_algorithm(results, transition_stat(\"transition_coverage\"), stat_method=np.mean)\n",
    "# bar_chart_per_algorithm(results, n_frequent_transition_coverage(4), stat_method=np.mean)\n",
    "# bar_chart_per_algorithm(results, relative_transition_coverage(2), stat_method=np.median)\n",
    "# scatterplot_per_alg(results, x_key=transition_stat(\"min_dominant_freq\"), y_key=transition_stat(\"max_glitched_freq\"))\n",
    "\n",
    "def plot_transition_coverage_over_threshold(results: list[dict], mode: str, start: float, end: float, step: float, stat_method=np.mean,\n",
    "                                            ax=None, legend: bool = True):\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    def add_labels(line):\n",
    "        x, y = line.get_data()\n",
    "        for x, y in zip(x, y):\n",
    "            ax.annotate(f'{y:.2f}', (x, y))\n",
    "\n",
    "    marker_per_alg = {alg: marker for alg, marker in\n",
    "                      zip(ALL_ALGORITHMS_IN_RESULTS, ['o', '*', 'p', 'h', 'd', 'v', '8', 's', 'P', 'H', 'X', '1', '2', '3' ])}\n",
    "\n",
    "    x_axis = np.arange(start, end, step)\n",
    "    for a in ALL_ALGORITHMS_IN_RESULTS:\n",
    "        values = []\n",
    "        for threshold in x_axis:\n",
    "            fun = n_frequent_transition_coverage(threshold) if mode == 'absolute' else relative_transition_coverage(threshold)\n",
    "            values_at_threshold = [fun(res) for res in results\n",
    "                                   if res['algorithm_name'] == a]\n",
    "            values.append(stat_method(values_at_threshold))\n",
    "        line, = ax.plot(x_axis, values, marker_per_alg[a], label=a, color=ALGORITHM_COLORS[a])\n",
    "        add_labels(line)\n",
    "\n",
    "    mode_str = \"N-frequent transition coverage\" if mode == 'absolute' else \"Relative transition coverage (at least N %)\"\n",
    "    ax.set_xlabel(f\"N (minimum {'number' if mode=='absolute' else 'percentage'} of steps for a transition to count as covered)\")\n",
    "    ax.set_ylabel('Transition coverage')\n",
    "    ax.set_title(f\"{mode_str} ({stat_method.__name__} over results)\")\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "plot_transition_coverage_over_threshold(results, 'absolute', 1, 15, 1, stat_method=np.mean, ax=ax1, legend=True)\n",
    "plot_transition_coverage_over_threshold(results, 'relative', 1, 5.5, 0.5, stat_method=np.mean, ax=ax2, legend=False)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / 'transition_coverage.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aalpy.utils import load_automaton_from_file\n",
    "from aalpy import MooreMachine\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "def get_number_of_times_each_valid_transition_was_taken(result: dict):\n",
    "    transitions = TracedMooreSUL.flatten_transitions_dict(result[\"sul_transitions\"])\n",
    "    \n",
    "    original_automaton_file: str = result[\"original_automaton\"]\n",
    "    original_automaton: MooreMachine = load_automaton_from_file(original_automaton_file, \"moore\")\n",
    "\n",
    "    transition_frequencies = []\n",
    "    for s1 in original_automaton.states:\n",
    "        for l, s2 in s1.transitions.items():\n",
    "            \n",
    "            if (s1.state_id, l, s2.state_id) not in transitions:\n",
    "                # JSON serialization made our int-transitions into strings - revert\n",
    "                l = str(l)\n",
    "                \n",
    "            times_this_transition_was_taken = transitions[(s1.state_id, l, s2.state_id)]\n",
    "            transition_frequencies.append(times_this_transition_was_taken)\n",
    "    \n",
    "    return transition_frequencies\n",
    "\n",
    "for result in results:\n",
    "    result[\"transition_frequencies\"] = get_number_of_times_each_valid_transition_was_taken(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "algos = defaultdict(list)\n",
    "\n",
    "for r in results:\n",
    "    algos[r[\"algorithm_name\"]].append(r[\"transition_frequencies\"])\n",
    "    \n",
    "for algo, runs in algos.items():\n",
    "    algos[algo] = [np.array(run) for run in runs]\n",
    "    \n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for algo, runs in algos.items():\n",
    "    all_counts = np.concatenate(runs)  # flatten across runs\n",
    "    data.append(all_counts)\n",
    "    labels.append(algo)\n",
    "\n",
    "plt.violinplot(data, showmeans=True)\n",
    "plt.xticks(range(1, len(labels)+1), labels)\n",
    "plt.ylabel(\"Transition usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = {algo: [np.sum(run > 3) for run in runs] for algo, runs in algos.items()}\n",
    "\n",
    "plt.boxplot(coverage.values(), tick_labels=coverage.keys())\n",
    "plt.ylabel(\"Transitions covered\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, runs in algos.items():\n",
    "    stacked = np.vstack(runs)  # shape (num_runs, num_transitions)\n",
    "    mean_usage = stacked.mean(axis=0)\n",
    "    plt.plot(sorted(mean_usage, reverse=True), label=algo)\n",
    "\n",
    "plt.xlabel(\"Transition rank (most to least used)\")\n",
    "plt.ylabel(\"Average times taken\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "entropy_scores = {algo: [entropy(run / run.sum()) for run in runs] for algo, runs in algos.items()}\n",
    "\n",
    "plt.boxplot(entropy_scores.values(), labels=entropy_scores.keys())\n",
    "plt.ylabel(\"Usage entropy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    for algorithm in ALL_ALGORITHMS_IN_RESULTS:\n",
    "        print(algorithm)\n",
    "        for i in range(5):\n",
    "            print(f\"{i}: {(np.array(result['transition_frequencies']) > i).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"transition_frequencies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aalpy.utils import load_automaton_from_file\n",
    "from adjustText import adjust_text\n",
    "\n",
    "def get_number_of_times_each_valid_transition_was_taken(result: dict):\n",
    "    transitions = TracedMooreSUL.flatten_transitions_dict(result[\"sul_transitions\"])\n",
    "    \n",
    "    original_automaton_file: str = result[\"original_automaton\"]\n",
    "    original_automaton: MooreMachine = load_automaton_from_file(original_automaton_file, \"moore\")\n",
    "\n",
    "    transition_frequencies = []\n",
    "    for s1 in original_automaton.states:\n",
    "        for l, s2 in s1.transitions.items():\n",
    "            \n",
    "            if (s1.state_id, l, s2.state_id) not in transitions:\n",
    "                # JSON serialization made our int-transitions into strings - revert\n",
    "                l = str(l)\n",
    "                \n",
    "            times_this_transition_was_taken = transitions[(s1.state_id, l, s2.state_id)]\n",
    "            transition_frequencies.append(times_this_transition_was_taken)\n",
    "    \n",
    "    return transition_frequencies\n",
    "\n",
    "def num_transitions_taken_at_least_n_times(result: dict, n: int):\n",
    "    return sum(1 for freq in result[\"transition_frequencies\"] if freq >= n)\n",
    "\n",
    "def transitions_taken_linechart(results: list[dict], ax, color, marker, label,\n",
    "                                start=1, end=15, step=1, agg_method = np.mean):\n",
    "    x_axis = np.arange(start, end, step)\n",
    "    values = []\n",
    "    for n in x_axis:\n",
    "        counts = []\n",
    "        possibles = []\n",
    "        for result in results:\n",
    "            counts.append(sum(1 for freq in result[\"transition_frequencies\"] if freq >= n))\n",
    "            possibles.append(len(result[\"transition_frequencies\"]))\n",
    "        values.append(sum(counts) / sum(possibles) * 100)\n",
    "        \n",
    "    def add_labels(line):\n",
    "        x, y = line.get_data()\n",
    "        for xi, yi in zip(x, y):\n",
    "            ax.annotate(f'{yi:.1f}%', (xi+1, yi+1.5), fontsize=9, ha='center')\n",
    "\n",
    "    line, = ax.plot(x_axis, values, marker, label=label, color=color)\n",
    "    # add_labels(line)\n",
    "    \n",
    "def relative_transitions_taken_linechart(results: list[dict], ax, color, marker, label,\n",
    "                                         start=1, end=15, step=1, agg_method = np.mean):\n",
    "    x_axis = np.arange(start, end, step)\n",
    "    values = []\n",
    "    for n in x_axis:\n",
    "        counts = []\n",
    "        possibles = []\n",
    "        for result in results:\n",
    "            total_amount_of_transitions_taken = sum(f for f in result[\"transition_frequencies\"])\n",
    "            \n",
    "            num_transitions_taken_more_often_than_threshold = 0\n",
    "            for freq in result['transition_frequencies']:\n",
    "                relative_freq = freq / total_amount_of_transitions_taken            \n",
    "                if relative_freq >= (n/100):\n",
    "                    num_transitions_taken_more_often_than_threshold += 1\n",
    "            \n",
    "            counts.append(num_transitions_taken_more_often_than_threshold)\n",
    "            possibles.append(len(result[\"transition_frequencies\"]))\n",
    "        values.append(sum(counts) / sum(possibles) * 100)\n",
    "        \n",
    "    def add_labels(line):\n",
    "        x, y = line.get_data()\n",
    "        for xi, yi in zip(x, y):\n",
    "            ax.annotate(f'{yi:.1f}%', (xi+1, yi+1.5), fontsize=9, ha='center')\n",
    "\n",
    "    line, = ax.plot(x_axis, values, marker, label=label, color=color)\n",
    "    # add_labels(line)\n",
    "    \n",
    "def transition_frequency_linechart(results: list[dict]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
    "    marker_per_alg = {alg: marker for alg, marker in zip(ALL_ALGORITHMS_IN_RESULTS, ['o', '*', 'p', 'h', 'd', 'v', '8', 's', 'P', 'H', 'X', '1', '2', '3' ])}\n",
    "\n",
    "    for alg in ALL_ALGORITHMS_IN_RESULTS:\n",
    "        alg_results = [r for r in results if r[\"algorithm_name\"] == alg]\n",
    "        transitions_taken_linechart(\n",
    "            results=alg_results,\n",
    "            ax=ax1,\n",
    "            color=ALGORITHM_COLORS[alg],\n",
    "            marker=marker_per_alg[alg],\n",
    "            label=alg,\n",
    "            start=0,\n",
    "            end=100,\n",
    "        )\n",
    "        \n",
    "        relative_transitions_taken_linechart(\n",
    "            results=alg_results,\n",
    "            ax=ax2,\n",
    "            color=ALGORITHM_COLORS[alg],\n",
    "            marker=marker_per_alg[alg],\n",
    "            label=alg,\n",
    "            # start=0,\n",
    "            # end=5,\n",
    "            step=0.5,\n",
    "            start=0,\n",
    "            end=25,\n",
    "        )\n",
    "    \n",
    "    ax1.grid(True)\n",
    "    ax1.set_xlabel(\"N\")\n",
    "    ax1.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: f\"{int(x)}%\"))\n",
    "    ax1.set_ylabel(\"% of transitions taken at least N times\")\n",
    "    ax1.set_ylim(-2, 102)\n",
    "    # ax1.set_xticks([0,2,4,6,8,10,12,14])\n",
    "    ax1.legend(title=\"Algorithm\")\n",
    "    \n",
    "    ax2.grid(True)\n",
    "    ax2.set_xlabel(\"P\")\n",
    "    ax2.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: f\"{int(x)}%\"))\n",
    "    ax2.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: f\"{int(x)}%\"))\n",
    "    ax2.set_ylabel(\"% of transitions taken at least P % of total transitions\")\n",
    "    # ax2.legend(title=\"Algorithm\")\n",
    "    ax2.set_ylim(-2, 102)\n",
    "\n",
    "\n",
    "    \n",
    "for result in results:\n",
    "    result[\"transition_frequencies\"] = get_number_of_times_each_valid_transition_was_taken(result)\n",
    "\n",
    "transition_frequency_linechart(results)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO) / f'transition_frequencies.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"num_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "marker_per_alg = {alg: marker for alg, marker in zip(ALL_ALGORITHMS_IN_RESULTS, ['o', '*', 'p', 'h', 'd', 'v', '8', 's', 'P', 'H', 'X', '1', '2', '3' ])}\n",
    "\n",
    "alg = \"APMSL(ic, replay, repro, term-impr)\"\n",
    "alg_results = [r for r in results if r[\"algorithm_name\"] == alg]\n",
    "\n",
    "# transitions_taken_linechart(\n",
    "#     results=alg_results,\n",
    "#     ax=ax1,\n",
    "#     color=ALGORITHM_COLORS[alg],\n",
    "#     marker=marker_per_alg[alg],\n",
    "#     label=alg,\n",
    "#     start=0,\n",
    "#     end=100,\n",
    "# )\n",
    "relative_transitions_taken_linechart(\n",
    "    results=alg_results,\n",
    "    ax=ax,\n",
    "    color=ALGORITHM_COLORS[alg],\n",
    "    marker=marker_per_alg[alg],\n",
    "    label=alg,\n",
    "    # start=0,\n",
    "    # end=5,\n",
    "    step=1,\n",
    "    start=0,\n",
    "    end=25,\n",
    ")\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"P\")\n",
    "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: f\"{int(x)}%\"))\n",
    "ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, pos: f\"{int(x)}%\"))\n",
    "ax.set_ylabel(\"% of transitions taken at least P % of total transitions\")\n",
    "# x2.legend(title=\"Algorithm\")\n",
    "ax.set_ylim(-2, 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4), sharex=True)\n",
    "handles, labels = bar_chart_pd(\n",
    "    ax1, results_df, \"steps_learning\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=\"Mean number of steps per run\", y_as_percentage=False, x_as_percentage=True,\n",
    "    title=\"Mean number of steps\", legend=True, show_num_results_on_bar=True,\n",
    ")\n",
    "\n",
    "bar_chart_pd(\n",
    "    ax2, results_df, \"learning_rounds\", agg_method=\"mean\", group_by=[\"glitch_percent\", \"algorithm_name\"],\n",
    "    xlabel='Glitch percentage', ylabel=f\"Mean number of learning rounds per run\", x_as_percentage=True,\n",
    "    y_as_percentage=False, title=\"Mean number of learning rounds\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# ax1.set_ylim(0, 105)\n",
    "# ax2.set_ylim(0, 1.05)\n",
    "\n",
    "if SAVE_FIGURES_TO:\n",
    "    plt.savefig(Path(SAVE_FIGURES_TO)/'steps_and_rounds.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def format_as_is(val):\n",
    "    return val\n",
    "\n",
    "def format_time(secs):\n",
    "    return secs\n",
    "\n",
    "def format_bool(val):\n",
    "    return \"Yes\" if val else \"No\"\n",
    "\n",
    "def format_float(val):\n",
    "    return f\"{val:.2f}\"\n",
    "\n",
    "\n",
    "def table_of_all_runs(df):\n",
    "    columns = [\n",
    "        (\"Algorithm\", \"algorithm_name\", format_as_is),\n",
    "        (\"Learning Rounds\",\"learning_rounds\",  format_as_is),\n",
    "        (\"Time (s)\",\"total_time\",  format_time),\n",
    "        (\"Timed-Out\", \"timed_out\",  format_bool),\n",
    "        (\"Bisimilar\", \"bisimilar\", format_bool),\n",
    "        (\"Accuracy\", \"Accuracy\", format_float),\n",
    "        (\"Learning Steps\", \"steps_learning\", format_as_is),\n",
    "        (\"Oracle Steps\", \"steps_eq_oracle\", format_as_is)\n",
    "    ]\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [title for title, *_ in columns]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        table_row = []\n",
    "        for title, key, formatter in columns:\n",
    "            table_row.append(\n",
    "                formatter(row[key])\n",
    "            )\n",
    "        table.add_row(table_row)\n",
    "\n",
    "    table.align = \"l\"\n",
    "    table.border = True\n",
    "    table.header = True\n",
    "\n",
    "    return table\n",
    "\n",
    "table = table_of_all_runs(results_df)\n",
    "print(table)\n",
    "# print(table.get_latex_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_rounds_with_ic(row, with_ic=True):\n",
    "    if not row[\"algorithm_name\"].startswith(\"APMSL\"):\n",
    "        return 0 if with_ic else row[\"learning_rounds\"]\n",
    "\n",
    "    learning_info = row[\"detailed_learning_info\"]\n",
    "    if \"params\" in learning_info:\n",
    "        learning_info = learning_info[\"learning_rounds\"]\n",
    "\n",
    "    c = 0\n",
    "    for round_id, round_info in learning_info.items():\n",
    "        if \"num_additional_traces_preprocessing_input_completeness\" not in round_info:\n",
    "            assert row[\"timed_out\"] and int(round_id) == row[\"learning_rounds\"]\n",
    "\n",
    "        if round_info.get(\"num_additional_traces_preprocessing_input_completeness\", 0) > 0:\n",
    "            assert \"heuristic_scores\" not in round_info\n",
    "            if with_ic:\n",
    "                c += 1\n",
    "        else:\n",
    "            assert \"heuristic_scores\" in round_info\n",
    "            if not with_ic:\n",
    "                c += 1\n",
    "    return c\n",
    "\n",
    "\n",
    "def calculate_num_rounds_without_ic(row):\n",
    "    return calculate_num_rounds_with_ic(row, False)\n",
    "\n",
    "def add_learning_rounds_breakdown_to_df(df):\n",
    "    df[\"rounds_with_ic\"] = df.apply(calculate_num_rounds_with_ic, axis=1)\n",
    "    df[\"rounds_without_ic\"] = df.apply(calculate_num_rounds_without_ic, axis=1)\n",
    "    assert (df[\"learning_rounds\"] == df[\"rounds_with_ic\"] + df[\"rounds_without_ic\"]).all(), df[(\"learning_rounds\", \"rounds_with_ic\", \"rounds_without_ic\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def format_as_is(val):\n",
    "    return val\n",
    "\n",
    "def format_time(secs):\n",
    "    secs = int(secs)\n",
    "\n",
    "    # break into days, hours, minutes, seconds\n",
    "    minutes, sec = divmod(secs, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "\n",
    "    return f\"{hours:02d}:{minutes:02d}:{sec:02d}\"\n",
    "\n",
    "def format_bool(val):\n",
    "    return \"Yes\" if val else \"No\"\n",
    "\n",
    "def format_float(val):\n",
    "    return f\"{val:.2f}\"\n",
    "\n",
    "def table_of_all_runs_by_algo(df, include_oracle_steps=False, include_learned_model_size=False, include_learning_rounds_breakdown=False):\n",
    "    columns = [\n",
    "        (\"#\", None, format_as_is),\n",
    "        (\"Learning Rounds\",\"learning_rounds\",  format_as_is),\n",
    "        (\"Time (s)\",\"total_time\",  format_time),\n",
    "        (\"Timed-Out\", \"timed_out\",  format_bool),\n",
    "        (\"Bisimilar\", \"bisimilar\", format_bool),\n",
    "        (\"Accuracy\", \"Accuracy\", format_float),\n",
    "    ]\n",
    "    if include_oracle_steps:\n",
    "        columns.append((\"Learning Steps\", \"steps_learning\", format_as_is),)\n",
    "        columns.append((\"Oracle Steps\", \"steps_eq_oracle\", format_as_is))\n",
    "    else:\n",
    "        columns.append((\"Steps\", \"steps_learning\", format_as_is))\n",
    "\n",
    "    if include_learned_model_size:\n",
    "        columns.append((r\"|\\Hyp|\", \"learned_automaton_size\", format_as_is))\n",
    "\n",
    "    if include_learning_rounds_breakdown:\n",
    "        columns.append((\"IC Rounds\", \"rounds_with_ic\", format_as_is))\n",
    "        columns.append((\"Non-IC Rounds\", \"rounds_without_ic\", format_as_is))\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [title for title, *_ in columns]\n",
    "\n",
    "    for algo, group in df.groupby(\"algorithm_name\", sort=False):\n",
    "        # subheader\n",
    "        # table.add_row([\"\" for _ in columns])\n",
    "        table.add_row([f\"=== {algo} ===\"] + [\"\"] * (len(columns) - 1))\n",
    "        # table.add_row([\"\" for _ in columns])\n",
    "\n",
    "        for i, (_, row) in enumerate(group.iterrows(), start=1):\n",
    "            table_row = [i]  # numbered row\n",
    "            for title, key, formatter in columns[1:]:\n",
    "                table_row.append(formatter(row[key]))\n",
    "            table.add_row(table_row)\n",
    "\n",
    "    table.align = \"l\"\n",
    "    table.border = True\n",
    "    table.header = True\n",
    "\n",
    "    return table\n",
    "\n",
    "#table = table_of_all_runs_by_algo(results_df, include_oracle_steps=False, include_learned_model_size=True, include_learning_rounds_breakdown=True)\n",
    "# print(table)\n",
    "#print(table.get_latex_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def format_as_is(val):\n",
    "    return val\n",
    "\n",
    "def format_time(secs):\n",
    "    secs = int(secs)\n",
    "\n",
    "    minutes, sec = divmod(secs, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "\n",
    "    return f\"{hours:02d}:{minutes:02d}:{sec:02d}\"\n",
    "\n",
    "def format_timeout(secs):\n",
    "    secs = int(secs)\n",
    "    minutes, sec = divmod(secs, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    if (minutes, sec) == (0, 0):\n",
    "        tstr = f\"{hours}h\"\n",
    "    else:\n",
    "        tstr = format_time(secs)\n",
    "    return fr\"\\timedout{{{tstr}}}\"\n",
    "\n",
    "def format_bool(val):\n",
    "    return \"Yes\" if val else \"No\"\n",
    "\n",
    "def format_float(val):\n",
    "    return f\"{val:.2f}\"\n",
    "\n",
    "def default_getter(row, key):\n",
    "    return row[key]\n",
    "\n",
    "def bool_getter(row, key):\n",
    "    return format_bool(row[key])\n",
    "\n",
    "def time_getter(row, key):\n",
    "    time = row[key]\n",
    "    if row[\"timed_out\"]:\n",
    "        timeout = int(row[\"detailed_learning_info\"][\"params\"][\"timeout\"])\n",
    "        assert time > timeout\n",
    "        return format_timeout(timeout)\n",
    "    else:\n",
    "        return format_time(time)\n",
    "\n",
    "def float_getter(row, key):\n",
    "    return format_float(row[key])\n",
    "\n",
    "def int_getter(row, key):\n",
    "    return int(row[key])\n",
    "\n",
    "def learning_rounds_getter(row, key):\n",
    "    rounds = row[key]\n",
    "    if row[\"algorithm_name\"].startswith(\"APMSL\"):\n",
    "        rounds_without_ic = row[\"rounds_without_ic\"]\n",
    "        return f\"{int(rounds)} ({int(rounds_without_ic)})\"\n",
    "    else:\n",
    "        return str(int(rounds))\n",
    "\n",
    "\n",
    "ALG_MARKER = \"===\"\n",
    "MEAN_MARKER = r\"\\bar{x}\"\n",
    "\n",
    "def new_table_of_all_runs_by_algo(df, include_oracle_steps=False, include_learned_model_size=False, include_learning_rounds_breakdown=False, add_mean_row=False):\n",
    "    columns = [\n",
    "        (\"\\#\", None, default_getter),\n",
    "        (\"Rounds\",\"learning_rounds\", learning_rounds_getter if include_learning_rounds_breakdown else default_getter),\n",
    "        (\"Time\",\"total_time\",  time_getter),\n",
    "        (\"Bisimilar\", \"bisimilar\", bool_getter),\n",
    "        (\"Accuracy\", \"Accuracy\", float_getter),\n",
    "    ]\n",
    "    if include_oracle_steps:\n",
    "        columns.append((\"Learning Steps\", \"steps_learning\", int_getter))\n",
    "        columns.append((\"Oracle Steps\", \"steps_eq_oracle\", int_getter))\n",
    "    else:\n",
    "        columns.append((\"Steps\", \"steps_learning\", int_getter))\n",
    "\n",
    "    if include_learned_model_size:\n",
    "        columns.append((r\"$|\\Hyp|$\", \"learned_automaton_size\", int_getter))\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [title for title, *_ in columns]\n",
    "\n",
    "    for algo, group in df.groupby(\"algorithm_name\", sort=False):\n",
    "        # subheader\n",
    "        table.add_row([f\"{ALG_MARKER}{algo}{ALG_MARKER}\"] + [\"\"] * (len(columns) - 1))\n",
    "\n",
    "        for i, (_, row) in enumerate(group.iterrows(), start=1):\n",
    "            table_row = [i]  # numbered row\n",
    "            for title, key, getter in columns[1:]:\n",
    "                table_row.append(getter(row, key))\n",
    "            table.add_row(table_row)\n",
    "\n",
    "        if add_mean_row:\n",
    "            mean_table_row = [MEAN_MARKER]\n",
    "\n",
    "            for title, key, getter in columns[1:]:\n",
    "                if key == \"Accuracy\":\n",
    "                    mean_table_row.append(format_float(group[\"Accuracy\"].mean()))\n",
    "                elif key == \"steps_learning\":\n",
    "                    mean_table_row.append(str(format_float(group[\"steps_learning\"].mean())))\n",
    "                elif key == \"steps_eq_oracle\":\n",
    "                    mean_table_row.append(str(format_float(group[\"steps_eq_oracle\"].mean())))\n",
    "                else:\n",
    "                    # For all other columns, leave blank for the mean row\n",
    "                    mean_table_row.append(\"\")\n",
    "\n",
    "            table.add_row(mean_table_row)\n",
    "\n",
    "    table.align = \"l\"\n",
    "    table.border = True\n",
    "    table.header = True\n",
    "\n",
    "    return table\n",
    "\n",
    "def to_latex(table):\n",
    "    num_cols = len(table.field_names)\n",
    "    default_lines = table.get_latex_string().splitlines()\n",
    "    new_lines = []\n",
    "    for line in default_lines:\n",
    "        if ALG_MARKER in line:\n",
    "            alg = line.split(ALG_MARKER)[1]\n",
    "            new_lines.extend([\n",
    "                r\"\\midrule\",\n",
    "                r\"\",\n",
    "                r\"\\multicolumn{%i}{c}{\\emph{%s}}\\\\\" % (num_cols, alg),\n",
    "                r\"\\addlinespace\",\n",
    "            ])\n",
    "        elif MEAN_MARKER in line:\n",
    "            new_lines.extend([\n",
    "                r\"\\midrule\",\n",
    "\n",
    "            ])\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    latex_str = \"\\n\".join(new_lines)\n",
    "    return latex_str\n",
    "\n",
    "add_learning_rounds_breakdown_to_df(results_df)\n",
    "table = new_table_of_all_runs_by_algo(results_df, include_oracle_steps=True, include_learned_model_size=True, include_learning_rounds_breakdown=True, add_mean_row=False)\n",
    "# print(table)\n",
    "print(to_latex(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
