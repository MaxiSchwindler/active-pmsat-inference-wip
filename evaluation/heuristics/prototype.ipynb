{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import functools\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "sys.path.append(f\"..\")\n",
    "sys.path.append(r\"../../../pmsat-inference\")\n",
    "sys.path.append(r\"../../../AALpy\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from aalpy import MooreMachine, MooreState\n",
    "from aalpy.utils import load_automaton_from_file\n",
    "\n",
    "from typing import Any, TypeAlias\n",
    "from collections.abc import Callable\n",
    "HeuristicFunction: TypeAlias = Callable[[MooreMachine, dict[str, Any], list[list[str | list[str]]]], float]\n",
    "\n",
    "# TODO: outdated file, needs new pythonpath\n",
    "\n",
    "\n",
    "# TODO: use heuristics from active_pmsatlearn/heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = \"ping_pong_example\", \"simple_example_server\", \"simple_example_server_with_glitch\"\n",
    "DEFAULT_DATA_DIR = \"../../generated_data_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example(example: str) -> tuple[MooreMachine, tuple[MooreMachine, dict[str, Any]], dict[str, Any]]:\n",
    "    \"\"\" Load the example from the pmsat-inference repo.\n",
    "    \n",
    "    Returns a tuple of:\n",
    "        real model\n",
    "        tuple or (learned model, pmsat info)\n",
    "        info dict (containing used traces)\n",
    "    \"\"\"\n",
    "    def load_automaton(*args, **kwargs) -> MooreMachine:\n",
    "        \"\"\" Call aalpy.utils.load_automaton_from_file, but suppress it's print output \n",
    "        (aalpy prints a warning that our automata are not input complete) \"\"\"\n",
    "        with io.StringIO() as f, redirect_stdout(f):\n",
    "            return load_automaton_from_file(*args, **kwargs)\n",
    "    \n",
    "    assert example in EXAMPLES\n",
    "    example_dir = Path(\"../pmsat-inference/examples-results/\" + example)\n",
    "    real_model = load_automaton(example_dir / \"RealModel.dot\", \"moore\")\n",
    "    \n",
    "    pmsat_learned_models = {}\n",
    "    pmsat_infos = {}\n",
    "    base_pattern = \"pmsatLearned-rc2-N(\\d)-RN(2|3|4)\"\n",
    "    model_pattern = fr\"{base_pattern}\\.dot\"\n",
    "    info_pattern = fr\"{base_pattern}\\.json\"\n",
    "    for root, _, files in os.walk(example_dir):\n",
    "        for file in files:\n",
    "            if match := re.search(model_pattern, file):\n",
    "                pmsat_learned_models[match.group(1)] = load_automaton(example_dir / file, \"moore\")\n",
    "            elif match := re.search(info_pattern, file):\n",
    "                with open(example_dir/file, \"r\") as f:\n",
    "                    pmsat_infos[match.group(1)] = json.load(f)\n",
    "    \n",
    "    models_and_info = []\n",
    "    for num_states_learned, model in pmsat_learned_models.items():\n",
    "        pmsat_info = pmsat_infos[num_states_learned]\n",
    "        models_and_info.append((model, pmsat_info))\n",
    "        \n",
    "    with open(example_dir / \"info.json\", \"r\") as f:\n",
    "        info = json.load(f)\n",
    "        \n",
    "    return real_model, models_and_info, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model_name: str, models_dir: str = DEFAULT_DATA_DIR) -> tuple[MooreMachine, tuple[MooreMachine, dict[str, Any], dict[str, Any]], dict[str, Any]]:\n",
    "    \"\"\" Load the data from one learned model.\n",
    "    NOTE: This only returns actually learned models!\n",
    "          If a model could not be learned with the given number of states, it is not included in learned_models!\n",
    "          Therefore, no info about runs with too few states (not possible to learn) or too many states (timeout) is included!\n",
    "    \n",
    "    Returns a tuple of:\n",
    "        real model\n",
    "        list of tuples of (learned model, pmsat info, learning result)\n",
    "        info dict of whole run (containing used traces)\n",
    "    \"\"\"\n",
    "    def load_automaton(*args, **kwargs) -> MooreMachine:\n",
    "        \"\"\" Call aalpy.utils.load_automaton_from_file, but suppress it's print output \n",
    "        (aalpy prints a warning that our automata are not input complete) \"\"\"\n",
    "        with io.StringIO() as f, redirect_stdout(f):\n",
    "            return load_automaton_from_file(*args, **kwargs)\n",
    "    \n",
    "    model_dir = Path(models_dir) / model_name\n",
    "    assert model_dir.exists(), f\"{model_dir} does not exist!\"\n",
    "    real_model = load_automaton(model_dir / \"RealModel.dot\", \"moore\")\n",
    "    \n",
    "    pmsat_learned_models = {}\n",
    "    pmsat_infos = {}\n",
    "    learning_results = {}\n",
    "    model_pattern = r\"LearnedModel_(\\d)States\\.dot\"\n",
    "    info_pattern = r\"LearningInfo_(\\d)States\\.json\"\n",
    "    result_pattern = r\"LearningResults_(\\d)States\\.json\"\n",
    "    for root, _, files in os.walk(model_dir):\n",
    "        for file in files:\n",
    "            if match := re.search(model_pattern, file):\n",
    "                pmsat_learned_models[match.group(1)] = load_automaton(model_dir / file, \"moore\")\n",
    "            elif match := re.search(info_pattern, file):\n",
    "                with open(model_dir/file, \"r\") as f:\n",
    "                    pmsat_infos[match.group(1)] = json.load(f)\n",
    "            elif match := re.search(result_pattern, file):\n",
    "                with open(model_dir/file, \"r\") as f:\n",
    "                    learning_results[match.group(1)] = json.load(f)\n",
    "    \n",
    "    models_and_info = []\n",
    "    for num_states_learned, model in pmsat_learned_models.items():\n",
    "        pmsat_info = pmsat_infos[num_states_learned]\n",
    "        learning_result = learning_results[num_states_learned]\n",
    "        models_and_info.append((model, pmsat_info, learning_result))\n",
    "        \n",
    "    with open(model_dir / \"info.json\", \"r\") as f:\n",
    "        info = json.load(f)\n",
    "        \n",
    "    return real_model, models_and_info, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_heurisic_scores(real_model: MooreMachine, learned_models: list[tuple[MooreMachine, dict, dict]], infos: dict, heuristic: HeuristicFunction) -> dict[int, float]:\n",
    "    \"\"\" \n",
    "    Calculate the heuristic scores for multiple learned models of one single automaton.\n",
    "    :param real_model: the original automaton\n",
    "    :param learned_models: list of tuples of (learned model, pmsat info)\n",
    "    :param infos: learning info dict for the whole run (e.g. containing used traces)\n",
    "    :param heuristic: the heuristic function to calculate the scores with\n",
    "    \n",
    "    :return a dict of (distance_to_real_model -> heuristic score)\n",
    "    \"\"\"\n",
    "    traces = infos[\"traces\"]\n",
    "    \n",
    "    def distance(real_model: MooreMachine, learned_model: MooreMachine):\n",
    "        return len(learned_model.states) - len(real_model.states)\n",
    "    \n",
    "    distance_to_heuristic = {\n",
    "        distance(real_model, learned_model): heuristic(learned_model, learning_info, traces) for learned_model, learning_info, learning_result in learned_models\n",
    "    }\n",
    "    \n",
    "    return distance_to_heuristic\n",
    "\n",
    "\n",
    "def plot_heuristic_scores(real_model: MooreMachine, learned_models: list[tuple[MooreMachine, dict, dict]], infos: dict, heuristic: HeuristicFunction, new_plot: bool = True, show_plot: bool = True, as_line: bool = False, mark_best_model: bool=False):\n",
    "    \"\"\" \n",
    "    Plot the heuristic scores of multiple learned models of one original automaton over distance to the original automaton.\n",
    "    This calls calculate_heuristic_scores() to calculate the values.\n",
    "    \n",
    "    :param real_model: the original automaton\n",
    "    :param learned_models: list of tuples of (learned model, pmsat info)\n",
    "    :param infos: learning info dict for the whole run (e.g. containing used traces)\n",
    "    :param heuristic: the heuristic function to calculate the scores with\n",
    "    :param new_plot: whether to create a new plot, or to write to the current plot instance in plt\n",
    "    :param show_plot: whether to show the plot directly\n",
    "    :param as_line: Whether to connect all points in this plot by a line\n",
    "    :param mark_best_model: whether to mark the best model in the plot by \n",
    "    \"\"\"\n",
    "    scores = calculate_heurisic_scores(real_model, learned_models, infos, heuristic)\n",
    "    if new_plot:\n",
    "        plt.figure()\n",
    "\n",
    "    plt.axvline(0, color='red', linewidth=0.5, linestyle='--') # TODO this line is added many times\n",
    "    \n",
    "    if as_line:\n",
    "        plt.plot(scores.keys(), scores.values())\n",
    "    else:\n",
    "        plt.scatter(scores.keys(), scores.values())\n",
    "        \n",
    "    if mark_best_model:\n",
    "        best_key = max(scores, key=scores.get)\n",
    "        best_value = scores[best_key]\n",
    "        plt.scatter([best_key], [best_value], marker='*', facecolor='none', edgecolors=\"red\", s=75, zorder=5)\n",
    "\n",
    "    curr_ticks, curr_tick_labels = plt.xticks()\n",
    "    if not set(scores.keys()).issubset(set(curr_ticks)) or any(int(ct) != ct for ct in curr_ticks):\n",
    "        # add new ticks if (a) current keys are not in current ticks or (b) current ticks contains floats\n",
    "        plt.xticks(list(scores.keys()))\n",
    "        \n",
    "    plt.xlabel(\"Distance to ground truth\")\n",
    "    plt.ylabel(f\"{heuristic.__name__} score\")\n",
    "    plt.title(f\"Scores of heuristic '{heuristic.__name__}'\")\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "def calculate_heuristic_scores_for_example(example: str, heuristic: HeuristicFunction) -> dict[int, float]:\n",
    "    \"\"\" Calculate the heuristic scores of a given example from the pmsat-inference repo. \"\"\"\n",
    "    real_model, learned_models, infos = load_example(example)\n",
    "    return calculate_heurisic_scores(real_model, learned_models, infos, heuristic)\n",
    "\n",
    "\n",
    "def plot_heuristic_scores_for_example(example: str, heuristic: HeuristicFunction, **kwargs):\n",
    "    \"\"\" Plot the heuristic scores of a given example from the pmsat-inference repo.\"\"\"\n",
    "    real_model, learned_models, infos = load_example(example)\n",
    "    plot_heuristic_scores(real_model, learned_models, infos, heuristic, **kwargs)\n",
    "    \n",
    "    \n",
    "def calculate_heuristic_scores_for_model(model: str, heuristic: HeuristicFunction, models_dir: str = DEFAULT_DATA_DIR) -> dict[int, float]:\n",
    "    \"\"\" Calculate the heuristic scores of the learnt models of a given model from the given models directory\"\"\"\n",
    "    real_model, learned_models, infos = load_data(model, models_dir)\n",
    "    return calculate_heurisic_scores(real_model, learned_models, infos, heuristic)\n",
    "\n",
    "\n",
    "def plot_heuristic_scores_for_model(model: str, heuristic: HeuristicFunction, models_dir: str = DEFAULT_DATA_DIR, **kwargs):\n",
    "    \"\"\" Plot the heuristic scores of the learnt models of a given model from the given models directory\"\"\"\n",
    "    real_model, learned_models, infos = load_data(model, models_dir)\n",
    "    plot_heuristic_scores(real_model, learned_models, infos, heuristic, **kwargs)\n",
    "\n",
    "\n",
    "def plot_heuristic_scores_for_all_models_in_dir(heuristic: HeuristicFunction, models_dir: str = DEFAULT_DATA_DIR, **kwargs):\n",
    "    \"\"\" Plot the heuristic scores of learnt models of all models in the given models directory\"\"\"\n",
    "    plt.figure()\n",
    "    for model_name in os.listdir(models_dir):\n",
    "        if not (Path(models_dir) / model_name).is_dir():\n",
    "            continue\n",
    "        plot_heuristic_scores_for_model(model_name, heuristic, models_dir=models_dir, new_plot=False, show_plot=False, **kwargs)\n",
    "    plt.show()\n",
    "    \n",
    "def _calc_dist_to_num_best_models(heuristic: HeuristicFunction, models_dir=DEFAULT_DATA_DIR):\n",
    "    dist_to_num_best_models_at_dist = defaultdict(int)\n",
    "    for model_name in os.listdir(models_dir):\n",
    "        if not (Path(models_dir) / model_name).is_dir():\n",
    "            continue\n",
    "        scores = calculate_heuristic_scores_for_model(model_name, heuristic, models_dir=models_dir)\n",
    "        best_key = max(scores, key=scores.get)\n",
    "        dist_to_num_best_models_at_dist[best_key] += 1\n",
    "        \n",
    "    return dict(dist_to_num_best_models_at_dist)\n",
    "\n",
    "def plot_number_of_best_models_identified(heuristic: HeuristicFunction, models_dir: str = DEFAULT_DATA_DIR, new_plot=False, show_plot=True, as_line=False):\n",
    "    \"\"\" \n",
    "    Plot the number of best models identified with the given heuristic, calculated on all models in the given models directory\n",
    "    \n",
    "    :param heuristic: The heuristic to use\n",
    "    :param models_dir: The models directory\n",
    "    :param new_plot: Whether to create a new plot instance or use the existing one\n",
    "    :param show_plot: Whether to show the plot\n",
    "    :param as_line: Whether to show the data points as a line or as disconnected points\n",
    "    \"\"\"\n",
    "    dist_to_num_best_models_at_dist = _calc_dist_to_num_best_models(heuristic, models_dir=models_dir)\n",
    "    if new_plot:\n",
    "        plt.figure()\n",
    "    \n",
    "    label = heuristic.__name__    \n",
    "    if as_line:\n",
    "        keys = sorted(dist_to_num_best_models_at_dist.keys())\n",
    "        vals = [dist_to_num_best_models_at_dist[k] for k in keys]\n",
    "        plt.plot(keys, vals, label=label)\n",
    "    else:\n",
    "        plt.scatter(dist_to_num_best_models_at_dist.keys(), dist_to_num_best_models_at_dist.values(), label=label)\n",
    "    \n",
    "    plt.title(f\"Number of best models identified at distance\")\n",
    "    plt.xlabel(\"Distance to ground truth\")\n",
    "    plt.ylabel(\"Number of best models identified\")\n",
    "    plt.legend(loc=(1.04, 1))\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "def print_heuristic_stats_for_all_models_in_dir(*heuristics: HeuristicFunction, models_dir=DEFAULT_DATA_DIR, **kwargs):    \n",
    "    from prettytable import PrettyTable\n",
    "    \n",
    "    num_models = sum(1 for m in os.listdir(models_dir) if (Path(models_dir) / m).is_dir())\n",
    "    \n",
    "    stats = {}\n",
    "    for heuristic in heuristics:\n",
    "        stats[heuristic.__name__] = _calc_dist_to_num_best_models(heuristic, models_dir=models_dir)\n",
    "\n",
    "    all_distances = set()\n",
    "    for dist_to_num_best in stats.values():\n",
    "        all_distances.update(dist_to_num_best.keys())\n",
    "    all_distances = sorted(all_distances)\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Distance\"] + [heuristic.__name__ for heuristic in heuristics]\n",
    "\n",
    "    for distance in all_distances:\n",
    "        row = [distance]\n",
    "        for heuristic in heuristics:\n",
    "            dist_to_num_best = stats[heuristic.__name__]\n",
    "            num_best_models = dist_to_num_best.get(distance, 0)\n",
    "            percent = (num_best_models / num_models * 100) if num_models > 0 else 0\n",
    "            row.append(f\"{num_best_models} ({percent:.1f}%)\")\n",
    "        table.add_row(row)\n",
    "\n",
    "    print(table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_heuristics(heuristics: list[HeuristicFunction], models_dir: str = DEFAULT_DATA_DIR):\n",
    "    for heuristic in heuristics:\n",
    "        plot_heuristic_scores_for_all_models_in_dir(heuristic, models_dir=models_dir, as_line=True, mark_best_model=True)\n",
    "    \n",
    "    plt.figure()\n",
    "    for heuristic in heuristics:\n",
    "        plot_number_of_best_models_identified(heuristic, models_dir=models_dir, new_plot=False, show_plot=False, as_line=True)\n",
    "    plt.show()    \n",
    "    \n",
    "    print_heuristic_stats_for_all_models_in_dir(*heuristics, models_dir=models_dir)\n",
    "\n",
    "def _create_partial_heuristics(heuristic: HeuristicFunction, **kwargs):\n",
    "    def to_str(x):\n",
    "        if callable(x):\n",
    "            return x.__name__\n",
    "        return str(x)\n",
    "    heuristic_functions = []\n",
    "    all_combinations = list(itertools.product(*kwargs.values()))\n",
    "    for combination in all_combinations:\n",
    "\n",
    "        # round floats to 2 comma\n",
    "        new_comb = []\n",
    "        for component in combination:\n",
    "            if isinstance(component, float):\n",
    "                component = round(component, 2)\n",
    "            new_comb.append(component)\n",
    "        combination = new_comb\n",
    "\n",
    "        kwarg_dict = dict(zip(kwargs.keys(), combination))\n",
    "        fun = functools.partial(heuristic, **kwarg_dict)\n",
    "        fun.__name__ = heuristic.__name__ + \"(\" + \", \".join(f\"{key}={to_str(value)}\" for key, value in kwarg_dict.items()) + \")\"\n",
    "        heuristic_functions.append(fun)\n",
    "    return heuristic_functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def base_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, stat_glitched=np.mean, stat_dominant=np.min):\n",
    "    score = 0\n",
    "        \n",
    "    # glitched_trans_freq: use mean (or max) of find upper outliers, which should actually be dominant transitions\n",
    "    # the higher the mean is, the more likely are glitched transitions which should be dominant transitions - i.e., too little states\n",
    "    score += 1 / stat_glitched(learning_info[\"glitched_delta_freq\"] or [1])\n",
    "    \n",
    "    # dominant_trans_freq: use min to find lower outliers, which could be glitches that were encoded as dominant transitions\n",
    "    # the lower min is, the worse it is for us\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    # remove frequency 0 from dominant_delta_freq # TODO actually, I think this should be removed by the pmsat alg?\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "    score -= 1 / stat_dominant(dominant_delta_freq)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# plot_heuristic_scores_for_example(\"ping_pong_example\", heuristic)\n",
    "# plot_heuristic_scores_for_model(\"MooreMachine_4States_4Inputs_3Outputs_11c10030c44546dfaa0e3e6cc27ce858\", heuristic, as_line=True)\n",
    "plot_heuristic_scores_for_all_models_in_dir(base_heuristic, as_line=True, mark_best_model=True)\n",
    "plot_number_of_best_models_identified(base_heuristic)\n",
    "print_heuristic_stats_for_all_models_in_dir(base_heuristic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_punish_unreachable(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, factor=0.5):\n",
    "    assert 0 <= factor <= 1, \"factor must be between 0 and 1\"\n",
    "    score = base_heuristic(learned_model, learning_info, traces)\n",
    "    \n",
    "    learned_model.compute_prefixes()\n",
    "    unreachable_states = [s for s in learned_model.states if s.prefix is None]\n",
    "    for state in unreachable_states:\n",
    "        if score > 0:\n",
    "            score *= factor\n",
    "        elif score < 0:\n",
    "            score *= (1+factor)\n",
    "        else:\n",
    "            score -= 1\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_heuristics([base_heuristic, *_create_partial_heuristics(base_punish_unreachable, factor=np.arange(0, 1, 0.25))])\n",
    "compare_heuristics(_create_partial_heuristics(base_heuristic, stat_glitched=[np.mean, np.max], stat_dominant=[np.min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_exp_penalty(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, weight_g=1.0, weight_d=1.0):\n",
    "        \n",
    "    g = np.array(learning_info[\"glitched_delta_freq\"])\n",
    "    d = np.array(learning_info[\"dominant_delta_freq\"])\n",
    "    \n",
    "    if len(g) > 0:\n",
    "        normalized_g = (g - np.min(g)) / (np.max(g) - np.min(g) + 1e-8)  # avoid division by zero\n",
    "        g_penalty = np.mean(normalized_g ** 2)  # penalize high values in g (quadratic penalty)\n",
    "    else:\n",
    "        g_penalty = 0\n",
    "    \n",
    "    # penalize very small values in d\n",
    "    normalized_d = (d - np.min(d)) / (np.max(d) - np.min(d) + 1e-8)\n",
    "    d_penalty = np.mean(np.exp(-normalized_d))\n",
    "    \n",
    "    score = -weight_g * g_penalty + weight_d * (1 - d_penalty)\n",
    "    return score\n",
    "\n",
    "compare_heuristics([base_heuristic, *_create_partial_heuristics(quad_exp_penalty, weight_g=np.arange(0, 1.1, 0.2), weight_d=np.arange(0, 1.1, 0.2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_penalty(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]]):\n",
    "    g = np.array(learning_info[\"glitched_delta_freq\"])\n",
    "    d = np.array(learning_info[\"dominant_delta_freq\"])\n",
    "\n",
    "    # penalize high values in `g` (relative to the mean of `g`)\n",
    "    if len(g) > 0:\n",
    "        g_penalty = np.mean(g) / (1 + np.max(g) - np.mean(g))\n",
    "    else:\n",
    "        g_penalty = 0\n",
    "    \n",
    "    # reward high values in `d`, penalizing small ones compared to the median\n",
    "    d_reward = (np.min(d) / (1 + np.median(d))) if np.min(d) > 0 else 0\n",
    "    \n",
    "    score = d_reward - g_penalty\n",
    "    return score\n",
    "\n",
    "\n",
    "compare_heuristics([base_heuristic, relative_penalty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_heuristic_parameterized(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, stat_glitched=np.mean, stat_dominant=np.min, numerator_g=1, numerator_d=1, log=False):\n",
    "    if log:\n",
    "        print_d = print\n",
    "    else:\n",
    "        print_d = lambda *_: None\n",
    "\n",
    "    print_d(\"Calculating base heuristic | score = 0\")\n",
    "\n",
    "    score = 0\n",
    "        \n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    if len(glitched_delta_freq) > 0:\n",
    "        score += numerator_g / stat_glitched(glitched_delta_freq)\n",
    "        print_d(f\"{glitched_delta_freq=} | {stat_glitched.__name__}(glitched_delta_freq)={stat_glitched(glitched_delta_freq)} | {numerator_g}/{stat_glitched.__name__}(glitched_delta_freq)={numerator_g / stat_glitched(glitched_delta_freq)} | {score=}\")\n",
    "    else:\n",
    "        score += 1\n",
    "        print_d(f\"No glitched delta frequencies | {score=}\")\n",
    "\n",
    "    \n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "    \n",
    "    score -= numerator_d / stat_dominant(dominant_delta_freq)\n",
    "\n",
    "    print_d(f\"{dominant_delta_freq=} | {stat_dominant.__name__}(dominant_delta_freq)={stat_dominant(dominant_delta_freq)} | {numerator_d}/{stat_dominant.__name__}(dominant_delta_freq)={numerator_d / stat_dominant(dominant_delta_freq)} | {score=}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# compare_heuristics([*_create_partial_heuristics(base_heuristic_parameterized, log=[True])])\n",
    "# plot_heuristic_scores_for_example(\"ping_pong_example\", base_heuristic_parameterized)\n",
    "compare_heuristics([base_heuristic, *_create_partial_heuristics(base_heuristic_parameterized, numerator_g=[3], numerator_d=[1, 2, 3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_10(lst: list[float]):\n",
    "    return np.percentile(lst, 10)\n",
    "\n",
    "def upper_10(lst: list[float]):\n",
    "    return np.percentile(lst, 90)\n",
    "\n",
    "compare_heuristics(_create_partial_heuristics(base_heuristic, stat_glitched=[np.max, upper_10], stat_dominant=[np.min, lower_10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_threshold(data, k=2):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return mean - k * std\n",
    "\n",
    "\n",
    "def threshold_penalty(lst, weight=1):\n",
    "    threshold = choose_threshold(lst)\n",
    "    penalty = 0\n",
    "    for x in lst:\n",
    "        if x < threshold:\n",
    "            penalty += weight * (threshold - x)  # Penalize based on how far below the threshold\n",
    "    return penalty\n",
    "\n",
    "def threshold_exp_penalty(lst, weight=1):\n",
    "    threshold = choose_threshold(lst)\n",
    "    penalty = 0\n",
    "    for x in lst:\n",
    "        if x < threshold:\n",
    "            penalty += weight * (threshold - x) ** 2  # Exponential penalty\n",
    "    return penalty\n",
    "\n",
    "def threshold_rel_penalty(lst, weight=1):\n",
    "    threshold = choose_threshold(lst)\n",
    "    penalty = 0\n",
    "    for x in lst:\n",
    "        if x < threshold:\n",
    "            penalty += weight * ((threshold - x) / threshold)  # Relative penalty\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def threshold_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, kind=\"normal\"):\n",
    "    match kind:\n",
    "        case \"normal\":\n",
    "            return -threshold_exp_penalty(learning_info[\"dominant_delta_freq\"], weight=1)\n",
    "        case \"exp\":\n",
    "            return -threshold_exp_penalty(learning_info[\"dominant_delta_freq\"], weight=1)\n",
    "        case \"rel\":\n",
    "            return -threshold_rel_penalty(learning_info[\"dominant_delta_freq\"], weight=1)\n",
    "        case _:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "compare_heuristics([base_heuristic, *_create_partial_heuristics(threshold_heuristic, kind=[\"normal\", \"exp\", \"rel\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_10_sum(lst: list[float]):\n",
    "    lt = np.percentile(lst, 10)\n",
    "    return sum(x for x in lst if x <= lt)\n",
    "\n",
    "def upper_10_sum(lst: list[float]):\n",
    "    ut = np.percentile(lst, 90)\n",
    "    return sum(x for x in lst if x >= ut)\n",
    "\n",
    "def q1_sum(lst: list[float]):\n",
    "    q1 = np.percentile(lst, 25)\n",
    "    return sum(x for x in lst if x <= q1)\n",
    "\n",
    "def q3_sum(lst: list[float]):\n",
    "    q3 = np.percentile(lst, 75)\n",
    "    return sum(x for x in lst if x >= q3)\n",
    "\n",
    "compare_heuristics(_create_partial_heuristics(base_heuristic_parameterized, stat_glitched=[np.max, q3_sum, upper_10_sum], stat_dominant=[np.min, q1_sum, lower_10_sum]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punish_overlapping_values(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]]):\n",
    "        \n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "    \n",
    "    score = base_heuristic_parameterized(learned_model, learning_info, traces)\n",
    "\n",
    "    for g_freq in glitched_delta_freq:\n",
    "        if min(dominant_delta_freq) <= g_freq <= max(dominant_delta_freq):\n",
    "            score -= score / len(glitched_delta_freq)\n",
    "    \n",
    "    if len(glitched_delta_freq) > 0:\n",
    "        for d_freq in dominant_delta_freq:\n",
    "            if min(glitched_delta_freq) <= d_freq <= max(glitched_delta_freq):\n",
    "                score -= score / len(dominant_delta_freq)\n",
    "    return score\n",
    "\n",
    "\n",
    "compare_heuristics([base_heuristic_parameterized, punish_overlapping_values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def compute_z_scores_rel_dominant(dominant_frequencies, glitched_frequencies):\n",
    "    \"\"\"\n",
    "    Compute the z scores of the dominant frequencies (in relation to dominant frequencies) and\n",
    "    of glitched frequencies (also in relation to dominant frequencies)\n",
    "    \"\"\"\n",
    "    mean_dominant = statistics.mean(dominant_frequencies)\n",
    "    std_dev_dominant = statistics.stdev(dominant_frequencies)\n",
    "\n",
    "    if std_dev_dominant == 0:\n",
    "        z_scores_dominant = [0] * len(dominant_frequencies)\n",
    "        z_scores_glitched = [0] * len(glitched_frequencies)\n",
    "    else:\n",
    "        z_scores_dominant = [(f - mean_dominant) / std_dev_dominant for f in dominant_frequencies]\n",
    "        z_scores_glitched = [(g - mean_dominant) / std_dev_dominant for g in glitched_frequencies]\n",
    "\n",
    "    return z_scores_dominant, z_scores_glitched\n",
    "\n",
    "def compute_z_scores(frequencies):\n",
    "    \"\"\"\n",
    "    Compute the z scores of the given list of frequencies\n",
    "    \"\"\"\n",
    "    if len(frequencies) < 2:\n",
    "        return [0] * len(frequencies)\n",
    "\n",
    "    mean = statistics.mean(frequencies)\n",
    "    std_dev = statistics.stdev(frequencies)\n",
    "\n",
    "    if std_dev == 0:\n",
    "        z_scores = [0] * len(frequencies)\n",
    "    else:\n",
    "        z_scores = [(f - mean) / std_dev for f in frequencies]\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "def zscores_sum(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, glitch_threshold=1, dominant_threshold=-1):\n",
    "    score = 0\n",
    "\n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "\n",
    "    glitched_z_scores = compute_z_scores(glitched_delta_freq)\n",
    "    dominant_z_scores = compute_z_scores(dominant_delta_freq)\n",
    "\n",
    "    glitch_sum_above_threshold = sum(zscore for zscore in glitched_z_scores if zscore >= glitch_threshold)\n",
    "    if glitch_sum_above_threshold > 0:\n",
    "        score -= glitch_sum_above_threshold\n",
    "\n",
    "    dominant_sum_below_threshold = sum(zscore for zscore in dominant_z_scores if zscore <= dominant_threshold)\n",
    "    if dominant_sum_below_threshold > 0:\n",
    "        score -= dominant_sum_below_threshold\n",
    "\n",
    "    return score\n",
    "\n",
    "def find_similar_frequencies(dominant_frequencies, glitched_frequencies, z_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the z-score of the dominant frequency relative to the glitched list to find dominant frequencies\n",
    "    which are similar to glitched frequencies.\n",
    "    \"\"\"\n",
    "    if len(glitched_frequencies) == 0:\n",
    "        return []\n",
    "\n",
    "    mean_glitched = np.mean(glitched_frequencies)\n",
    "    std_glitched = np.std(glitched_frequencies)\n",
    "\n",
    "    similar_frequencies = []\n",
    "    for d in dominant_frequencies:\n",
    "        z_score = (d - mean_glitched) / std_glitched if std_glitched != 0 else 0\n",
    "        if abs(z_score) <= z_threshold:\n",
    "            similar_frequencies.append(d)\n",
    "\n",
    "    return similar_frequencies\n",
    "\n",
    "def new_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]]):\n",
    "    score = base_heuristic_parameterized(learned_model, learning_info, traces)\n",
    "    # score = 0\n",
    "\n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "\n",
    "    # encoded_glitches = find_similar_frequencies(dominant_delta_freq, glitched_delta_freq)\n",
    "    # if len(encoded_glitches) > 0:\n",
    "    #     score += 1 / len(encoded_glitches)\n",
    "\n",
    "    num_glitches = len(learning_info[\"glitch_steps\"])\n",
    "    # score -= num_glitches * len(learned_model.states)  # this works quite well on its own\n",
    "    if num_glitches > 0:\n",
    "        score += 1 / num_glitches\n",
    "\n",
    "    learned_model.compute_prefixes()\n",
    "    assert len(learned_model.states) == learning_info[\"num_states\"]\n",
    "    num_unreachable_states = sum(1 for state in learned_model.states if state.prefix is None)\n",
    "    if num_unreachable_states > 0:\n",
    "        score -= 1/num_unreachable_states\n",
    "\n",
    "    return score\n",
    "\n",
    "# compare_heuristics([base_heuristic_parameterized, *_create_partial_heuristics(zscores_sum, glitch_threshold=[1], dominant_threshold=[-3])])\n",
    "compare_heuristics([new_heuristic, base_heuristic_parameterized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_heuristic3(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, punish_zero=False):\n",
    "    score = 0\n",
    "        \n",
    "    # reward lower mean of glitched frequencies\n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    if len(glitched_delta_freq) > 0:\n",
    "        score += 1 / np.mean(glitched_delta_freq)\n",
    "    else:\n",
    "        score += 1  # this rewards \"no glitches\" a lot\n",
    "        \n",
    "    # reward lower numbers of glitches\n",
    "    num_glitches = len(learning_info[\"glitch_steps\"])\n",
    "    if num_glitches > 0:\n",
    "        score += 1 / num_glitches\n",
    "\n",
    "    # penalize unreachable states (in absolute numbers!) a LOT\n",
    "    learned_model.compute_prefixes()\n",
    "    num_unreachable = sum(1 for state in learned_model.states if state.prefix is None)\n",
    "    score -= num_unreachable\n",
    "    \n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq2 = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "    score -= 1 / np.min(dominant_delta_freq2)\n",
    "    # score -= len(learned_model.states) / lower_10(dominant_delta_freq)\n",
    "    \n",
    "    if punish_zero:\n",
    "        dom_zero_trans = [freq for freq in dominant_delta_freq if freq == 0]\n",
    "        score -= len(dom_zero_trans)\n",
    "    \n",
    "    return score\n",
    "\n",
    "compare_heuristics([*_create_partial_heuristics(new_heuristic3, punish_zero=[True, False]), base_heuristic], models_dir=\"../generated_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_base_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]]):\n",
    "    score = 0\n",
    "        \n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    if len(glitched_delta_freq) > 0:\n",
    "        score += 1 / np.mean(glitched_delta_freq)\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    dominant_delta_freq = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "    \n",
    "    score -= 1 / np.min(dominant_delta_freq)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, reward_lower_glitch_mean=True, reward_lower_num_glitches=True, punish_unreachable_states=True, punish_low_dominant_freq=True, punish_input_incompleteness=True):\n",
    "    score = 0\n",
    "        \n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "    \n",
    "    # reward lower mean of glitched frequencies\n",
    "    if reward_lower_glitch_mean:\n",
    "        if len(glitched_delta_freq) > 0:\n",
    "            score += 1 / np.mean(glitched_delta_freq)\n",
    "        else:\n",
    "            score += 1  # this rewards \"no glitches\" a lot\n",
    "        \n",
    "    # reward lower numbers of glitches\n",
    "    if reward_lower_num_glitches:\n",
    "        num_glitches = len(learning_info[\"glitch_steps\"])\n",
    "        if num_glitches > 0:\n",
    "            score += 1 / num_glitches\n",
    "\n",
    "    # penalize unreachable states (in absolute numbers!) a LOT\n",
    "    if punish_unreachable_states:\n",
    "        learned_model.compute_prefixes()\n",
    "        num_unreachable = sum(1 for state in learned_model.states if state.prefix is None)\n",
    "        score -= num_unreachable\n",
    "    \n",
    "    # penalize low dominant frequencies\n",
    "    if punish_low_dominant_freq:\n",
    "        dominant_delta_freq_without_zero = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "        score -= 1 / np.min(dominant_delta_freq_without_zero)\n",
    "    \n",
    "    # punish dominant frequencies that are 0 (-> not input complete)\n",
    "    if punish_input_incompleteness:\n",
    "        num_dominant_zero_freq = sum(1 for freq in dominant_delta_freq if freq == 0)\n",
    "        score -= num_dominant_zero_freq\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_heuristics([simple_base_heuristic, advanced_heuristic], models_dir=\"../generated_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_heuristics([\n",
    "    simple_base_heuristic, \n",
    "    *_create_partial_heuristics(advanced_heuristic, \n",
    "                                reward_lower_glitch_mean=[True, False], \n",
    "                                reward_lower_num_glitches=[True, False], \n",
    "                                punish_unreachable_states=[True, False], \n",
    "                                punish_low_dominant_freq=[True, False], \n",
    "                                punish_input_incompleteness=[True, False])], \n",
    "    models_dir=\"../generated_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _(func, **kwargs):\n",
    "    f = functools.partial(func, **kwargs)\n",
    "    f.__name__ = func.__name__ + \"(\" + \", \".join(f\"{key}={value}\" for key, value in kwargs.items()) + \")\"\n",
    "    return f\n",
    "\n",
    "compare_heuristics(\n",
    "    [\n",
    "        simple_base_heuristic, \n",
    "        advanced_heuristic,  # all True\n",
    "        _(advanced_heuristic, punish_input_incompleteness=False),\n",
    "        _(advanced_heuristic, punish_unreachable_states=False),\n",
    "        _(advanced_heuristic, punish_input_incompleteness=False,\n",
    "                              punish_unreachable_states=False),\n",
    "        _(advanced_heuristic, reward_lower_num_glitches=False),\n",
    "        \n",
    "    ],\n",
    "    models_dir=\"../generated_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_heuristic2(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]], *, reward_lower_glitch_mean=True, reward_lower_num_glitches=True, punish_unreachable_states=True, punish_low_dominant_freq=True, punish_input_incompleteness=True):\n",
    "    score = 0\n",
    "\n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "\n",
    "    # reward lower mean of glitched frequencies\n",
    "    # ADDS between 0 and 1\n",
    "    if reward_lower_glitch_mean:\n",
    "        if len(glitched_delta_freq) > 0:\n",
    "            score += 1 / np.mean(glitched_delta_freq)\n",
    "\n",
    "    # reward lower numbers of glitches\n",
    "    # ADDS between 0 and 1\n",
    "    if reward_lower_num_glitches:\n",
    "        num_glitches = len(learning_info[\"glitch_steps\"])\n",
    "        if num_glitches > 0:\n",
    "            score += 1 / num_glitches\n",
    "        else:\n",
    "            score += 1  # this rewards \"no glitches\" a lot\n",
    "\n",
    "    # penalize low dominant frequencies\n",
    "    # SUBTRACTS between 0 and 1\n",
    "    if punish_low_dominant_freq:\n",
    "        dominant_delta_freq_without_zero = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "        score -= 1 / np.min(dominant_delta_freq_without_zero)\n",
    "\n",
    "    # # punish dominant frequencies that are 0 (-> not input complete)\n",
    "    # if punish_input_incompleteness:\n",
    "    #     num_dominant_zero_freq = sum(1 for freq in dominant_delta_freq if freq == 0)\n",
    "    #     num_dominant_freq = len(dominant_delta_freq)\n",
    "    #     score -= 1 / (1 + np.exp(-num_dominant_zero_freq))\n",
    "\n",
    "    # penalize unreachable states\n",
    "    # SETS score to -1 or does nothing\n",
    "    # TODO: this does nothing to the distribution\n",
    "    if punish_unreachable_states:\n",
    "        learned_model.compute_prefixes()\n",
    "        num_unreachable = sum(1 for state in learned_model.states if state.prefix is None)\n",
    "        # num_states = len(learned_model.states)\n",
    "        # percent = num_unreachable / num_states\n",
    "        # score -= 1 / (1 + np.exp(-num_unreachable))\n",
    "        # score -= np.exp(-num_unreachable)\n",
    "    #     if num_unreachable > 0:\n",
    "    #         score = -2\n",
    "\n",
    "    return score\n",
    "\n",
    "def advanced_heuristic3(*args, **kwargs):\n",
    "    return advanced_heuristic(*args, **kwargs, punish_input_incompleteness=False, punish_unreachable_states=False)\n",
    "\n",
    "compare_heuristics([simple_base_heuristic, advanced_heuristic, advanced_heuristic3], models_dir=\"../generated_data_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_something(value_name: str, stat_method = np.sum, models_dir=\"../generated_data\", new_plot=True, show_plot=True, as_line=False,\n",
    "                   value_retriever_function=lambda learned_model, learning_info, learning_result, value_name: learning_result[value_name]):\n",
    "    if new_plot:\n",
    "        plt.figure()\n",
    "\n",
    "    values_at_distance = defaultdict(list)\n",
    "    for model_name in os.listdir(models_dir):\n",
    "        if not (Path(models_dir) / model_name).is_dir():\n",
    "            continue\n",
    "        real_model, learned_models, infos = load_data(model_name, models_dir)\n",
    "        for learned_model, learning_info, learning_result in learned_models:\n",
    "            val = value_retriever_function(learned_model, learning_info, learning_result, value_name)\n",
    "            values_at_distance[learning_result[\"distance\"]].append(val)\n",
    "\n",
    "    keys = sorted(values_at_distance.keys())\n",
    "    vals = [stat_method(values_at_distance[k]) for k in keys]\n",
    "    if as_line:\n",
    "        plt.plot(keys, vals, label=value_name)\n",
    "    else:\n",
    "        plt.scatter(keys, vals, label=value_name)\n",
    "\n",
    "    plt.title(f\"{stat_method.__name__} of {value_name} at distance\")\n",
    "    plt.xlabel(\"Distance to ground truth\")\n",
    "    plt.ylabel(f\"{stat_method.__name__} of {value_name}\")\n",
    "    plt.legend(loc=(1.04, 1))\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_something('num_states_unreachable', stat_method=np.mean, models_dir='../generated_data_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_something('dominant transition 0-frequencies', stat_method=np.mean, models_dir='../generated_data_1',\n",
    "               value_retriever_function=lambda lm, li, lr, vn: sum(1 for freq in lr['dominant_delta_freq'] if freq == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "things = [\n",
    "    \"num_states_unreachable\",\n",
    "    \"percent_states_unreachable\",\n",
    "    \"num_glitches\",\n",
    "    \"percent_glitches\",\n",
    "    \"mean_glitch_trans_freq\",\n",
    "    \"median_glitch_trans_freq\",\n",
    "    \"min_glitch_trans_freq\",\n",
    "    \"max_glitch_trans_freq\",\n",
    "    \"mean_dominant_trans_freq\",\n",
    "    \"median_dominant_trans_freq\",\n",
    "    \"max_dominant_trans_freq\",\n",
    "    \"min_dominant_trans_freq\",\n",
    "]\n",
    "\n",
    "for thing in things:\n",
    "    for stat_method in (np.mean, np.median):\n",
    "        plot_something(thing, stat_method=stat_method, models_dir='../generated_data_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_heuristic(learned_model: MooreMachine, learning_info: dict[str, Any], traces: list[list[str | list[str]]]):\n",
    "    score = 0\n",
    "\n",
    "    glitched_delta_freq = learning_info[\"glitched_delta_freq\"]\n",
    "    dominant_delta_freq = learning_info[\"dominant_delta_freq\"]\n",
    "\n",
    "    num_dominant_zero_freq = sum(1 for freq in dominant_delta_freq if freq == 0)\n",
    "    score = num_dominant_zero_freq\n",
    "\n",
    "    return score\n",
    "\n",
    "plot_heuristic_scores_for_all_models_in_dir(test_heuristic, models_dir='../generated_data_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_pmsatlearn.learnalgo import simple_heuristic, intermediary_heuristic\n",
    "compare_heuristics([simple_heuristic, intermediary_heuristic], models_dir='../generated_data_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from active_pmsatlearn.defs import Trace\n",
    "\n",
    "def _get_number_of_next_valid_steps(state: MooreState, trace: Trace) -> int:\n",
    "    c = 0\n",
    "    for inp, out in trace:\n",
    "        next_state = state.transitions[inp]\n",
    "        if next_state.output == out:\n",
    "            c += 1\n",
    "            state = next_state\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return c\n",
    "\n",
    "def approximate_glitch_frequencies(learned_model: MooreMachine, traces: list[Trace]) -> tuple[list[int], list[int]]:\n",
    "    learned_model.make_input_complete()  # TODO: this might be the problem. If the glitch leads to an unreachable state, ...\n",
    "    \n",
    "    dominant_steps = defaultdict(int)\n",
    "    glitch_steps = defaultdict(int)\n",
    "\n",
    "    for trace in traces:\n",
    "        learned_model.reset_to_initial()\n",
    "        assert learned_model.current_state.output == trace[0]\n",
    "        for step_index, (inp, out) in enumerate(trace[1:], start=1):\n",
    "            old_state = learned_model.current_state\n",
    "            model_out = learned_model.step(inp)\n",
    "            new_state = learned_model.current_state\n",
    "            if model_out != out:\n",
    "                # this step (must|would) have been marked by pmsat as a glitch\n",
    "\n",
    "                believe_states = {s: _get_number_of_next_valid_steps(s, trace[step_index+1:]) for s in learned_model.states if s.output == out}\n",
    "\n",
    "                best_original_believe_state = max(believe_states, key=believe_states.get)\n",
    "                learned_model.current_state = best_original_believe_state\n",
    "                glitch_steps[(old_state, inp, best_original_believe_state)] += 1\n",
    "            else:\n",
    "                dominant_steps[(old_state, inp, new_state)] += 1\n",
    "\n",
    "    return [d for d in dominant_steps.values()], [g for g in glitch_steps.values()]\n",
    "\n",
    "\n",
    "def old_approximate_glitch_frequencies(learned_model: MooreMachine, traces: list[Trace]) -> list[float]:\n",
    "    # learned_model.make_input_complete()  # TODO: this might be the problem. If the glitch leads to an unreachable state, ...\n",
    "    \n",
    "    glitch_steps = defaultdict(int)\n",
    "\n",
    "    for trace in traces:\n",
    "        learned_model.reset_to_initial()\n",
    "        assert learned_model.current_state.output == trace[0]\n",
    "        for step_index, (inp, out) in enumerate(trace[1:], start=1):\n",
    "            old_state = learned_model.current_state\n",
    "            model_out = learned_model.step(inp)\n",
    "            if model_out != out:\n",
    "                # this step (must|would) have been marked by pmsat as a glitch\n",
    "\n",
    "                # dict of <first believe state> -> <current believe state>\n",
    "                believe_states = {s: s for s in learned_model.states if s.output == out}\n",
    "                while len(believe_states) > 1:\n",
    "\n",
    "                    _inp, _out = trace[step_index]\n",
    "                    curr_believe_states = list(believe_states.items())\n",
    "                    for orig_state, curr_state in curr_believe_states:\n",
    "                        if (nxt := curr_state.transitions[_inp]).output == _out:\n",
    "                            believe_states[orig_state] = nxt\n",
    "                        else:\n",
    "                            if len(believe_states) >= 2:\n",
    "                                believe_states.pop(orig_state)\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                    step_index += 1\n",
    "                    if step_index == len(trace):\n",
    "                        # we reached the end of the trace -> choose one state at random\n",
    "                        while len(believe_states) > 1:\n",
    "                            believe_states.pop(random.choice(list(believe_states.keys())))\n",
    "\n",
    "                best_original_believe_state = list(believe_states.values())[0]\n",
    "                learned_model.current_state = best_original_believe_state\n",
    "                glitch_steps[(old_state, inp, best_original_believe_state)] += 1\n",
    "\n",
    "\n",
    "    return list(glitch_steps.values())\n",
    "\n",
    "def _compare(actuals: list, approxs: list):\n",
    "    actuals = np.array(actuals)\n",
    "    approxs = np.array(approxs)\n",
    "    # Metrics\n",
    "    mae = np.mean(np.abs(actuals - approxs))\n",
    "    mse = np.mean((actuals - approxs) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((actuals - approxs) / actuals)) * 100\n",
    "    r2 = 1 - (np.sum((actuals - approxs) ** 2) / np.sum((actuals - np.mean(actuals)) ** 2))\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"mean absolute error: {mae}\")\n",
    "    print(f\"mean squared error: {mse}\")\n",
    "    print(f\"root mean squared error: {rmse}\")\n",
    "    print(f\"mean absolute percentage error: {mape:.2f}%\")\n",
    "    print(f\"R: {r2:.2f}\")\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 20))  # Adjusted figure size for vertical stacking\n",
    "\n",
    "    # Scatter Plot\n",
    "    plt.subplot(3, 1, 1)  # 3 rows, 1 column, 1st plot\n",
    "    plt.scatter(actuals, approxs, color='blue', label='Approximations')\n",
    "    plt.plot(actuals, actuals, color='red', linestyle='--', label='Perfect Fit (y=x)')\n",
    "    plt.xlabel('Actuals')\n",
    "    plt.ylabel('Approximations')\n",
    "    plt.title('Scatter Plot')\n",
    "    plt.legend()\n",
    "\n",
    "    # Line Plot\n",
    "    plt.subplot(3, 1, 2)  # 3 rows, 1 column, 2nd plot\n",
    "    plt.plot(actuals, label='Actuals', marker='o')\n",
    "    plt.plot(approxs, label='Approximations', marker='x')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Line Plot')\n",
    "    plt.legend()\n",
    "\n",
    "    # Error Plot\n",
    "    plt.subplot(3, 1, 3)  # 3 rows, 1 column, 3rd plot\n",
    "    errors = actuals - approxs\n",
    "    plt.bar(range(len(errors)), errors, color='purple')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Error (Actual - Approx)')\n",
    "    plt.title('Error Plot')\n",
    "\n",
    "    # Adjust layout and show\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_actual_with_approx_freq(stat_method=np.mean, freqs=\"glitched\", models_dir=DEFAULT_DATA_DIR):\n",
    "    actuals = []\n",
    "    approxs = []\n",
    "\n",
    "    for model_name in os.listdir(models_dir):\n",
    "        if not os.path.isdir(Path(models_dir) / model_name):\n",
    "            continue\n",
    "        ground_truth, learned_models, info = load_data(model_name, models_dir)\n",
    "        for learned_model, learning_info, learning_result in learned_models:\n",
    "            if not learned_model.is_input_complete():\n",
    "                continue\n",
    "            actual_freqs = learning_info[\"glitched_delta_freq\"] if freqs == \"glitched\" else learning_info[\"dominant_delta_freq\"]\n",
    "            actual = stat_method(actual_freqs or [0])\n",
    "            actuals.append(actual)\n",
    "\n",
    "            approx_freqs = approximate_glitch_frequencies(learned_model, info[\"traces\"])[1 if freqs == \"glitched\" else 0]\n",
    "            approx = np.max(approx_freqs or [0])\n",
    "            approxs.append(approx)\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    print(f\"{freqs.upper()}:\")\n",
    "    _compare(actuals, approxs)\n",
    "    # diffs = []\n",
    "    # for actual, approx in zip(actuals, approxs):\n",
    "    #     diff = actual - approx\n",
    "    #     print(f\"{stat_method.__name__}: ACTUAL: {actual:.2f} | APPROX: {approx:.2f} | diff: {diff:.2f}\")\n",
    "    # stdev = np.std\n",
    "\n",
    "compare_actual_with_approx_freq(stat_method=np.mean, freqs=\"glitched\")\n",
    "compare_actual_with_approx_freq(stat_method=np.mean, freqs=\"dominant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_pmsatlearn.defs import SupportedAutomaton, PmSatLearningInfo\n",
    "\n",
    "\n",
    "def approximate_advanced_heuristic(learned_model: SupportedAutomaton, learning_info: PmSatLearningInfo, traces: list[Trace],\n",
    "                       *, reward_lower_glitch_mean=True, reward_lower_num_glitches=True, punish_unreachable_states=True,\n",
    "                       punish_low_dominant_freq=True, punish_input_incompleteness=True):\n",
    "    score = 0\n",
    "\n",
    "    if learned_model is None:\n",
    "        return -100\n",
    "\n",
    "    dominant_delta_freq, glitched_delta_freq = approximate_glitch_frequencies(learned_model, traces)\n",
    "\n",
    "    # reward lower mean of glitched frequencies\n",
    "    # ADDS between 0 and 1\n",
    "    if reward_lower_glitch_mean:\n",
    "        if len(glitched_delta_freq) > 0:\n",
    "            score += 1 / np.mean(glitched_delta_freq)\n",
    "\n",
    "    # reward lower numbers of glitches\n",
    "    # ADDS between 0 and 1\n",
    "    if reward_lower_num_glitches:\n",
    "        num_glitches = sum(glitched_delta_freq)\n",
    "        if num_glitches > 0:\n",
    "            score += 1 / num_glitches\n",
    "        else:\n",
    "            score += 1\n",
    "\n",
    "    # penalize low dominant frequencies\n",
    "    # SUBTRACTS between 0 and 1\n",
    "    if punish_low_dominant_freq:\n",
    "        dominant_delta_freq_without_zero = [freq for freq in dominant_delta_freq if freq > 0]\n",
    "        score -= 1 / np.min(dominant_delta_freq_without_zero)\n",
    "\n",
    "    # penalize unreachable states (in absolute numbers!) a LOT\n",
    "    # SUBTRACTS between 0 and inf\n",
    "    if punish_unreachable_states:\n",
    "        learned_model.compute_prefixes()\n",
    "        num_unreachable = sum(1 for state in learned_model.states if state.prefix is None)\n",
    "        score -= num_unreachable\n",
    "\n",
    "    # punish dominant frequencies that are 0 (-> not input complete)\n",
    "    # SUBTRACTS between 0 and inf\n",
    "    if punish_input_incompleteness:\n",
    "        num_dominant_zero_freq = sum(1 for freq in dominant_delta_freq if freq == 0)\n",
    "        score -= num_dominant_zero_freq\n",
    "\n",
    "    return float(score)  # cast numpy away (nicer output)\n",
    "\n",
    "from active_pmsatlearn.learnalgo import simple_heuristic\n",
    "compare_heuristics([simple_heuristic, approximate_advanced_heuristic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
